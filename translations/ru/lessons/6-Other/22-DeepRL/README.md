# Глубокое Обучение с Подкреплением

Обучение с подкреплением (RL) считается одним из основных парадигм машинного обучения, наряду с обучением с учителем и без учителя. В то время как в обучении с учителем мы полагаемся на набор данных с известными результатами, RL основано на **обучении через действие**. Например, когда мы впервые видим компьютерную игру, мы начинаем играть, даже не зная правил, и вскоре можем улучшить свои навыки просто в процессе игры и корректируя свое поведение.

## [Предлекционный тест](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Для выполнения RL нам нужно:

* **Среда** или **симулятор**, который устанавливает правила игры. Мы должны иметь возможность проводить эксперименты в симуляторе и наблюдать за результатами.
* Некоторая **функция вознаграждения**, которая указывает, насколько успешным было наше эксперимент. В случае обучения игре в компьютерную игру, вознаграждение будет нашим итоговым счетом.

На основе функции вознаграждения мы должны быть в состоянии корректировать свое поведение и улучшать свои навыки, чтобы в следующий раз играть лучше. Основное отличие других типов машинного обучения от RL заключается в том, что в RL мы обычно не знаем, выиграли мы или проиграли, пока не закончим игру. Таким образом, мы не можем сказать, является ли определенный ход хорошим или нет — мы получаем вознаграждение только в конце игры.

Во время RL мы обычно проводим много экспериментов. В каждом эксперименте нам нужно находить баланс между следованием оптимальной стратегии, которую мы узнали до сих пор (**эксплуатация**), и исследованием новых возможных состояний (**исследование**).

## OpenAI Gym

Отличным инструментом для RL является [OpenAI Gym](https://gym.openai.com/) - **симуляционная среда**, которая может моделировать множество различных окружений, начиная от игр Atari и заканчивая физикой балансировки столба. Это одна из самых популярных симуляционных сред для обучения алгоритмов обучения с подкреплением и поддерживается [OpenAI](https://openai.com/).

> **Примечание**: Вы можете увидеть все доступные среды OpenAI Gym [здесь](https://gym.openai.com/envs/#classic_control).

## Балансировка CartPole

Вы, вероятно, все видели современные устройства для балансировки, такие как *Segway* или *Гироскутеры*. Они способны автоматически балансировать, регулируя свои колеса в ответ на сигнал от акселерометра или гироскопа. В этом разделе мы узнаем, как решить аналогичную задачу - балансировку столба. Это похоже на ситуацию, когда цирковой артист должен сбалансировать столб на своей руке - но эта балансировка столба происходит только в 1D.

Упрощенная версия балансировки известна как задача **CartPole**. В мире cartpole у нас есть горизонтальный слайдер, который может двигаться влево или вправо, и цель состоит в том, чтобы сбалансировать вертикальный столб на верхней части слайдера, пока он движется.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

Чтобы создать и использовать эту среду, нам нужно несколько строк кода на Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Каждую среду можно получить точно так же:
* `env.reset` starts a new experiment
* `env.step` выполняет шаг симуляции. Он получает **действие** из **пространства действий** и возвращает **наблюдение** (из пространства наблюдений), а также вознаграждение и флаг завершения.

В приведенном выше примере мы выполняем случайное действие на каждом шаге, поэтому жизнь эксперимента очень коротка:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Цель алгоритма RL — обучить модель, так называемую **политику** π, которая будет возвращать действие в ответ на данное состояние. Мы также можем рассматривать политику как вероятностную, т.е. для любого состояния *s* и действия *a* она будет возвращать вероятность π(*a*|*s*), что мы должны предпринять *a* в состоянии *s*.

## Алгоритм Градиентов Политики

Самый очевидный способ моделировать политику — создать нейронную сеть, которая будет принимать состояния на вход и возвращать соответствующие действия (или, точнее, вероятности всех действий). В некотором смысле это будет похоже на обычную задачу классификации, с одной большой разницей — мы не знаем заранее, какие действия мы должны предпринять на каждом из шагов.

Идея заключается в том, чтобы оценить эти вероятности. Мы строим вектор **кумулятивных вознаграждений**, который показывает наше общее вознаграждение на каждом шаге эксперимента. Мы также применяем **дисконтирование вознаграждений**, умножая более ранние вознаграждения на некоторый коэффициент γ=0.99, чтобы уменьшить роль более ранних вознаграждений. Затем мы усиливаем те шаги вдоль пути эксперимента, которые приносят большие вознаграждения.

> Узнайте больше об алгоритме градиентов политики и посмотрите его в действии в [примерной записной книжке](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Алгоритм Актор-Критик

Улучшенная версия подхода Градиентов Политики называется **Актор-Критик**. Основная идея заключается в том, что нейронная сеть будет обучена возвращать две вещи:

* Политику, которая определяет, какое действие предпринять. Эта часть называется **актор**.
* Оценку общего вознаграждения, которое мы можем ожидать получить в этом состоянии — эта часть называется **критик**.

В некотором смысле эта архитектура напоминает [GAN](../../4-ComputerVision/10-GANs/README.md), где у нас есть две сети, которые обучаются друг против друга. В модели актор-критик актер предлагает действие, которое нам нужно предпринять, а критик пытается быть критичным и оценить результат. Однако наша цель — обучить эти сети в унисон.

Поскольку мы знаем как реальные кумулятивные вознаграждения, так и результаты, возвращаемые критиком во время эксперимента, относительно легко построить функцию потерь, которая минимизирует разницу между ними. Это даст нам **потерю критика**. Мы можем вычислить **потерю актора**, используя тот же подход, что и в алгоритме градиентов политики.

После запуска одного из этих алгоритмов мы можем ожидать, что наш CartPole будет вести себя так:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ Упражнения: Градиенты Политики и RL Актор-Критик

Продолжайте обучение в следующих записных книжках:

* [RL в TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL в PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Другие Задачи RL

Обучение с подкреплением в настоящее время является быстро развивающейся областью исследований. Некоторые интересные примеры обучения с подкреплением:

* Обучение компьютера играть в **игры Atari**. Сложной частью этой проблемы является то, что у нас нет простого состояния, представленное в виде вектора, а скорее скриншот — и нам нужно использовать CNN, чтобы преобразовать это изображение экрана в вектор признаков или извлечь информацию о вознаграждении. Игры Atari доступны в Gym.
* Обучение компьютера играть в настольные игры, такие как Шахматы и Го. Недавно программы, находящиеся на передовом уровне, такие как **Alpha Zero**, были обучены с нуля двумя агентами, играющими друг против друга и улучшающимися на каждом шаге.
* В промышленности RL используется для создания систем управления из симуляции. Сервис под названием [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) специально разработан для этого.

## Заключение

Теперь мы узнали, как обучать агентов для достижения хороших результатов, просто предоставляя им функцию вознаграждения, которая определяет желаемое состояние игры, и давая им возможность разумно исследовать пространство поиска. Мы успешно протестировали два алгоритма и добились хорошего результата за относительно короткий период времени. Однако это всего лишь начало вашего пути в RL, и вам определенно стоит рассмотреть возможность прохождения отдельного курса, если вы хотите углубиться.

## 🚀 Вызов

Изучите приложения, перечисленные в разделе 'Другие Задачи RL', и постарайтесь реализовать одно из них!

## [Постлекционный тест](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Обзор и Самостоятельное Изучение

Узнайте больше о классическом обучении с подкреплением в нашем [Учебном плане по машинному обучению для начинающих](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Посмотрите [это замечательное видео](https://www.youtube.com/watch?v=qv6UVOQ0F44), в котором рассказывается о том, как компьютер может научиться играть в Super Mario.

## Задание: [Обучите Гору](lab/README.md)

Ваша цель в этом задании — обучить другую среду Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Отказ от ответственности**:  
Этот документ был переведен с использованием услуг машинного перевода на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неверные интерпретации, возникающие в результате использования этого перевода.