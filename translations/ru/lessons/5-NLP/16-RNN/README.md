# Рекуррентные Нейронные Сети

## [Тест перед лекцией](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

В предыдущих разделах мы использовали богатые семантические представления текста и простой линейный классификатор на основе встраиваний. Эта архитектура захватывает агрегированное значение слов в предложении, но не учитывает **порядок** слов, так как операция агрегации на встраиваниях удаляет эту информацию из исходного текста. Поскольку эти модели не способны моделировать порядок слов, они не могут решать более сложные или неоднозначные задачи, такие как генерация текста или ответ на вопросы.

Чтобы захватить значение последовательности текста, нам нужно использовать другую архитектуру нейронной сети, которая называется **рекуррентной нейронной сетью**, или RNN. В RNN мы пропускаем наше предложение через сеть по одному символу за раз, и сеть производит некое **состояние**, которое мы затем снова передаем в сеть с следующим символом.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ru.png)

> Изображение предоставлено автором

Учитывая входную последовательность токенов X<sub>0</sub>,...,X<sub>n</sub>, RNN создает последовательность блоков нейронной сети и обучает эту последовательность от начала до конца с использованием обратного распространения ошибки. Каждый блок сети принимает пару (X<sub>i</sub>,S<sub>i</sub>) в качестве входных данных и производит S<sub>i+1</sub> в качестве результата. Конечное состояние S<sub>n</sub> или (выход Y<sub>n</sub>) идет в линейный классификатор для получения результата. Все блоки сети имеют одинаковые веса и обучаются от начала до конца с использованием одного прохода обратного распространения.

Поскольку векторы состояния S<sub>0</sub>,...,S<sub>n</sub> передаются через сеть, она способна изучать последовательные зависимости между словами. Например, когда слово *not* появляется где-то в последовательности, оно может научиться отрицать определенные элементы внутри вектора состояния, что приводит к отрицанию.

> ✅ Поскольку веса всех блоков RNN на изображении выше общие, то же изображение может быть представлено как один блок (справа) с рекуррентной обратной связью, которая передает выходное состояние сети обратно на вход.

## Анатомия ячейки RNN

Давайте посмотрим, как организована простая ячейка RNN. Она принимает предыдущее состояние S<sub>i-1</sub> и текущий символ X<sub>i</sub> в качестве входных данных и должна производить выходное состояние S<sub>i</sub> (и, иногда, нас также интересует какой-то другой выход Y<sub>i</sub>, как в случае с генеративными сетями).

Простая ячейка RNN имеет две матрицы весов внутри: одна преобразует входной символ (назовем ее W), а другая преобразует входное состояние (H). В этом случае выход сети рассчитывается как σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b), где σ — это функция активации, а b — дополнительный смещение.

<img alt="Анатомия ячейки RNN" src="images/rnn-anatomy.png" width="50%"/>

> Изображение предоставлено автором

Во многих случаях входные токены проходят через слой встраивания перед тем, как попасть в RNN, чтобы снизить размерность. В этом случае, если размерность входных векторов составляет *emb_size*, а вектора состояния — *hid_size*, то размер W составляет *emb_size*×*hid_size*, а размер H — *hid_size*×*hid_size*.

## Долгая Краткосрочная Память (LSTM)

Одной из основных проблем классических RNN является так называемая проблема **исчезающих градиентов**. Поскольку RNN обучаются от начала до конца за один проход обратного распространения, им трудно распространять ошибку на первые слои сети, и, следовательно, сеть не может изучать отношения между удаленными токенами. Один из способов избежать этой проблемы — ввести **явное управление состоянием** с помощью так называемых **ворот**. Существует две хорошо известные архитектуры такого рода: **Долгая Краткосрочная Память** (LSTM) и **Gated Relay Unit** (GRU).

![Изображение, показывающее пример ячейки долгой краткосрочной памяти](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Источник изображения TBD

Сеть LSTM организована аналогично RNN, но существует два состояния, которые передаются от слоя к слою: текущее состояние C и скрытый вектор H. На каждом узле скрытый вектор H<sub>i</sub> конкатенируется с входом X<sub>i</sub>, и они контролируют, что происходит с состоянием C через **ворота**. Каждые ворота представляют собой нейронную сеть с сигмоидной активацией (выход в диапазоне [0,1]), которые можно рассматривать как побитовую маску при умножении на вектор состояния. Существуют следующие ворота (слева направо на изображении выше):

* **Ворота забвения** принимают скрытый вектор и определяют, какие компоненты вектора C нужно забыть, а какие пропустить.
* **Ворота ввода** берут некоторую информацию из входных и скрытых векторов и вставляют ее в состояние.
* **Ворота вывода** преобразуют состояние через линейный слой с активацией *tanh*, затем выбирают некоторые из его компонентов, используя скрытый вектор H<sub>i</sub>, чтобы произвести новое состояние C<sub>i+1</sub>.

Компоненты состояния C можно рассматривать как некоторые флаги, которые могут быть включены и выключены. Например, когда мы встречаем имя *Alice* в последовательности, мы можем предположить, что оно относится к женскому персонажу, и поднять флаг в состоянии о том, что у нас есть женский существительное в предложении. Когда мы далее встречаем фразу *and Tom*, мы поднимем флаг о том, что у нас есть множественное число. Таким образом, манипулируя состоянием, мы можем предположительно отслеживать грамматические свойства частей предложения.

> ✅ Отличный ресурс для понимания внутреннего устройства LSTM — это замечательная статья [Понимание LSTM-сетей](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Кристофера Олаха.

## Двунаправленные и Многослойные RNN

Мы обсуждали рекуррентные сети, которые работают в одном направлении, от начала последовательности до конца. Это выглядит естественно, поскольку напоминает способ, которым мы читаем и слушаем речь. Однако, поскольку во многих практических случаях у нас есть произвольный доступ к входной последовательности, имеет смысл выполнять рекуррентные вычисления в обоих направлениях. Такие сети называются **двунаправленными** RNN. При работе с двунаправленной сетью нам понадобятся два скрытых вектора состояния, по одному для каждого направления.

Рекуррентная сеть, будь то однонаправленная или двунаправленная, захватывает определенные шаблоны внутри последовательности и может хранить их в векторе состояния или передавать в выход. Как и в случае с свёрточными сетями, мы можем построить другой рекуррентный слой поверх первого, чтобы захватить более высокоуровневые шаблоны и строить их на основе низкоуровневых шаблонов, извлеченных первым слоем. Это приводит нас к понятию **многослойной RNN**, которая состоит из двух или более рекуррентных сетей, где выход предыдущего слоя передается в следующий слой в качестве входа.

![Изображение, показывающее многослойную долгую краткосрочную память RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ru.jpg)

*Изображение из [этого замечательного поста](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) Фернандо Лопеса*

## ✍️ Упражнения: Встраивания

Продолжайте обучение в следующих блокнотах:

* [RNN с PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNN с TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Заключение

В этом модуле мы увидели, что RNN могут использоваться для классификации последовательностей, но на самом деле они могут справляться со многими другими задачами, такими как генерация текста, машинный перевод и многое другое. Мы рассмотрим эти задачи в следующем модуле.

## 🚀 Вызов

Прочитайте несколько материалов о LSTM и рассмотрите их применение:

- [Сетка Долгой Краткосрочной Памяти](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Показать, Обратить Внимание и Сказать: Генерация Подписей к Изображениям с Нейронным Вниманием](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Тест после лекции](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Обзор и Самостоятельное Изучение

- [Понимание LSTM-сетей](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Кристофера Олаха.

## [Задание: Блокноты](assignment.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием машинных переводческих услуг на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неверные толкования, возникающие в результате использования этого перевода.