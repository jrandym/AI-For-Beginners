# Предобученные большие языковые модели

Во всех наших предыдущих задачах мы обучали нейронную сеть для выполнения определенной задачи с использованием размеченного набора данных. С большими трансформерными моделями, такими как BERT, мы используем языковое моделирование в самообучающемся формате для создания языковой модели, которая затем специализируется на конкретной задаче с дальнейшим обучением в конкретной области. Однако было продемонстрировано, что большие языковые модели могут также решать многие задачи без какого-либо обучения в конкретной области. Семейство моделей, способных на это, называется **GPT**: Генеративный предобученный трансформер.

## [Викторина перед лекцией](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)

## Генерация текста и перплексность

Идея о том, что нейронная сеть может выполнять общие задачи без обучения на конкретной задаче, представлена в статье [Языковые модели являются неконтролируемыми многозадачными обучающими системами](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Основная идея заключается в том, что многие другие задачи могут быть смоделированы с использованием **генерации текста**, потому что понимание текста по сути означает возможность его производить. Поскольку модель обучена на огромном количестве текста, охватывающего человеческие знания, она также становится осведомленной о широком круге тем.

> Понимание и способность производить текст также предполагает знание чего-то о мире вокруг нас. Люди также в значительной степени учатся, читая, и сеть GPT схожа в этом отношении.

Сети генерации текста работают, предсказывая вероятность следующего слова $$P(w_N)$$. Однако безусловная вероятность следующего слова равна частоте этого слова в текстовом корпусе. GPT может предоставить нам **условную вероятность** следующего слова, учитывая предыдущие: $$P(w_N | w_{n-1}, ..., w_0)$$

> Вы можете узнать больше о вероятностях в нашей [Программе по науке о данных для начинающих](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability)

Качество модели генерации языка можно определить с помощью **перплексности**. Это внутренний метрик, который позволяет нам измерять качество модели без каких-либо наборов данных, специфичных для задачи. Он основан на понятии *вероятности предложения* — модель присваивает высокую вероятность предложению, которое, вероятно, является реальным (т.е. модель не **смущена** им), и низкую вероятность предложениям, которые имеют меньше смысла (например, *Может ли это сделать что?*). Когда мы предоставляем нашей модели предложения из реального текстового корпуса, мы ожидаем, что они будут иметь высокую вероятность и низкую **перплексность**. Математически это определяется как нормализованная обратная вероятность тестового набора:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**Вы можете поэкспериментировать с генерацией текста, используя [текстовый редактор на базе GPT от Hugging Face](https://transformer.huggingface.co/doc/gpt2-large)**. В этом редакторе вы начинаете писать свой текст, и нажатие **[TAB]** предложит вам несколько вариантов завершения. Если они слишком короткие или вас не устраивают, нажмите [TAB] снова, и у вас будет больше вариантов, включая более длинные фрагменты текста.

## GPT — это семья

GPT — это не одна модель, а скорее коллекция моделей, разработанных и обученных [OpenAI](https://openai.com). 

Под моделями GPT у нас есть:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
| Языковая модель с до 1,5 миллиарда параметров. | Языковая модель с до 175 миллиардов параметров | 100 триллионов параметров и принимает как текстовые, так и визуальные входные данные, выдавая текст. |

Модели GPT-3 и GPT-4 доступны [как когнитивный сервис от Microsoft Azure](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) и [как OpenAI API](https://openai.com/api/).

## Инженерия подсказок

Поскольку GPT обучен на огромных объемах данных для понимания языка и кода, он предоставляет результаты в ответ на вводимые данные (подсказки). Подсказки — это входные данные или запросы для GPT, в которых даются инструкции моделям по задачам, которые они должны выполнить. Чтобы получить желаемый результат, вам нужна наиболее эффективная подсказка, которая включает выбор правильных слов, форматов, фраз или даже символов. Этот подход называется [Инженерия подсказок](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum)

[Эта документация](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) предоставляет вам дополнительную информацию о инженерии подсказок.

## ✍️ Пример блокнота: [Играя с OpenAI-GPT](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

Продолжите свое обучение в следующих блокнотах:

* [Генерация текста с помощью OpenAI-GPT и Hugging Face Transformers](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## Заключение

Новые общие предобученные языковые модели не только моделируют структуру языка, но и содержат огромное количество естественного языка. Таким образом, их можно эффективно использовать для решения некоторых задач NLP в условиях нулевого или небольшого обучения.

## [Викторина после лекции](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)

**Отказ от ответственности**:  
Этот документ был переведен с использованием услуг машинного перевода на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникающие в результате использования этого перевода.