# Обработка Естественного Языка

![Сводка задач NLP в рисунке](../../../../translated_images/ai-nlp.b22dcb8ca4707ceaee8576db1c5f4089c8cac2f454e9e03ea554f07fda4556b8.ru.png)

В этом разделе мы сосредоточимся на использовании нейронных сетей для решения задач, связанных с **обработкой естественного языка (NLP)**. Существует множество проблем NLP, которые мы хотим, чтобы компьютеры могли решать:

* **Классификация текста** — это типичная задача классификации, относящаяся к последовательностям текста. Примеры включают классификацию электронных писем как спам или не спам, или категоризацию статей как спорт, бизнес, политика и т.д. Также, при разработке чат-ботов, нам часто нужно понять, что пользователь хотел сказать — в этом случае мы имеем дело с **классификацией намерений**. Часто при классификации намерений нам нужно работать с множеством категорий.
* **Анализ настроений** — это типичная задача регрессии, где нам нужно присвоить число (настроение), соответствующее тому, насколько положительное или отрицательное значение предложения. Более продвинутая версия анализа настроений — это **анализ настроений на основе аспектов** (ABSA), где мы присваиваем настроение не всему предложению, а различным его частям (аспектам), например, *В этом ресторане мне понравилась кухня, но атмосфера была ужасной*.
* **Распознавание именованных сущностей** (NER) относится к задаче извлечения определенных сущностей из текста. Например, нам может понадобиться понять, что в фразе *Мне нужно лететь в Париж завтра* слово *завтра* относится к ДАТЕ, а *Париж* — это МЕСТОПОЛОЖЕНИЕ.  
* **Извлечение ключевых слов** похоже на NER, но нам нужно автоматически извлекать слова, важные для смысла предложения, без предварительной настройки для конкретных типов сущностей.
* **Кластеризация текста** может быть полезна, когда мы хотим сгруппировать похожие предложения, например, похожие запросы в разговорах технической поддержки.
* **Ответы на вопросы** относятся к способности модели отвечать на конкретный вопрос. Модель получает текстовый фрагмент и вопрос в качестве входных данных, и ей нужно предоставить место в тексте, где содержится ответ на вопрос (или, иногда, сгенерировать текст ответа).
* **Генерация текста** — это способность модели генерировать новый текст. Это можно рассматривать как задачу классификации, которая предсказывает следующую букву/слово на основе некоторого *текстового подсказки*. Продвинутые модели генерации текста, такие как GPT-3, способны решать другие задачи NLP, такие как классификация, с использованием техники, называемой [программированием подсказок](https://towardsdatascience.com/software-3-0-how-prompting-will-change-the-rules-of-the-game-a982fbfe1e0) или [инженерией подсказок](https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29).
* **Суммаризация текста** — это техника, когда мы хотим, чтобы компьютер "прочитал" длинный текст и подытожил его в нескольких предложениях.
* **Машинный перевод** можно рассматривать как комбинацию понимания текста на одном языке и генерации текста на другом.

Изначально большинство задач NLP решались с использованием традиционных методов, таких как грамматики. Например, в машинном переводе использовались парсеры для преобразования исходного предложения в синтаксическое дерево, затем извлекались более высокоуровневые семантические структуры для представления смысла предложения, и на основе этого смысла и грамматики целевого языка генерировался результат. В настоящее время многие задачи NLP более эффективно решаются с помощью нейронных сетей.

> Многие классические методы NLP реализованы в библиотеке Python [Natural Language Processing Toolkit (NLTK)](https://www.nltk.org). Существует отличная [книга NLTK](https://www.nltk.org/book/), доступная онлайн, которая охватывает, как различные задачи NLP могут быть решены с помощью NLTK.

В нашем курсе мы в основном сосредоточимся на использовании нейронных сетей для NLP и будем использовать NLTK, где это необходимо.

Мы уже изучили использование нейронных сетей для работы с табличными данными и изображениями. Основное различие между этими типами данных и текстом заключается в том, что текст представляет собой последовательность переменной длины, в то время как размер входных данных в случае изображений известен заранее. Хотя сверточные сети могут извлекать шаблоны из входных данных, шаблоны в тексте более сложные. Например, отрицание может отделяться от подлежащего произвольно для многих слов (например, *Мне не нравятся апельсины* против *Мне не нравятся эти большие красочные вкусные апельсины*), и это все равно должно интерпретироваться как один шаблон. Таким образом, чтобы справиться с языком, нам нужно ввести новые типы нейронных сетей, такие как *рекуррентные сети* и *трансформеры*.

## Установка библиотек

Если вы используете локальную установку Python для прохождения этого курса, вам может потребоваться установить все необходимые библиотеки для NLP, используя следующие команды:

**Для PyTorch**
```bash
pip install -r requirements-torch.txt
```
**Для TensorFlow**
```bash
pip install -r requirements-tf.txt
```

> Вы можете попробовать NLP с TensorFlow на [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/?WT.mc_id=academic-77998-cacaste)

## Предупреждение о GPU

В этом разделе в некоторых примерах мы будем обучать довольно крупные модели.
* **Используйте компьютер с поддержкой GPU**: Рекомендуется запускать ваши ноутбуки на компьютере с поддержкой GPU, чтобы сократить время ожидания при работе с крупными моделями.
* **Ограничения памяти GPU**: Работа на GPU может привести к ситуациям, когда у вас закончится память GPU, особенно при обучении крупных моделей.
* **Потребление памяти GPU**: Объем памяти GPU, потребляемой во время обучения, зависит от различных факторов, включая размер мини-партии.
* **Минимизируйте размер мини-партии**: Если вы столкнулись с проблемами памяти GPU, рассмотрите возможность уменьшения размера мини-партии в вашем коде как потенциальное решение.
* **Освобождение памяти GPU в TensorFlow**: Более старые версии TensorFlow могут неправильно освобождать память GPU при обучении нескольких моделей в одном ядре Python. Чтобы эффективно управлять использованием памяти GPU, вы можете настроить TensorFlow на выделение памяти GPU только по мере необходимости.
* **Включение кода**: Чтобы настроить TensorFlow на увеличение выделения памяти GPU только при необходимости, включите следующий код в ваши ноутбуки:

```python
physical_devices = tf.config.list_physical_devices('GPU') 
if len(physical_devices)>0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True) 
```

Если вас интересует изучение NLP с классической точки зрения машинного обучения, посетите [этот набор уроков](https://github.com/microsoft/ML-For-Beginners/tree/main/6-NLP).

## В этом разделе
В этом разделе мы изучим:

* [Представление текста в виде тензоров](13-TextRep/README.md)
* [Векторные представления слов](14-Emdeddings/README.md)
* [Моделирование языка](15-LanguageModeling/README.md)
* [Рекуррентные нейронные сети](16-RNN/README.md)
* [Генеративные сети](17-GenerativeNetworks/README.md)
* [Трансформеры](18-Transformers/README.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием услуг машинного перевода на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования этого перевода.