# Обучение модели Skip-Gram

Лабораторная работа из [Курса ИИ для начинающих](https://github.com/microsoft/ai-for-beginners).

## Задача

В этой лабораторной работе мы предлагаем вам обучить модель Word2Vec, используя технику Skip-Gram. Обучите сеть с встраиванием для предсказания соседних слов в окне Skip-Gram шириной $N$ токенов. Вы можете использовать [код из этого урока](../../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb) и немного изменить его.

## Набор данных

Вы можете использовать любую книгу. Множество бесплатных текстов можно найти на [Project Gutenberg](https://www.gutenberg.org/), например, вот прямая ссылка на [Приключения Алисы в Стране Чудес](https://www.gutenberg.org/files/11/11-0.txt) Льюиса Кэрролла. Или вы можете использовать пьесы Шекспира, которые можно получить с помощью следующего кода:

```python
path_to_file = tf.keras.utils.get_file(
   'shakespeare.txt', 
   'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
```

## Исследуйте!

Если у вас есть время и вы хотите углубиться в тему, попробуйте исследовать несколько аспектов:

* Как размер встраивания влияет на результаты?
* Как различные стили текста влияют на результат?
* Возьмите несколько очень разных типов слов и их синонимы, получите их векторные представления, примените PCA для уменьшения размерности до 2 и изобразите их в 2D пространстве. Видите ли вы какие-либо закономерности?

**Отказ от ответственности**:  
Этот документ был переведен с использованием машинных переводческих сервисов на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникающие в результате использования этого перевода.