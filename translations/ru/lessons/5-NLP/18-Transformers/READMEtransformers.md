# Механизмы внимания и трансформеры

## [Викторина перед лекцией](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Одной из самых важных задач в области обработки естественного языка (NLP) является **машинный перевод**, который лежит в основе таких инструментов, как Google Translate. В этом разделе мы сосредоточимся на машинном переводе или, более общо, на любой задаче *последовательность-в-последовательность* (также называемой **трансдукцией предложений**).

С помощью РСН (рекуррентных нейронных сетей) задача последовательность-в-последовательность реализуется двумя рекуррентными сетями: одна сеть, **кодировщик**, сворачивает входную последовательность в скрытое состояние, в то время как другая сеть, **декодировщик**, разворачивает это скрытое состояние в переведенный результат. У этого подхода есть несколько проблем:

* Финальное состояние сети кодировщика с трудом запоминает начало предложения, что приводит к плохому качеству модели для длинных предложений.
* Все слова в последовательности оказывают одинаковое влияние на результат. Однако на самом деле конкретные слова во входной последовательности часто оказывают большее влияние на последовательные выходные данные, чем другие.

**Механизмы внимания** предоставляют способ взвешивания контекстного влияния каждого входного вектора на каждое предсказание выхода РСН. Это реализуется путем создания кратких путей между промежуточными состояниями входной РСН и выходной РСН. Таким образом, при генерации выходного символа y<sub>t</sub> мы будем учитывать все входные скрытые состояния h<sub>i</sub> с различными весовыми коэффициентами α<sub>t,i</sub>.

![Изображение, показывающее модель кодировщика/декодировщика с аддитивным слоем внимания](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.ru.png)

> Модель кодировщика-декодировщика с аддитивным механизмом внимания в [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), процитировано из [этого блога](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

Матрица внимания {α<sub>i,j</sub>} будет представлять степень, в которой определенные входные слова участвуют в генерации данного слова в выходной последовательности. Ниже приведен пример такой матрицы:

![Изображение, показывающее пример выравнивания, найденного RNNsearch-50, взято из Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.ru.png)

> Рисунок из [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Рис. 3)

Механизмы внимания отвечают за многие достижения в области NLP. Однако добавление внимания значительно увеличивает количество параметров модели, что приводит к проблемам масштабирования с РСН. Ключевым ограничением масштабирования РСН является то, что рекуррентная природа моделей затрудняет пакетную обработку и параллелизацию обучения. В РСН каждый элемент последовательности необходимо обрабатывать в последовательном порядке, что означает, что это не может быть легко параллелизировано.

![Кодировщик Декодер с вниманием](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Рисунок из [Блога Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

Применение механизмов внимания в сочетании с этим ограничением привело к созданию современных трансформерных моделей, которые мы знаем и используем сегодня, таких как BERT и Open-GPT3.

## Модели трансформеров

Одна из основных идей трансформеров — избежать последовательной природы РСН и создать модель, которая может быть параллелизована во время обучения. Это достигается за счет реализации двух идей:

* позиционное кодирование
* использование механизма самовнимания для захвата паттернов вместо РСН (или ССН) (поэтому статья, вводящая трансформеры, называется *[Внимание — это всё, что вам нужно](https://arxiv.org/abs/1706.03762)*)

### Позиционное кодирование/встраивание

Идея позиционного кодирования следующая. 
1. При использовании РСН относительное положение токенов представляется количеством шагов и, следовательно, не требует явного представления. 
2. Однако, как только мы переходим к вниманию, нам необходимо знать относительные позиции токенов в последовательности. 
3. Чтобы получить позиционное кодирование, мы дополняем нашу последовательность токенов последовательностью позиций токенов в последовательности (т.е. последовательностью чисел 0, 1, ...).
4. Затем мы смешиваем позицию токена с вектором встраивания токена. Чтобы преобразовать позицию (целое число) в вектор, мы можем использовать различные подходы:

* Обучаемое встраивание, аналогичное встраиванию токенов. Это подход, который мы рассматриваем здесь. Мы применяем слои встраивания как к токенам, так и к их позициям, в результате чего получаем векторные встраивания одинаковых размерностей, которые затем складываем.
* Функция фиксированного позиционного кодирования, предложенная в оригинальной статье.

<img src="images/pos-embedding.png" width="50%"/>

> Изображение автора

Результат, который мы получаем с помощью позиционного встраивания, включает как оригинальный токен, так и его позицию в последовательности.

### Многоголовое самовнимание

Далее нам нужно захватить некоторые паттерны в нашей последовательности. Для этого трансформеры используют механизм **самовнимания**, который по сути представляет собой внимание, примененное к той же последовательности как входной и выходной. Применение самовнимания позволяет нам учитывать **контекст** в предложении и видеть, какие слова взаимосвязаны. Например, это позволяет нам видеть, какие слова упоминаются через копреференции, такие как *это*, и также учитывать контекст:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.ru.png)

> Изображение из [Блога Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

В трансформерах мы используем **многоголовое внимание**, чтобы дать сети возможность захватывать несколько различных типов зависимостей, например, долгосрочные и краткосрочные отношения между словами, копреференции и что-то еще и т.д.

[TensorFlow Notebook](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) содержит больше деталей о реализации слоев трансформеров.

### Внимание кодировщика-декодировщика

В трансформерах внимание используется в двух местах:

* Для захвата паттернов во входном тексте с помощью самовнимания
* Для выполнения перевода последовательностей — это слой внимания между кодировщиком и декодировщиком.

Внимание кодировщика-декодировщика очень похоже на механизм внимания, используемый в РСН, как описано в начале этого раздела. Эта анимационная диаграмма объясняет роль внимания кодировщика-декодировщика.

![Анимированное GIF, показывающее, как выполняются оценки в моделях трансформеров.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Поскольку каждая входная позиция сопоставляется независимо с каждой выходной позицией, трансформеры могут лучше параллелизироваться, чем РСН, что позволяет создавать гораздо более крупные и выразительные языковые модели. Каждая голова внимания может использоваться для изучения различных отношений между словами, что улучшает последующие задачи обработки естественного языка.

## BERT

**BERT** (Bidirectional Encoder Representations from Transformers) — это очень большая многослойная трансформерная сеть с 12 слоями для *BERT-base* и 24 слоями для *BERT-large*. Модель сначала обучается на большом корпусе текстовых данных (WikiPedia + книги) с использованием неконтролируемого обучения (предсказание замаскированных слов в предложении). Во время предварительного обучения модель поглощает значительные уровни языкового понимания, которые затем могут быть использованы с другими наборами данных с помощью тонкой настройки. Этот процесс называется **переносным обучением**.

![изображение с http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.ru.png)

> Изображение [источник](http://jalammar.github.io/illustrated-bert/)

## ✍️ Упражнения: Трансформеры

Продолжайте обучение в следующих ноутбуках:

* [Трансформеры в PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Трансформеры в TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Заключение

На этом уроке вы узнали о трансформерах и механизмах внимания — всех необходимых инструментах в арсенале NLP. Существует множество вариантов архитектуры трансформеров, включая BERT, DistilBERT, BigBird, OpenGPT3 и другие, которые могут быть тонко настроены. Пакет [HuggingFace](https://github.com/huggingface/) предоставляет репозиторий для обучения многих из этих архитектур как с PyTorch, так и с TensorFlow.

## 🚀 Вызов

## [Викторина после лекции](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Обзор и самостоятельное изучение

* [Блог пост](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), объясняющий классическую статью [Внимание — это всё, что вам нужно](https://arxiv.org/abs/1706.03762) о трансформерах.
* [Серия блогов](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) о трансформерах, объясняющая архитектуру в деталях.

## [Задание](assignment.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием машинных переводческих услуг на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования этого перевода.