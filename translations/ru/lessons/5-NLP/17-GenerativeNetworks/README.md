# Генеративные сети

## [Викторина перед лекцией](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Рекуррентные нейронные сети (RNN) и их варианты с затвором, такие как ячейки долгосрочной и краткосрочной памяти (LSTM) и затворные рекуррентные единицы (GRU), предоставили механизм для языкового моделирования, так как они могут изучать порядок слов и делать предсказания для следующего слова в последовательности. Это позволяет нам использовать RNN для **генеративных задач**, таких как обычная генерация текста, машинный перевод и даже создание подписей к изображениям.

> ✅ Подумайте о том, сколько раз вы извлекали пользу из генеративных задач, таких как автозаполнение текста во время набора. Проведите исследование своих любимых приложений, чтобы узнать, использовали ли они RNN.

В архитектуре RNN, обсужденной в предыдущем блоке, каждая единица RNN производила следующее скрытое состояние в качестве вывода. Однако мы также можем добавить еще один выход к каждой рекуррентной единице, что позволит нам выдавать **последовательность** (которая равна по длине исходной последовательности). Более того, мы можем использовать единицы RNN, которые не принимают вход на каждом шаге, а просто берут некоторый начальный вектор состояния и затем производят последовательность выходов.

Это позволяет создавать различные нейронные архитектуры, которые показаны на рисунке ниже:

![Изображение, показывающее общие шаблоны рекуррентных нейронных сетей.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.ru.jpg)

> Изображение из блога [Непомерная эффективность рекуррентных нейронных сетей](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) автора [Андрея Карпатого](http://karpathy.github.io/)

* **Один-к-одному** — это традиционная нейронная сеть с одним входом и одним выходом
* **Один-ко-многим** — это генеративная архитектура, которая принимает одно входное значение и генерирует последовательность выходных значений. Например, если мы хотим обучить сеть **создания подписей к изображениям**, которая будет производить текстовое описание изображения, мы можем использовать изображение в качестве входа, пропустить его через CNN, чтобы получить его скрытое состояние, а затем рекуррентная цепочка будет генерировать подпись слово за словом
* **Много-к-одному** соответствует архитектурам RNN, которые мы описали в предыдущем блоке, таким как классификация текста
* **Много-к-многим**, или **последовательность-к-последовательности**, соответствует задачам, таким как **машинный перевод**, где первая RNN собирает всю информацию из входной последовательности в скрытом состоянии, а другая цепочка RNN разворачивает это состояние в выходную последовательность.

В этом блоке мы сосредоточимся на простых генеративных моделях, которые помогают нам генерировать текст. Для простоты мы будем использовать токенизацию на уровне символов.

Мы будем обучать эту RNN для генерации текста шаг за шагом. На каждом шаге мы будем брать последовательность символов длиной `nchars` и просить сеть сгенерировать следующий выходной символ для каждого входного символа:

![Изображение, показывающее пример генерации RNN слова 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.ru.png)

При генерации текста (во время вывода) мы начинаем с некоторого **подсказки**, которая проходит через ячейки RNN для генерации своего промежуточного состояния, а затем от этого состояния начинается генерация. Мы генерируем по одному символу за раз и передаем состояние и сгенерированный символ другой ячейке RNN для генерации следующего, пока не сгенерируем достаточное количество символов.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Изображение от автора

## ✍️ Упражнения: Генеративные сети

Продолжайте свое обучение в следующих блокнотах:

* [Генеративные сети с PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Генеративные сети с TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Мягкая генерация текста и температура

Выход каждой ячейки RNN — это вероятностное распределение символов. Если мы всегда берем символ с наивысшей вероятностью в качестве следующего символа в сгенерированном тексте, текст часто может "циклически" повторяться между одними и теми же последовательностями символов снова и снова, как в этом примере:

```
today of the second the company and a second the company ...
```

Однако, если мы посмотрим на вероятностное распределение для следующего символа, может оказаться, что разница между несколькими наивысшими вероятностями не велика, например, один символ может иметь вероятность 0.2, другой - 0.19 и так далее. Например, когда мы ищем следующий символ в последовательности '*play*', следующим символом может быть как пробел, так и **e** (как в слове *player*).

Это приводит нас к выводу, что не всегда "справедливо" выбирать символ с более высокой вероятностью, потому что выбор второго по величине все равно может привести нас к осмысленному тексту. Более разумно **выбирать** символы из вероятностного распределения, заданного выходом сети. Мы также можем использовать параметр, **температуру**, который будет сглаживать вероятностное распределение, если мы хотим добавить больше случайности, или делать его более крутым, если мы хотим больше придерживаться символов с наивысшей вероятностью.

Изучите, как эта мягкая генерация текста реализована в вышеупомянутых блокнотах.

## Заключение

Хотя генерация текста может быть полезной сама по себе, основные преимущества заключаются в способности генерировать текст с помощью RNN из некоторого начального векторного признака. Например, генерация текста используется как часть машинного перевода (последовательность-к-последовательности, в этом случае вектор состояния из *кодировщика* используется для генерации или *декодирования* переведенного сообщения) или для создания текстового описания изображения (в этом случае вектор признаков будет поступать от извлекателя CNN).

## 🚀 Задача

Пройдите несколько уроков на Microsoft Learn по этой теме

* Генерация текста с [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Викторина после лекции](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Обзор и самостоятельное изучение

Вот несколько статей для расширения ваших знаний

* Разные подходы к генерации текста с использованием цепи Маркова, LSTM и GPT-2: [блог](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Пример генерации текста в [документации Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Задание](lab/README.md)

Мы увидели, как генерировать текст символ за символом. В лаборатории вы исследуете генерацию текста на уровне слов.

**Отказ от ответственности**:  
Этот документ был переведен с использованием услуг машинного перевода на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования этого перевода.