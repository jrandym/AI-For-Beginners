# Мультимодальные сети

После успеха трансформерных моделей для решения задач обработки естественного языка (NLP), аналогичные архитектуры были применены к задачам компьютерного зрения. Возрастает интерес к созданию моделей, которые будут *объединять* возможности зрения и естественного языка. Одной из таких попыток является модель от OpenAI, названная CLIP и DALL.E.

## Контрастная предобучение изображений (CLIP)

Основная идея CLIP заключается в том, чтобы иметь возможность сравнивать текстовые подсказки с изображением и определять, насколько хорошо изображение соответствует подсказке.

![Архитектура CLIP](../../../../../translated_images/clip-arch.b3dbf20b4e8ed8be1c38e2bc6100fd3cc257c33cda4692b301be91f791b13ea7.ru.png)

> *Изображение из [этого блога](https://openai.com/blog/clip/)*

Модель обучается на изображениях, полученных из Интернета, и их подписях. Для каждой партии мы берем N пар (изображение, текст) и преобразуем их в некоторые векторные представления I и T. Эти представления затем сопоставляются друг с другом. Функция потерь определяется для максимизации косинусного сходства между векторами, соответствующими одной паре (например, I и T), и минимизации косинусного сходства между всеми другими парами. Именно поэтому этот подход называется **контрастным**.

Модель/библиотека CLIP доступна на [GitHub OpenAI](https://github.com/openai/CLIP). Подход описан в [этом блоге](https://openai.com/blog/clip/), а более подробно в [этой статье](https://arxiv.org/pdf/2103.00020.pdf).

После предобучения этой модели мы можем предоставить ей партию изображений и партию текстовых подсказок, и она вернет тензор с вероятностями. CLIP можно использовать для нескольких задач:

**Классификация изображений**

Предположим, нам нужно классифицировать изображения, например, между кошками, собаками и людьми. В этом случае мы можем предоставить модели изображение и серию текстовых подсказок: "*изображение кошки*", "*изображение собаки*", "*изображение человека*". В результирующем векторе из 3 вероятностей нам просто нужно выбрать индекс с наивысшим значением.

![CLIP для классификации изображений](../../../../../translated_images/clip-class.3af42ef0b2b19369a633df5f20ddf4f5a01d6c8ffa181e9d3a0572c19f919f72.ru.png)

> *Изображение из [этого блога](https://openai.com/blog/clip/)*

**Поиск изображений по тексту**

Мы также можем сделать наоборот. Если у нас есть коллекция изображений, мы можем передать эту коллекцию модели и текстовую подсказку - это даст нам изображение, которое наиболее похоже на данную подсказку.

## ✍️ Пример: [Использование CLIP для классификации изображений и поиска изображений](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb)

Откройте блокнот [Clip.ipynb](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb), чтобы увидеть CLIP в действии.

## Генерация изображений с VQGAN+ CLIP

CLIP также может быть использован для **генерации изображений** из текстовой подсказки. Для этого нам нужна **модель-генератор**, которая сможет генерировать изображения на основе некоторого векторного ввода. Одна из таких моделей называется [VQGAN](https://compvis.github.io/taming-transformers/) (векторно-квантованный GAN).

Основные идеи VQGAN, которые отличают его от обычного [GAN](../../4-ComputerVision/10-GANs/README.md), следующие:
* Использование авторегрессионной архитектуры трансформера для генерации последовательности визуальных частей, богатых контекстом, которые составляют изображение. Эти визуальные части, в свою очередь, обучаются с помощью [CNN](../../4-ComputerVision/07-ConvNets/README.md).
* Использование дискриминатора подизображений, который определяет, являются ли части изображения "реальными" или "фальшивыми" (в отличие от подхода "все или ничего" в традиционном GAN).

Узнайте больше о VQGAN на сайте [Taming Transformers](https://compvis.github.io/taming-transformers/).

Одно из важных отличий между VQGAN и традиционным GAN заключается в том, что последний может создавать приемлемое изображение из любого векторного ввода, в то время как VQGAN, скорее всего, создаст изображение, которое не будет согласованным. Таким образом, нам нужно дополнительно направлять процесс создания изображения, что можно сделать с помощью CLIP.

![Архитектура VQGAN+CLIP](../../../../../translated_images/vqgan.5027fe05051dfa3101950cfa930303f66e6478b9bd273e83766731796e462d9b.ru.png)

Чтобы сгенерировать изображение, соответствующее текстовой подсказке, мы начинаем с некоторого случайного векторного кодирования, который проходит через VQGAN для получения изображения. Затем CLIP используется для создания функции потерь, которая показывает, насколько хорошо изображение соответствует текстовой подсказке. Цель состоит в том, чтобы минимизировать эту потерю, используя обратное распространение для корректировки параметров входного вектора.

Отличная библиотека, реализующая VQGAN+CLIP, - это [Pixray](http://github.com/pixray/pixray).

![Изображение, созданное Pixray](../../../../../translated_images/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.2384968e9db8a0d09dc96de938b9f95bde8a7e1c721f48f286a7795bf16d56c7.ru.png) |  ![Изображение, созданное Pixray](../../../../../translated_images/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.e0b6495f210a439077e1c32cc8afdf714e634fe24dc78dc5aa45fd2f560b0ed5.ru.png) | ![Изображение, созданное Pixray](../../../../../translated_images/a_closeup_oil_portrait_of_old_male_teacher_of_math.5362e67aa7fc2683b9d36a613b364deb7454760cd39205623fc1e3938fa133c0.ru.png)
----|----|----
Изображение, сгенерированное по подсказке *портрет молодого мужчины-преподавателя литературы в акварельной технике с книгой* | Изображение, сгенерированное по подсказке *портрет молодой женщины-преподавателя информатики в масляной технике с компьютером* | Изображение, сгенерированное по подсказке *портрет пожилого мужчины-преподавателя математики перед доской*

> Изображения из коллекции **Искусственные Учителя** от [Дмитрия Сошникова](http://soshnikov.com)

## DALL-E
### [DALL-E 1](https://openai.com/research/dall-e)
DALL-E - это версия GPT-3, обученная генерировать изображения из подсказок. Она была обучена с использованием 12 миллиардов параметров.

В отличие от CLIP, DALL-E получает как текст, так и изображение в виде единого потока токенов для изображений и текста. Таким образом, из нескольких подсказок вы можете генерировать изображения на основе текста.

### [DALL-E 2](https://openai.com/dall-e-2)
Основное отличие между DALL-E 1 и 2 заключается в том, что она генерирует более реалистичные изображения и искусство.

Примеры генерации изображений с DALL-E:
![Изображение, созданное Pixray](../../../../../translated_images/DALL·E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.6c235e8271d9ed10ce985d86aeb241a58518958647973af136912116b9518fce.ru.png) |  ![Изображение, созданное Pixray](../../../../../translated_images/DALL·E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.f21dc4166340b6c8b4d1cb57efd1e22127407f9b28c9ac7afe11344065369e64.ru.png) | ![Изображение, созданное Pixray](../../../../../translated_images/DALL·E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.d331c2dfbdc3f7c46aa65c0809066f5e7ed4b49609cd259852e760df21051e4a.ru.png)
----|----|----
Изображение, сгенерированное по подсказке *портрет молодого мужчины-преподавателя литературы в акварельной технике с книгой* | Изображение, сгенерированное по подсказке *портрет молодой женщины-преподавателя информатики в масляной технике с компьютером* | Изображение, сгенерированное по подсказке *портрет пожилого мужчины-преподавателя математики перед доской*

## Ссылки

* Статья о VQGAN: [Укрощение трансформеров для синтеза изображений высокого разрешения](https://compvis.github.io/taming-transformers/paper/paper.pdf)
* Статья о CLIP: [Обучение переносимых визуальных моделей с использованием естественного языка](https://arxiv.org/pdf/2103.00020.pdf)

**Отказ от ответственности**:  
Этот документ был переведен с использованием машинных AI-сервисов перевода. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникающие в результате использования этого перевода.