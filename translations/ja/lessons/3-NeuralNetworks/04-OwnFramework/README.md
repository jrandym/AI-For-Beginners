# ニューラルネットワーク入門：多層パーセプトロン

前のセクションでは、最も単純なニューラルネットワークモデルである単層パーセプトロンについて学びました。これは線形の二クラス分類モデルです。

このセクションでは、このモデルをより柔軟なフレームワークに拡張し、以下のことを可能にします：

* 二クラスに加えて、**多クラス分類**を実行する
* 分類に加えて、**回帰問題**を解決する
* 線形に分離できないクラスを分ける

また、異なるニューラルネットワークアーキテクチャを構築できる独自のモジュラーPythonフレームワークも開発します。

## [講義前クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## 機械学習の定式化

機械学習の問題を定式化することから始めましょう。トレーニングデータセット **X** とラベル **Y** があるとします。そして、最も正確な予測を行うモデル *f* を構築する必要があります。予測の質は **損失関数** ℒ によって測定されます。以下の損失関数がよく使用されます：

* 回帰問題の場合、数値を予測する必要があるときは、**絶対誤差** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| や **二乗誤差** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> を使用できます。
* 分類の場合、**0-1損失**（これはモデルの**精度**と本質的に同じです）や **ロジスティック損失** を使用します。

単層パーセプトロンの場合、関数 *f* は線形関数 *f(x)=wx+b* として定義されました（ここで *w* は重み行列、*x* は入力特徴のベクトル、*b* はバイアスベクトルです）。異なるニューラルネットワークアーキテクチャの場合、この関数はより複雑な形を取ることができます。

> 分類の場合、ネットワークの出力として対応するクラスの確率を得ることがしばしば望まれます。任意の数を確率に変換するため（例えば、出力を正規化するために）、私たちはしばしば **ソフトマックス** 関数 σ を使用し、関数 *f* は *f(x)=σ(wx+b)* になります。

上記の *f* の定義では、*w* と *b* は **パラメータ** θ=⟨*w,b*⟩ と呼ばれます。データセット ⟨**X**,**Y**⟩ が与えられたとき、全データセットに対する全体の誤差をパラメータ θ の関数として計算できます。

> ✅ **ニューラルネットワークのトレーニングの目標は、パラメータ θ を変化させることによって誤差を最小化することです。**

## 勾配降下法最適化

**勾配降下法** と呼ばれる関数最適化のよく知られた方法があります。このアイデアは、損失関数のパラメータに関する導関数（多次元の場合は **勾配** と呼ばれます）を計算し、誤差が減少するようにパラメータを変化させることができるというものです。これを次のように定式化できます：

* パラメータをいくつかのランダムな値 w<sup>(0)</sup>、b<sup>(0)</sup> で初期化する
* 次のステップを何度も繰り返す：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

トレーニング中、最適化ステップは全データセットを考慮して計算されることになっています（損失はすべてのトレーニングサンプルを通して合計されることを思い出してください）。しかし、実際には **ミニバッチ** と呼ばれるデータセットの小さな部分を取り、データのサブセットに基づいて勾配を計算します。サブセットは毎回ランダムに取得されるため、この方法は **確率的勾配降下法**（SGD）と呼ばれます。

## 多層パーセプトロンとバックプロパゲーション

上記で見たように、単層ネットワークは線形に分離可能なクラスを分類することができます。より豊かなモデルを構築するために、ネットワークのいくつかの層を組み合わせることができます。数学的には、関数 *f* はより複雑な形を持ち、いくつかのステップで計算されることになります：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

ここで、α は **非線形活性化関数**、σ はソフトマックス関数であり、パラメータは θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>* です。

勾配降下法アルゴリズムは同じままですが、勾配を計算するのが難しくなります。連鎖微分法則に基づいて、次のように導関数を計算できます：

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 連鎖微分法則は、パラメータに関する損失関数の導関数を計算するために使用されます。

これらの表現の最も左側の部分は同じであり、したがって、損失関数から計算グラフを「逆方向」にたどって導関数を効果的に計算できます。このように、多層パーセプトロンのトレーニング方法は **バックプロパゲーション**、または「バックプロップ」と呼ばれます。

<img alt="計算グラフ" src="images/ComputeGraphGrad.png"/>

> TODO: 画像の引用

> ✅ バックプロパゲーションについては、ノートブックの例でより詳細に説明します。  

## 結論

このレッスンでは、独自のニューラルネットワークライブラリを構築し、単純な二次元分類タスクにそれを使用しました。

## 🚀 チャレンジ

付随するノートブックでは、あなた自身の多層パーセプトロンを構築し、トレーニングするためのフレームワークを実装します。現代のニューラルネットワークがどのように機能するかを詳細に見ることができます。

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) ノートブックに進み、作業を進めてください。

## [講義後クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## 復習と自己学習

バックプロパゲーションはAIや機械学習で一般的に使用されるアルゴリズムであり、[詳細に学ぶ価値があります](https://wikipedia.org/wiki/Backpropagation)。

## [課題](lab/README.md)

このラボでは、このレッスンで構築したフレームワークを使用して、MNIST手書き数字分類を解決することが求められます。

* [指示](lab/README.md)
* [ノートブック](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**免責事項**:  
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確さが含まれる可能性があります。元の文書は、その母国語での権威ある情報源と見なされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用によって生じた誤解や誤訳については、一切の責任を負いません。