# 再帰型ニューラルネットワーク

## [講義前クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

前のセクションでは、テキストの豊かな意味表現を使用し、その上にシンプルな線形分類器を置いてきました。このアーキテクチャの目的は、文中の単語の集約された意味を捉えることですが、単語の**順序**を考慮していません。なぜなら、埋め込みの上での集約操作が元のテキストからこの情報を取り除いてしまったからです。これらのモデルは単語の順序をモデル化できないため、テキスト生成や質問応答のような、より複雑または曖昧なタスクを解決できません。

テキストシーケンスの意味を捉えるためには、**再帰型ニューラルネットワーク**（RNN）という別のニューラルネットワークアーキテクチャを使用する必要があります。RNNでは、文を1つのシンボルずつネットワークに通し、ネットワークは何らかの**状態**を生成し、その後次のシンボルと共に再びネットワークに渡します。

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ja.png)

> 著者による画像

入力トークンのシーケンス X<sub>0</sub>,...,X<sub>n</sub> に対して、RNNはニューラルネットワークブロックのシーケンスを作成し、このシーケンスをバックプロパゲーションを使用してエンドツーエンドでトレーニングします。各ネットワークブロックはペア (X<sub>i</sub>,S<sub>i</sub>) を入力として受け取り、結果として S<sub>i+1</sub> を生成します。最終的な状態 S<sub>n</sub> または（出力 Y<sub>n</sub>）は、結果を生成するために線形分類器に入ります。すべてのネットワークブロックは同じ重みを共有し、1回のバックプロパゲーションパスを使用してエンドツーエンドでトレーニングされます。

状態ベクトル S<sub>0</sub>,...,S<sub>n</sub> がネットワークを通過するため、単語間の順次依存関係を学習することができます。たとえば、シーケンスのどこかに単語 *not* が現れると、状態ベクトル内の特定の要素を否定することを学習できます。これにより否定が生じます。

> ✅ 上の画像のすべての RNN ブロックの重みが共有されているため、同じ画像は再帰的なフィードバックループを持つ1つのブロック（右側）として表現できます。このループは、ネットワークの出力状態を入力に戻します。

## RNNセルの構造

シンプルな RNN セルがどのように構成されているか見てみましょう。これは前の状態 S<sub>i-1</sub> と現在のシンボル X<sub>i</sub> を入力として受け取り、出力状態 S<sub>i</sub> を生成する必要があります（場合によっては、生成ネットワークの場合のように他の出力 Y<sub>i</sub> も興味があります）。

シンプルな RNN セルには2つの重み行列が内部にあります：1つは入力シンボルを変換し（これを W と呼びます）、もう1つは入力状態を変換します（H）。この場合、ネットワークの出力は σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) として計算され、ここで σ は活性化関数、b は追加のバイアスです。

<img alt="RNNセルの構造" src="images/rnn-anatomy.png" width="50%"/>

> 著者による画像

多くの場合、入力トークンは RNN に入る前に埋め込み層を通過して次元を下げます。この場合、入力ベクトルの次元が *emb_size* で、状態ベクトルが *hid_size* の場合、W のサイズは *emb_size*×*hid_size*、H のサイズは *hid_size*×*hid_size* です。

## 長短期記憶（LSTM）

古典的な RNN の主な問題の1つは、いわゆる **消失勾配** 問題です。RNN は1回のバックプロパゲーションパスでエンドツーエンドでトレーニングされるため、ネットワークの最初の層にエラーを伝播するのが難しく、したがってネットワークは遠くのトークン間の関係を学習できません。この問題を回避する方法の1つは、いわゆる **ゲート** を使用して **明示的な状態管理** を導入することです。この種のよく知られたアーキテクチャには、**長短期記憶**（LSTM）と **ゲーテッドリレー単位**（GRU）があります。

![長短期記憶セルの例を示す画像](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> 画像の出典 TBD

LSTM ネットワークは RNN と似た構造で組織されていますが、層から層へと渡される2つの状態があります：実際の状態 C と隠れベクトル H です。各ユニットでは、隠れベクトル H<sub>i</sub> が入力 X<sub>i</sub> と連結され、これらが **ゲート** を介して状態 C に何が起こるかを制御します。各ゲートはシグモイド活性化を持つニューラルネットワーク（出力範囲 [0,1]）であり、状態ベクトルと掛け算することでビットマスクのように考えることができます。上の画像の左から右にかけて次のゲートがあります：

* **忘却ゲート** は隠れベクトルを受け取り、ベクトル C のどの要素を忘れ、どの要素を通過させる必要があるかを決定します。
* **入力ゲート** は入力と隠れベクトルからの情報を受け取り、状態に挿入します。
* **出力ゲート** は状態を *tanh* 活性化を持つ線形層を介して変換し、その後隠れベクトル H<sub>i</sub> を使用していくつかの要素を選択し、新しい状態 C<sub>i+1</sub> を生成します。

状態 C の要素は、オンオフできるフラグのように考えることができます。たとえば、シーケンスの中で名前 *Alice* に遭遇した場合、これは女性キャラクターを指していると仮定し、文中に女性名詞があることを示すフラグを立てたいかもしれません。その後、*and Tom* というフレーズに遭遇すると、複数名詞があることを示すフラグを立てます。このように状態を操作することで、文の部分の文法的特性を追跡できると考えられます。

> ✅ LSTM の内部を理解するための優れたリソースは、Christopher Olah によるこの素晴らしい記事 [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) です。

## 双方向および多層 RNN

私たちは、シーケンスの始まりから終わりまでの1方向に操作する再帰型ネットワークについて議論しました。これは自然に見えます。なぜなら、私たちが読む方法や音声を聞く方法に似ているからです。しかし、多くの実用的なケースでは入力シーケンスにランダムにアクセスできるため、再帰計算を両方向で行うことが理にかなう場合があります。このようなネットワークは **双方向** RNN と呼ばれます。双方向ネットワークを扱う場合、各方向に対して2つの隠れ状態ベクトルが必要になります。

再帰型ネットワークは、単方向でも双方向でも、シーケンス内の特定のパターンを捉え、それらを状態ベクトルに保存したり、出力に渡したりします。畳み込みネットワークと同様に、最初の層から抽出された低レベルのパターンから高レベルのパターンを捉えるために、別の再帰層をその上に構築することができます。これにより、出力が次の層に入力として渡される2つ以上の再帰ネットワークから構成される **多層 RNN** の概念に至ります。

![多層長短期記憶 RNN の画像](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ja.jpg)

*画像は [この素晴らしい投稿](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) に由来しています。*

## ✍️ 演習：埋め込み

次のノートブックで学習を続けてください：

* [PyTorchによるRNN](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlowによるRNN](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## 結論

このユニットでは、RNN がシーケンス分類に使用できることを見てきましたが、実際にはテキスト生成、機械翻訳など、さらに多くのタスクを処理できます。次のユニットでは、これらのタスクについて考察します。

## 🚀 チャレンジ

LSTM に関する文献を読み、その応用を考えてみてください：

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [講義後クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## 復習と自己学習

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.

## [課題：ノートブック](assignment.md)

**免責事項**：  
この文書は、機械翻訳AIサービスを使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご理解ください。原文の母国語の文書が権威ある情報源と見なされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用から生じる誤解や誤解釈については、当社は責任を負いません。