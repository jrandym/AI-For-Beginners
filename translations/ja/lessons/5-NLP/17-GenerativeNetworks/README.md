# 生成ネットワーク

## [講義前のクイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

再帰神経ネットワーク（RNN）や、長短期記憶セル（LSTM）やゲート付き再帰ユニット（GRU）などのゲート付きセルのバリエーションは、言語モデリングのメカニズムを提供します。これにより、単語の順序を学習し、シーケンス内の次の単語の予測を行うことができます。これにより、RNNを使って、通常のテキスト生成、機械翻訳、さらには画像キャプション生成などの**生成タスク**に利用できるようになります。

> ✅ タイピング中にテキスト補完などの生成タスクから利益を得たことを思い出してみてください。お気に入りのアプリケーションがRNNを活用しているかどうか調査してみましょう。

前のユニットで議論したRNNアーキテクチャでは、各RNNユニットは次の隠れ状態を出力として生成しました。しかし、各再帰ユニットにもう一つの出力を追加することもでき、これにより**シーケンス**（元のシーケンスと同じ長さ）を出力できるようになります。さらに、各ステップで入力を受け取らず、初期状態ベクトルを受け取ってから出力のシーケンスを生成するRNNユニットを使用することもできます。

これにより、以下の図に示すような異なる神経アーキテクチャが可能になります。

![一般的な再帰神経ネットワークパターンを示す画像。](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.ja.jpg)

> 画像は[Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)のブログ投稿からのもので、[Andrej Karpaty](http://karpathy.github.io/)によるものです。

* **一対一**は、入力が1つと出力が1つの従来の神経ネットワークです。
* **一対多**は、1つの入力値を受け取り、出力値のシーケンスを生成する生成アーキテクチャです。たとえば、画像のテキスト説明を生成する**画像キャプション生成**ネットワークを訓練したい場合、画像を入力としてCNNを通して隠れ状態を取得し、再帰的なチェーンでキャプションを単語ごとに生成することができます。
* **多対一**は、前のユニットで説明したRNNアーキテクチャに対応し、テキスト分類などがあります。
* **多対多**、または**シーケンス対シーケンス**は、**機械翻訳**のようなタスクに対応し、最初のRNNが入力シーケンスから隠れ状態にすべての情報を集め、別のRNNチェーンがこの状態を出力シーケンスに展開します。

このユニットでは、テキスト生成を助けるシンプルな生成モデルに焦点を当てます。簡単のために、文字レベルのトークン化を使用します。

このRNNを段階的にテキスト生成するために訓練します。各ステップでは、長さ`nchars`の文字のシーケンスを取得し、ネットワークに対して各入力文字に対する次の出力文字を生成するように求めます：

![「HELLO」という単語のRNN生成の例を示す画像。](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.ja.png)

テキストを生成する際（推論中）、いくつかの**プロンプト**から始め、それをRNNセルを通して中間状態を生成し、その状態から生成が始まります。1文字ずつ生成し、状態と生成された文字を別のRNNセルに渡して次の文字を生成し、十分な文字を生成するまで続けます。

<img src="images/rnn-generate-inf.png" width="60%"/>

> 画像は著者によるものです。

## ✍️ 演習: 生成ネットワーク

以下のノートブックで学習を続けてください：

* [PyTorchによる生成ネットワーク](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlowによる生成ネットワーク](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## ソフトテキスト生成と温度

各RNNセルの出力は文字の確率分布です。常に最も高い確率の文字を生成されたテキストの次の文字として選択すると、テキストはしばしば同じ文字シーケンスの間で「循環」してしまうことがあります。例えば、次のような場合です：

```
today of the second the company and a second the company ...
```

しかし、次の文字の確率分布を見ると、最も高い確率のいくつかの間の差がそれほど大きくない場合があります。例えば、ある文字が確率0.2、別の文字が0.19などです。例えば、シーケンス'*play*'の次の文字を探すとき、次の文字はスペースでも**e**（*player*という単語のように）でも同様に可能です。

これは、常に高い確率の文字を選択することが「公平」ではないという結論につながります。なぜなら、2番目に高い確率の文字を選んでも意味のあるテキストに到達する可能性があるからです。ネットワークの出力によって与えられる確率分布から文字を**サンプリング**する方が賢明です。また、確率分布を平坦にする**温度**というパラメータを使用して、より多くのランダム性を追加したり、最も高い確率の文字により固執するために急峻にしたりすることができます。

このソフトテキスト生成が上記のノートブックでどのように実装されているかを探ってみてください。

## 結論

テキスト生成はそれ自体に有用かもしれませんが、主な利点は、初期の特徴ベクトルからRNNを使用してテキストを生成する能力にあります。例えば、テキスト生成は機械翻訳の一部として使用されます（この場合、*エンコーダ*からの状態ベクトルが翻訳されたメッセージを生成または*デコード*するために使用されます）や、画像のテキスト説明を生成する場合（この場合、特徴ベクトルはCNNエクストラクタから得られます）。

## 🚀 チャレンジ

このトピックに関するMicrosoft Learnのレッスンを受けてみてください。

* テキスト生成に関する[PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [講義後のクイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## 復習と自己学習

知識を広げるためのいくつかの記事を紹介します。

* マルコフ連鎖、LSTM、GPT-2によるテキスト生成の異なるアプローチ：[ブログ投稿](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Kerasドキュメント](https://keras.io/examples/generative/lstm_character_level_text_generation/)のテキスト生成サンプル

## [課題](lab/README.md)

文字単位でテキストを生成する方法を見てきました。ラボでは、単語単位のテキスト生成を探求します。

**免責事項**:  
この文書は、機械翻訳サービスを使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご理解ください。原文の母国語の文書が権威ある情報源と見なされるべきです。重要な情報については、専門の人間翻訳を推奨します。この翻訳の使用に起因する誤解や誤訳について、当社は責任を負いません。