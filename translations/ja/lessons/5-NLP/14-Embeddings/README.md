# 埋め込み

## [講義前のクイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

BoWやTF/IDFに基づく分類器を訓練する際、私たちは長さ`vocab_size`の高次元の袋-単語ベクトルを扱い、低次元の位置表現ベクトルから疎なワンホット表現に明示的に変換していました。しかし、このワンホット表現はメモリ効率が良くありません。さらに、各単語は互いに独立して扱われるため、ワンホットエンコードされたベクトルは単語間の意味的類似性を表現することができません。

**埋め込み**のアイデアは、単語を意味を反映した低次元の密なベクトルで表現することです。後で意味のある単語埋め込みを構築する方法について議論しますが、今は埋め込みを単語ベクトルの次元を下げる方法として考えましょう。

したがって、埋め込み層は単語を入力として受け取り、指定された`embedding_size`の出力ベクトルを生成します。ある意味では、これは`Linear`層と非常に似ていますが、ワンホットエンコードされたベクトルを受け取るのではなく、単語番号を入力として受け取ることができるため、大きなワンホットエンコードベクトルを作成する必要がなくなります。

分類器ネットワークの最初の層として埋め込み層を使用することで、私たちは袋-単語から**埋め込み袋**モデルに切り替えることができ、テキスト内の各単語を対応する埋め込みに変換し、`sum`、`average`、または`max`などの集約関数をこれらの埋め込み全体に対して計算します。  

![五つの連続した単語の埋め込み分類器を示す画像。](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ja.png)

> 画像提供者：著者

## ✍️ 演習：埋め込み

次のノートブックで学習を続けましょう：
* [PyTorchによる埋め込み](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlowによる埋め込み](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## 意味的埋め込み：Word2Vec

埋め込み層が単語をベクトル表現にマッピングすることを学習している間、この表現には必ずしも多くの意味的な意味があるわけではありません。似たような単語や同義語が、あるベクトル距離（例えば、ユークリッド距離）の観点で互いに近いベクトルに対応するようなベクトル表現を学習できれば良いでしょう。

そのためには、特定の方法で大規模なテキストコレクションで埋め込みモデルを事前学習する必要があります。意味的埋め込みを訓練する方法の一つは[Word2Vec](https://en.wikipedia.org/wiki/Word2vec)と呼ばれています。これは、単語の分散表現を生成するために使用される2つの主要なアーキテクチャに基づいています：

 - **連続袋-単語**（CBoW） — このアーキテクチャでは、モデルは周囲の文脈から単語を予測するように訓練されます。ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$が与えられた場合、モデルの目標は$(W_{-2},W_{-1},W_1,W_2)$から$W_0$を予測することです。
 - **連続スキップグラム**はCBoWの逆です。モデルは周囲の文脈単語のウィンドウを使用して現在の単語を予測します。

CBoWは速いですが、スキップグラムは遅いですが、稀な単語をより良く表現することができます。

![単語をベクトルに変換するためのCBoWとスキップグラムアルゴリズムの両方を示す画像。](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ja.png)

> 画像出典：[この論文](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vecの事前学習済み埋め込み（およびGloVeなどの他の類似モデル）もニューラルネットワークの埋め込み層の代わりに使用できます。ただし、Word2Vec/GloVeの事前学習に使用された語彙は、私たちのテキストコーパスの語彙と異なる可能性があるため、語彙に対処する必要があります。この問題を解決する方法については、上記のノートブックを参照してください。

## コンテキスト埋め込み

Word2Vecのような従来の事前学習済み埋め込み表現の主な制限の一つは、単語の意味の曖昧性の問題です。事前学習済みの埋め込みは、文脈内の単語の意味の一部を捉えることができますが、単語の可能なすべての意味が同じ埋め込みにエンコードされています。これは、'play'のように文脈によって異なる意味を持つ多くの単語があるため、下流のモデルに問題を引き起こす可能性があります。

例えば、'play'という単語は以下の2つの異なる文でかなり異なる意味を持っています：

- 私は劇場で**演劇**を見に行きました。
- ジョンは友達と**遊びたい**と思っています。

上記の事前学習済み埋め込みは、これらの'play'の意味を同じ埋め込みで表現します。この制限を克服するためには、**言語モデル**に基づいた埋め込みを構築する必要があります。これは、大規模なテキストコーパスで訓練され、単語が異なる文脈でどのように組み合わされるかを*知っている*モデルです。コンテキスト埋め込みについての議論はこのチュートリアルの範囲外ですが、コースの後半で言語モデルについて話す際に再度触れる予定です。

## 結論

このレッスンでは、単語の意味をよりよく反映するために、TensorFlowとPytorchで埋め込み層を構築および使用する方法を学びました。

## 🚀 チャレンジ

Word2Vecは、歌詞や詩を生成するなどの興味深いアプリケーションに使用されています。著者がWord2Vecを使用して詩を生成する方法を紹介している[この記事](https://www.politetype.com/blog/word2vec-color-poems)を見てみてください。また、[Dan Shiffmannによるこの動画](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain)も見て、別の説明を発見してください。次に、これらの技術をKaggleなどから取得した自分のテキストコーパスに適用してみてください。

## [講義後のクイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## レビューと自己学習

Word2Vecに関するこの論文を読んでください：[ベクトル空間における単語表現の効率的推定](https://arxiv.org/pdf/1301.3781.pdf)

## [課題：ノートブック](assignment.md)

**免責事項**:  
この文書は、機械翻訳AIサービスを使用して翻訳されています。正確性を追求していますが、自動翻訳にはエラーや不正確さが含まれる可能性があることにご留意ください。元の文書はその母国語での権威ある情報源と見なされるべきです。重要な情報については、専門の人間翻訳をお勧めします。この翻訳の使用から生じる誤解や誤訳について、当社は一切の責任を負いません。