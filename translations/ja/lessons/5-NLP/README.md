# 自然言語処理

![NLPタスクの要約](../../../../translated_images/ai-nlp.b22dcb8ca4707ceaee8576db1c5f4089c8cac2f454e9e03ea554f07fda4556b8.ja.png)

このセクションでは、**自然言語処理 (NLP)** に関連するタスクを扱うためにニューラルネットワークを使用することに焦点を当てます。コンピュータに解決してもらいたいNLPの問題はたくさんあります：

* **テキスト分類**は、テキストシーケンスに関する典型的な分類問題です。例えば、メールメッセージをスパムか非スパムに分類したり、記事をスポーツ、ビジネス、政治などに分類したりします。また、チャットボットを開発する際には、ユーザーが言いたかったことを理解する必要があることが多く、この場合は**意図分類**を扱っています。意図分類では、多くのカテゴリを扱う必要があることがよくあります。
* **感情分析**は、典型的な回帰問題であり、文の意味がどれだけポジティブ/ネガティブであるかに対応する数値（感情）を割り当てる必要があります。感情分析のより高度なバージョンは、**アスペクトベースの感情分析** (ABSA) であり、文全体ではなくその一部（アスペクト）に感情を割り当てます。例えば、*このレストランでは、料理が気に入りましたが、雰囲気はひどかったです*のように。
* **固有表現認識** (NER) は、テキストから特定のエンティティを抽出する問題を指します。例えば、*明日パリに飛ばなければならない*というフレーズの中で、*明日*がDATEを指し、*パリ*がLOCATIONであることを理解する必要があります。
* **キーワード抽出**はNERに似ていますが、特定のエンティティタイプのために事前学習することなく、文の意味に重要な単語を自動的に抽出する必要があります。
* **テキストクラスタリング**は、似たような文をグループ化したいときに役立ちます。例えば、技術サポートの会話における似たリクエストをまとめることができます。
* **質問応答**は、モデルが特定の質問に答える能力を指します。モデルはテキストのパッセージと質問を入力として受け取り、質問の答えが含まれているテキストの場所を提供する必要があります（あるいは、時には答えのテキストを生成することもあります）。
* **テキスト生成**は、モデルが新しいテキストを生成する能力です。これは、ある*テキストプロンプト*に基づいて次の文字や単語を予測する分類タスクと考えられます。GPT-3のような高度なテキスト生成モデルは、[プロンプトプログラミング](https://towardsdatascience.com/software-3-0-how-prompting-will-change-the-rules-of-the-game-a982fbfe1e0)や[プロンプトエンジニアリング](https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29)と呼ばれる技術を使用して、分類などの他のNLPタスクを解決することができます。
* **テキスト要約**は、コンピュータに長いテキストを「読む」ことを要求し、それを数文で要約する技術です。
* **機械翻訳**は、一つの言語でのテキスト理解と、別の言語でのテキスト生成の組み合わせと見なすことができます。

当初、ほとんどのNLPタスクは文法のような従来の手法を使用して解決されていました。例えば、機械翻訳では、パーサーを使用して初期の文を構文木に変換し、その後、文の意味を表現するために高次の意味構造を抽出し、この意味とターゲット言語の文法に基づいて結果が生成されました。最近では、多くのNLPタスクがニューラルネットワークを使用してより効果的に解決されています。

> 多くの古典的なNLP手法は、[自然言語処理ツールキット (NLTK)](https://www.nltk.org) Pythonライブラリに実装されています。異なるNLPタスクをNLTKを使用して解決する方法をカバーした素晴らしい[NLTK Book](https://www.nltk.org/book/)がオンラインで利用可能です。

私たちのコースでは、主にNLPのためにニューラルネットワークを使用することに焦点を当て、必要に応じてNLTKを使用します。

私たちはすでに、タブularデータや画像を扱うためのニューラルネットワークの使用について学びました。これらのデータタイプとテキストの主な違いは、テキストが可変長のシーケンスであるのに対し、画像の場合は入力サイズが事前に知られていることです。畳み込みネットワークは入力データからパターンを抽出できますが、テキストのパターンはより複雑です。例えば、否定が主語から分離されることが多くの単語にとって任意である場合（例：*私はオレンジが好きではありません* vs. *私はあの大きくてカラフルで美味しいオレンジが好きではありません*）でも、これは依然として一つのパターンとして解釈されるべきです。したがって、言語を扱うためには、*再帰ネットワーク*や*トランスフォーマー*などの新しいニューラルネットワークのタイプを導入する必要があります。

## ライブラリのインストール

このコースを実行するためにローカルのPythonインストールを使用している場合は、次のコマンドを使用してNLPに必要なすべてのライブラリをインストールする必要があります：

**PyTorchの場合**
```bash
pip install -r requirements-torch.txt
```
**TensorFlowの場合**
```bash
pip install -r requirements-tf.txt
```

> TensorFlowを使ってNLPを試してみたい方は、[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/?WT.mc_id=academic-77998-cacaste)をご覧ください。

## GPUの警告

このセクションでは、いくつかの例でかなり大きなモデルをトレーニングします。
* **GPU対応コンピュータの使用**: 大きなモデルで作業する際の待機時間を短縮するために、GPU対応コンピュータでノートブックを実行することをお勧めします。
* **GPUメモリの制約**: GPUで実行すると、特に大きなモデルをトレーニングする際に、GPUメモリが不足する状況が発生する可能性があります。
* **GPUメモリの消費**: トレーニング中に消費されるGPUメモリの量は、ミニバッチサイズを含むさまざまな要因によって異なります。
* **ミニバッチサイズの最小化**: GPUメモリの問題が発生した場合は、コード内のミニバッチサイズを減少させることを検討してください。
* **TensorFlowのGPUメモリ解放**: 古いバージョンのTensorFlowは、1つのPythonカーネル内で複数のモデルをトレーニングする際にGPUメモリを正しく解放しない場合があります。GPUメモリの使用を効果的に管理するために、必要に応じてGPUメモリを割り当てるようにTensorFlowを設定できます。
* **コードの追加**: TensorFlowが必要なときにのみGPUメモリの割り当てを増やすように設定するには、ノートブックに次のコードを含めてください：

```python
physical_devices = tf.config.list_physical_devices('GPU') 
if len(physical_devices)>0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True) 
```

古典的なMLの観点からNLPを学ぶことに興味がある場合は、[このレッスンのスイート](https://github.com/microsoft/ML-For-Beginners/tree/main/6-NLP)を訪れてください。

## このセクションでは
このセクションでは、以下について学びます：

* [テキストをテンソルとして表現する](13-TextRep/README.md)
* [単語埋め込み](14-Emdeddings/README.md)
* [言語モデル](15-LanguageModeling/README.md)
* [再帰型ニューラルネットワーク](16-RNN/README.md)
* [生成ネットワーク](17-GenerativeNetworks/README.md)
* [トランスフォーマー](18-Transformers/README.md)

**免責事項**:  
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。正確さを追求していますが、自動翻訳には誤りや不正確さが含まれる可能性があることにご留意ください。原文のネイティブ言語の文書を権威ある情報源と見なすべきです。重要な情報については、専門の人間翻訳をお勧めします。この翻訳の使用に起因する誤解や誤訳について、当社は一切の責任を負いません。