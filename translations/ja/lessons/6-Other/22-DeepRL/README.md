# ディープ強化学習

強化学習 (RL) は、教師あり学習や教師なし学習と並ぶ基本的な機械学習のパラダイムの一つと見なされています。教師あり学習では、結果が既知のデータセットに依存しますが、RLは**実践による学習**に基づいています。例えば、コンピュータゲームを初めて見たとき、ルールを知らなくてもプレイを始め、すぐにプレイしながら行動を調整することでスキルを向上させることができます。

## [事前講義クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RLを実行するには、以下が必要です：

* ゲームのルールを設定する**環境**または**シミュレーター**。シミュレーターで実験を行い、その結果を観察できる必要があります。
* 実験がどれだけ成功したかを示す**報酬関数**。コンピュータゲームをプレイする場合、報酬は最終スコアになります。

報酬関数に基づいて、行動を調整しスキルを向上させることができ、次回はより良いプレイができるようになります。他のタイプの機械学習とRLの主な違いは、RLではゲームが終了するまで勝ったか負けたかを通常知ることができない点です。したがって、特定の動きが良いかどうかを単独で判断することはできず、ゲームの最後にのみ報酬を受け取ります。

RLの間、通常は多くの実験を行います。各実験では、これまで学習した最適な戦略に従うこと（**利用**）と、新しい可能な状態を探索すること（**探索**）のバランスを取る必要があります。

## OpenAI Gym

RLにとって素晴らしいツールは、[OpenAI Gym](https://gym.openai.com/)です。これは、アタリゲームからポールバランシングの物理学まで、さまざまな環境をシミュレートできる**シミュレーション環境**です。強化学習アルゴリズムのトレーニングに最も人気のあるシミュレーション環境の一つであり、[OpenAI](https://openai.com/)によって維持されています。

> **注意**: OpenAI Gymで利用可能なすべての環境は[こちら](https://gym.openai.com/envs/#classic_control)で確認できます。

## カートポールバランシング

現代のバランスデバイス、例えば*セグウェイ*や*ジャイロスクーター*を見たことがあるでしょう。これらは、加速度センサーやジャイロスコープからの信号に応じて車輪を調整することで自動的にバランスを取ることができます。このセクションでは、ポールをバランスさせるという似たような問題を解決する方法を学びます。これは、サーカスのパフォーマーが手の上でポールをバランスさせる状況に似ていますが、このポールバランシングは1次元でのみ行われます。

バランシングの簡略化されたバージョンは**カートポール**問題として知られています。カートポールの世界では、左右に動くことができる水平スライダーがあり、目標はスライダーの上に垂直のポールをバランスさせることです。

<img alt="カートポール" src="images/cartpole.png" width="200"/>

この環境を作成して使用するには、いくつかの行のPythonコードが必要です：

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

各環境には、まったく同じ方法でアクセスできます：
* `env.reset` starts a new experiment
* `env.step`はシミュレーションステップを実行します。これは**アクション**を**アクションスペース**から受け取り、**観測**（観測スペースから）を返し、報酬と終了フラグも返します。

上記の例では、各ステップでランダムなアクションを実行しているため、実験の寿命は非常に短くなっています：

![バランスしていないカートポール](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RLアルゴリズムの目標は、特定の状態に応じてアクションを返すモデル、いわゆる**ポリシー**πを訓練することです。また、ポリシーを確率的に考えることもできます。例えば、任意の状態*s*とアクション*a*に対して、状態*s*で*a*を取る確率π(*a*|*s*)を返すとします。

## ポリシー勾配アルゴリズム

ポリシーをモデル化する最も明白な方法は、状態を入力として受け取り、対応するアクション（またはすべてのアクションの確率）を返すニューラルネットワークを作成することです。ある意味で、これは通常の分類タスクに似ていますが、主な違いは、各ステップでどのアクションを取るべきかを事前に知ることができない点です。

ここでのアイデアは、これらの確率を推定することです。実験の各ステップでの総報酬を示す**累積報酬**のベクトルを構築します。また、早期の報酬の役割を減少させるために、早期の報酬に係数γ=0.99を掛けて**報酬の割引**を適用します。その後、より大きな報酬をもたらす実験経路に沿ったステップを強化します。

> ポリシー勾配アルゴリズムについて詳しく学び、[例のノートブック](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)でその動作を確認してください。

## アクター-クリティックアルゴリズム

ポリシー勾配アプローチの改善版は**アクター-クリティック**と呼ばれています。その主なアイデアは、ニューラルネットワークが2つのことを返すように訓練されることです：

* どのアクションを取るかを決定するポリシー。この部分は**アクター**と呼ばれます。
* この状態で期待できる総報酬の推定。この部分は**クリティック**と呼ばれます。

ある意味で、このアーキテクチャは[GAN](../../4-ComputerVision/10-GANs/README.md)に似ており、互いに対抗して訓練される2つのネットワークがあります。アクター-クリティックモデルでは、アクターが取るべきアクションを提案し、クリティックが結果を批判的に評価しようとします。しかし、私たちの目標は、これらのネットワークを協調して訓練することです。

実験中に実際の累積報酬とクリティックが返す結果の両方を知っているため、これらの違いを最小化する損失関数を構築することは比較的簡単です。これにより、**クリティック損失**が得られます。**アクター損失**は、ポリシー勾配アルゴリズムと同じアプローチを使用して計算できます。

これらのアルゴリズムのいずれかを実行した後、私たちのカートポールがこのように振る舞うことを期待できます：

![バランスの取れたカートポール](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ 演習: ポリシー勾配とアクター-クリティックRL

次のノートブックで学習を続けてください：

* [TensorFlowにおけるRL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorchにおけるRL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## その他のRLタスク

現在、強化学習は急成長している研究分野です。強化学習のいくつかの興味深い例は以下の通りです：

* コンピュータに**アタリゲーム**をプレイさせること。この問題の難しい点は、単純な状態をベクトルとして表すのではなく、スクリーンショットとして表す必要があることです。CNNを使用してこの画面画像を特徴ベクトルに変換したり、報酬情報を抽出する必要があります。アタリゲームはGymで利用可能です。
* コンピュータにチェスや囲碁などのボードゲームをプレイさせること。最近、**Alpha Zero**のような最先端のプログラムは、互いに対戦する2つのエージェントによってゼロから訓練され、各ステップで改善されました。
* 業界では、RLはシミュレーションから制御システムを作成するために使用されています。[Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste)というサービスは、そのために特別に設計されています。

## 結論

私たちは、エージェントを訓練して、ゲームの望ましい状態を定義する報酬関数を提供し、探索空間をインテリジェントに探索する機会を与えることで良好な結果を達成する方法を学びました。2つのアルゴリズムを成功裏に試し、比較的短期間で良好な結果を得ました。しかし、これはRLへの旅の始まりに過ぎず、さらに深く掘り下げたい場合は、別のコースを受講することを検討すべきです。

## 🚀 チャレンジ

「その他のRLタスク」セクションにリストされたアプリケーションを探索し、1つを実装してみてください！

## [講義後クイズ](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## レビュー & 自習

私たちの[初心者向け機械学習カリキュラム](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)で、古典的な強化学習についてさらに学んでください。

コンピュータがスーパーマリオをプレイする方法について語る[この素晴らしいビデオ](https://www.youtube.com/watch?v=qv6UVOQ0F44)を見てください。

## 課題: [マウンテンカーを訓練する](lab/README.md)

この課題の目標は、別のGym環境である[マウンテンカー](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)を訓練することです。

**免責事項**:  
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご理解ください。原文の母国語の文書が権威ある情報源と見なされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤訳について、当社は責任を負いません。