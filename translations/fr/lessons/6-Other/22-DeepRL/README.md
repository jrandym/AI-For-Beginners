# Apprentissage par Renforcement Profond

L'apprentissage par renforcement (RL) est consid√©r√© comme l'un des paradigmes fondamentaux de l'apprentissage machine, aux c√¥t√©s de l'apprentissage supervis√© et de l'apprentissage non supervis√©. Alors que dans l'apprentissage supervis√© nous nous appuyons sur un ensemble de donn√©es avec des r√©sultats connus, le RL repose sur **l'apprentissage par la pratique**. Par exemple, lorsque nous d√©couvrons un jeu vid√©o pour la premi√®re fois, nous commen√ßons √† jouer, m√™me sans conna√Ætre les r√®gles, et bient√¥t nous sommes capables d'am√©liorer nos comp√©tences simplement par le processus de jeu et d'ajustement de notre comportement.

## [Quiz pr√©-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Pour r√©aliser du RL, nous avons besoin de :

* Un **environnement** ou **simulateur** qui fixe les r√®gles du jeu. Nous devrions √™tre capables de mener des exp√©riences dans le simulateur et d'observer les r√©sultats.
* Une **fonction de r√©compense**, qui indique √† quel point notre exp√©rience a √©t√© r√©ussie. Dans le cas d'apprendre √† jouer √† un jeu vid√©o, la r√©compense serait notre score final.

Sur la base de la fonction de r√©compense, nous devrions √™tre capables d'ajuster notre comportement et d'am√©liorer nos comp√©tences, de sorte qu'√† la prochaine fois nous jouions mieux. La principale diff√©rence entre les autres types d'apprentissage machine et le RL est qu'en RL, nous ne savons g√©n√©ralement pas si nous gagnons ou perdons jusqu'√† ce que nous ayons termin√© le jeu. Ainsi, nous ne pouvons pas dire si un certain coup seul est bon ou non - nous ne recevons une r√©compense qu'√† la fin du jeu.

Au cours du RL, nous r√©alisons g√©n√©ralement de nombreuses exp√©riences. Lors de chaque exp√©rience, nous devons √©quilibrer entre le suivi de la strat√©gie optimale que nous avons apprise jusqu'√† pr√©sent (**exploitation**) et l'exploration de nouveaux √©tats possibles (**exploration**).

## OpenAI Gym

Un excellent outil pour le RL est [OpenAI Gym](https://gym.openai.com/) - un **environnement de simulation**, qui peut simuler de nombreux environnements diff√©rents allant des jeux Atari √† la physique derri√®re l'√©quilibre de poteaux. C'est l'un des environnements de simulation les plus populaires pour former des algorithmes d'apprentissage par renforcement, et il est maintenu par [OpenAI](https://openai.com/).

> **Note** : Vous pouvez voir tous les environnements disponibles sur OpenAI Gym [ici](https://gym.openai.com/envs/#classic_control).

## √âquilibre CartPole

Vous avez probablement tous vu des dispositifs d'√©quilibre modernes tels que le *Segway* ou les *Gyroscooters*. Ils sont capables de s'√©quilibrer automatiquement en ajustant leurs roues en r√©ponse √† un signal provenant d'un acc√©l√©rom√®tre ou d'un gyroscope. Dans cette section, nous allons apprendre √† r√©soudre un probl√®me similaire - √©quilibrer un poteau. C'est similaire √† une situation o√π un artiste de cirque doit √©quilibrer un poteau sur sa main - mais cet √©quilibre de poteau n'a lieu qu'en 1D.

Une version simplifi√©e de l'√©quilibre est connue sous le nom de probl√®me **CartPole**. Dans le monde du cartpole, nous avons un curseur horizontal qui peut se d√©placer √† gauche ou √† droite, et l'objectif est d'√©quilibrer un poteau vertical au sommet du curseur pendant qu'il se d√©place.

<img alt="un cartpole" src="images/cartpole.png" width="200"/>

Pour cr√©er et utiliser cet environnement, nous avons besoin de quelques lignes de code Python :

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Chaque environnement peut √™tre accessible de la m√™me mani√®re :
* `env.reset` starts a new experiment
* `env.step` effectue une √©tape de simulation. Il re√ßoit une **action** de l'**espace d'action**, et retourne une **observation** (de l'espace d'observation), ainsi qu'une r√©compense et un indicateur de terminaison.

Dans l'exemple ci-dessus, nous effectuons une action al√©atoire √† chaque √©tape, ce qui explique pourquoi la dur√©e de l'exp√©rience est tr√®s courte :

![cartpole non √©quilibr√©](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

L'objectif d'un algorithme RL est d'entra√Æner un mod√®le - la soi-disant **politique** œÄ - qui retournera l'action en r√©ponse √† un √©tat donn√©. Nous pouvons √©galement consid√©rer la politique comme √©tant probabiliste, par exemple, pour tout √©tat *s* et action *a*, elle retournera la probabilit√© œÄ(*a*|*s*) que nous devrions prendre *a* dans l'√©tat *s*.

## Algorithme des Gradients de Politique

La mani√®re la plus √©vidente de mod√©liser une politique est de cr√©er un r√©seau de neurones qui prendra des √©tats en entr√©e et retournera les actions correspondantes (ou plut√¥t les probabilit√©s de toutes les actions). En un sens, cela serait similaire √† une t√¢che de classification normale, avec une diff√©rence majeure - nous ne savons pas √† l'avance quelles actions nous devrions prendre √† chacune des √©tapes.

L'id√©e ici est d'estimer ces probabilit√©s. Nous construisons un vecteur de **r√©compenses cumul√©es** qui montre notre r√©compense totale √† chaque √©tape de l'exp√©rience. Nous appliquons √©galement un **escompte de r√©compense** en multipliant les r√©compenses ant√©rieures par un certain coefficient Œ≥=0.99, afin de diminuer le r√¥le des r√©compenses ant√©rieures. Ensuite, nous renfor√ßons ces √©tapes le long du chemin de l'exp√©rience qui produisent de plus grandes r√©compenses.

> En savoir plus sur l'algorithme des gradients de politique et le voir en action dans le [carnet d'exemples](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algorithme Acteur-Critique

Une version am√©lior√©e de l'approche des gradients de politique est appel√©e **Acteur-Critique**. L'id√©e principale derri√®re cela est que le r√©seau de neurones serait entra√Æn√© pour retourner deux choses :

* La politique, qui d√©termine quelle action prendre. Cette partie est appel√©e **acteur**.
* L'estimation de la r√©compense totale que nous pouvons nous attendre √† obtenir √† cet √©tat - cette partie est appel√©e **critique**.

En un sens, cette architecture ressemble √† un [GAN](../../4-ComputerVision/10-GANs/README.md), o√π nous avons deux r√©seaux qui sont entra√Æn√©s l'un contre l'autre. Dans le mod√®le acteur-critique, l'acteur propose l'action que nous devons prendre, et le critique essaie d'√™tre critique et d'estimer le r√©sultat. Cependant, notre objectif est d'entra√Æner ces r√©seaux en harmonie.

Parce que nous connaissons √† la fois les vraies r√©compenses cumul√©es et les r√©sultats retourn√©s par le critique pendant l'exp√©rience, il est relativement facile de construire une fonction de perte qui minimisera la diff√©rence entre elles. Cela nous donnerait la **perte du critique**. Nous pouvons calculer la **perte de l'acteur** en utilisant la m√™me approche que dans l'algorithme des gradients de politique.

Apr√®s avoir ex√©cut√© l'un de ces algorithmes, nous pouvons nous attendre √† ce que notre CartPole se comporte comme ceci :

![un cartpole √©quilibr√©](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Exercices : Gradients de Politique et RL Acteur-Critique

Poursuivez votre apprentissage dans les carnets suivants :

* [RL dans TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL dans PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Autres T√¢ches RL

L'apprentissage par renforcement est aujourd'hui un domaine de recherche en pleine expansion. Quelques exemples int√©ressants d'apprentissage par renforcement sont :

* Apprendre √† un ordinateur √† jouer √† des **jeux Atari**. La partie difficile de ce probl√®me est que nous n'avons pas un √©tat simple repr√©sent√© comme un vecteur, mais plut√¥t une capture d'√©cran - et nous devons utiliser le CNN pour convertir cette image d'√©cran en un vecteur de caract√©ristiques, ou pour extraire des informations de r√©compense. Les jeux Atari sont disponibles dans le Gym.
* Apprendre √† un ordinateur √† jouer √† des jeux de soci√©t√©, tels que les √©checs et le Go. R√©cemment, des programmes √† la pointe de la technologie comme **Alpha Zero** ont √©t√© entra√Æn√©s √† partir de z√©ro par deux agents jouant l'un contre l'autre et s'am√©liorant √† chaque √©tape.
* Dans l'industrie, le RL est utilis√© pour cr√©er des syst√®mes de contr√¥le √† partir de simulations. Un service appel√© [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) est sp√©cifiquement con√ßu pour cela.

## Conclusion

Nous avons maintenant appris comment entra√Æner des agents pour obtenir de bons r√©sultats simplement en leur fournissant une fonction de r√©compense qui d√©finit l'√©tat d√©sir√© du jeu, et en leur donnant l'occasion d'explorer intelligemment l'espace de recherche. Nous avons essay√© avec succ√®s deux algorithmes et obtenu un bon r√©sultat dans un d√©lai relativement court. Cependant, ce n'est que le d√©but de votre parcours dans le RL, et vous devriez certainement envisager de suivre un cours s√©par√© si vous souhaitez approfondir vos connaissances.

## üöÄ D√©fi

Explorez les applications √©num√©r√©es dans la section 'Autres T√¢ches RL' et essayez d'en impl√©menter une !

## [Quiz post-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Revue & Auto-apprentissage

En savoir plus sur l'apprentissage par renforcement classique dans notre [Curriculum d'Apprentissage Machine pour D√©butants](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Regardez [cette excellente vid√©o](https://www.youtube.com/watch?v=qv6UVOQ0F44) qui parle de la mani√®re dont un ordinateur peut apprendre √† jouer √† Super Mario.

## Mission : [Entra√Æner une Voiture de Montagne](lab/README.md)

Votre objectif lors de cette mission serait d'entra√Æner un environnement Gym diff√©rent - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous visons √† garantir l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.