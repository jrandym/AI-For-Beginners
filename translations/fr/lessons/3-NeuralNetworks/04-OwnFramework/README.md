# Introduction aux R√©seaux de Neurones. Perceptron Multi-Couches

Dans la section pr√©c√©dente, vous avez appris le mod√®le de r√©seau de neurones le plus simple - le perceptron √† une couche, un mod√®le de classification lin√©aire √† deux classes.

Dans cette section, nous allons √©tendre ce mod√®le dans un cadre plus flexible, nous permettant de :

* effectuer une **classification multi-classes** en plus de la classification √† deux classes
* r√©soudre des **probl√®mes de r√©gression** en plus de la classification
* s√©parer des classes qui ne sont pas lin√©airement s√©parables

Nous allons √©galement d√©velopper notre propre cadre modulaire en Python qui nous permettra de construire diff√©rentes architectures de r√©seaux de neurones.

## [Quiz pr√©-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalisation de l'Apprentissage Automatique

Commen√ßons par formaliser le probl√®me de l'apprentissage automatique. Supposons que nous avons un ensemble de donn√©es d'entra√Ænement **X** avec des √©tiquettes **Y**, et nous devons construire un mod√®le *f* qui fera les pr√©dictions les plus pr√©cises. La qualit√© des pr√©dictions est mesur√©e par la **fonction de perte** ‚Ñí. Les fonctions de perte suivantes sont souvent utilis√©es :

* Pour le probl√®me de r√©gression, lorsque nous devons pr√©dire un nombre, nous pouvons utiliser l'**erreur absolue** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ou l'**erreur quadratique** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pour la classification, nous utilisons la **perte 0-1** (qui est essentiellement la m√™me que l'**exactitude** du mod√®le), ou la **perte logistique**.

Pour le perceptron √† une couche, la fonction *f* √©tait d√©finie comme une fonction lin√©aire *f(x)=wx+b* (ici *w* est la matrice de poids, *x* est le vecteur des caract√©ristiques d'entr√©e, et *b* est le vecteur de biais). Pour diff√©rentes architectures de r√©seaux de neurones, cette fonction peut prendre une forme plus complexe.

> Dans le cas de la classification, il est souvent souhaitable d'obtenir des probabilit√©s des classes correspondantes comme sortie du r√©seau. Pour convertir des nombres arbitraires en probabilit√©s (par exemple, pour normaliser la sortie), nous utilisons souvent la fonction **softmax** œÉ, et la fonction *f* devient *f(x)=œÉ(wx+b)*

Dans la d√©finition de *f* ci-dessus, *w* et *b* sont appel√©s **param√®tres** Œ∏=‚ü®*w,b*‚ü©. √âtant donn√© l'ensemble de donn√©es ‚ü®**X**,**Y**‚ü©, nous pouvons calculer une erreur globale sur l'ensemble de donn√©es entier en fonction des param√®tres Œ∏.

> ‚úÖ **L'objectif de l'entra√Ænement du r√©seau de neurones est de minimiser l'erreur en faisant varier les param√®tres Œ∏**

## Optimisation par Descente de Gradient

Il existe une m√©thode bien connue d'optimisation de fonction appel√©e **descente de gradient**. L'id√©e est que nous pouvons calculer une d√©riv√©e (dans le cas multidimensionnel appel√©e **gradient**) de la fonction de perte par rapport aux param√®tres, et faire varier les param√®tres de mani√®re √† ce que l'erreur diminue. Cela peut √™tre formalis√© comme suit :

* Initialiser les param√®tres par des valeurs al√©atoires w<sup>(0)</sup>, b<sup>(0)</sup>
* R√©p√©ter l'√©tape suivante plusieurs fois :
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

Lors de l'entra√Ænement, les √©tapes d'optimisation sont cens√©es √™tre calcul√©es en tenant compte de l'ensemble de donn√©es entier (rappelez-vous que la perte est calcul√©e comme une somme √† travers tous les √©chantillons d'entra√Ænement). Cependant, dans la vie r√©elle, nous prenons de petites portions de l'ensemble de donn√©es appel√©es **minibatchs**, et calculons les gradients en fonction d'un sous-ensemble de donn√©es. Comme le sous-ensemble est pris al√©atoirement √† chaque fois, cette m√©thode est appel√©e **descente de gradient stochastique** (SGD).

## Perceptrons Multi-Couches et R√©tropropagation

Le r√©seau √† une couche, comme nous l'avons vu ci-dessus, est capable de classifier des classes lin√©airement s√©parables. Pour construire un mod√®le plus riche, nous pouvons combiner plusieurs couches du r√©seau. Math√©matiquement, cela signifierait que la fonction *f* aurait une forme plus complexe et serait calcul√©e en plusieurs √©tapes :
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

Ici, Œ± est une **fonction d'activation non lin√©aire**, œÉ est une fonction softmax, et les param√®tres Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*.

L'algorithme de descente de gradient resterait le m√™me, mais il serait plus difficile de calculer les gradients. √âtant donn√© la r√®gle de diff√©rentiation en cha√Æne, nous pouvons calculer les d√©riv√©es comme suit :

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ La r√®gle de diff√©rentiation en cha√Æne est utilis√©e pour calculer les d√©riv√©es de la fonction de perte par rapport aux param√®tres.

Notez que la partie la plus √† gauche de toutes ces expressions est la m√™me, et nous pouvons donc calculer efficacement les d√©riv√©es en partant de la fonction de perte et en allant "en arri√®re" √† travers le graphe de calcul. Ainsi, la m√©thode d'entra√Ænement d'un perceptron multi-couches est appel√©e **r√©tropropagation**, ou 'backprop'.

<img alt="graphe de calcul" src="images/ComputeGraphGrad.png"/>

> TODO : citation de l'image

> ‚úÖ Nous couvrirons la r√©tropropagation en d√©tail dans notre exemple de notebook.

## Conclusion

Dans cette le√ßon, nous avons construit notre propre biblioth√®que de r√©seaux de neurones, et nous l'avons utilis√©e pour une t√¢che de classification simple en deux dimensions.

## üöÄ D√©fi

Dans le notebook accompagnant, vous allez impl√©menter votre propre cadre pour construire et entra√Æner des perceptrons multi-couches. Vous pourrez voir en d√©tail comment fonctionnent les r√©seaux de neurones modernes.

Proc√©dez au notebook [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) et travaillez √† travers.

## [Quiz post-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Revue & Auto-√©tude

La r√©tropropagation est un algorithme courant utilis√© en IA et en apprentissage automatique, il vaut la peine d'√™tre √©tudi√© [plus en d√©tail](https://wikipedia.org/wiki/Backpropagation)

## [Devoir](lab/README.md)

Dans ce laboratoire, vous √™tes invit√© √† utiliser le cadre que vous avez construit dans cette le√ßon pour r√©soudre la classification de chiffres manuscrits MNIST.

* [Instructions](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des erreurs d'interpr√©tation r√©sultant de l'utilisation de cette traduction.