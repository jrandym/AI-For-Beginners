# Architectures CNN Bien Connues

### VGG-16

VGG-16 est un r√©seau qui a atteint 92,7 % de pr√©cision dans la classification top-5 d'ImageNet en 2014. Il a la structure de couches suivante :

![Couches ImageNet](../../../../../translated_images/vgg-16-arch1.d901a5583b3a51baeaab3e768567d921e5d54befa46e1e642616c5458c934028.fr.jpg)

Comme vous pouvez le voir, VGG suit une architecture pyramidale traditionnelle, qui est une s√©quence de couches de convolution et de pooling.

![Pyramide ImageNet](../../../../../translated_images/vgg-16-arch.64ff2137f50dd49fdaa786e3f3a975b3f22615efd13efb19c5d22f12e01451a1.fr.jpg)

> Image de [Researchgate](https://www.researchgate.net/figure/Vgg16-model-structure-To-get-the-VGG-NIN-model-we-replace-the-2-nd-4-th-6-th-7-th_fig2_335194493)

### ResNet

ResNet est une famille de mod√®les propos√©e par Microsoft Research en 2015. L'id√©e principale de ResNet est d'utiliser des **blocs r√©siduels** :

<img src="images/resnet-block.png" width="300"/>

> Image de [cet article](https://arxiv.org/pdf/1512.03385.pdf)

La raison d'utiliser un passage identitaire est de faire en sorte que notre couche pr√©dit **la diff√©rence** entre le r√©sultat d'une couche pr√©c√©dente et la sortie du bloc r√©siduel - d'o√π le nom *r√©siduel*. Ces blocs sont beaucoup plus faciles √† entra√Æner, et on peut construire des r√©seaux avec plusieurs centaines de ces blocs (les variantes les plus courantes sont ResNet-52, ResNet-101 et ResNet-152).

Vous pouvez √©galement penser √† ce r√©seau comme √©tant capable d'ajuster sa complexit√© au jeu de donn√©es. Au d√©but, lorsque vous commencez √† entra√Æner le r√©seau, les valeurs des poids sont faibles, et la plupart du signal passe √† travers des couches identitaires. √Ä mesure que l'entra√Ænement progresse et que les poids deviennent plus grands, l'importance des param√®tres du r√©seau augmente, et le r√©seau s'ajuste pour s'adapter √† la puissance expressive requise pour classer correctement les images d'entra√Ænement.

### Google Inception

L'architecture Google Inception pousse cette id√©e un peu plus loin et construit chaque couche du r√©seau comme une combinaison de plusieurs chemins diff√©rents :

<img src="images/inception.png" width="400"/>

> Image de [Researchgate](https://www.researchgate.net/figure/Inception-module-with-dimension-reductions-left-and-schema-for-Inception-ResNet-v1_fig2_355547454)

Ici, nous devons souligner le r√¥le des convolutions 1x1, car au premier abord, elles n'ont pas de sens. Pourquoi devrions-nous parcourir l'image avec un filtre 1x1 ? Cependant, vous devez vous rappeler que les filtres de convolution fonctionnent √©galement avec plusieurs canaux de profondeur (√† l'origine - couleurs RGB, dans les couches suivantes - canaux pour diff√©rents filtres), et la convolution 1x1 est utilis√©e pour m√©langer ces canaux d'entr√©e ensemble en utilisant diff√©rents poids entra√Ænables. Cela peut √©galement √™tre consid√©r√© comme un sous-√©chantillonnage (pooling) sur la dimension des canaux.

Voici [un bon article de blog](https://medium.com/analytics-vidhya/talented-mr-1x1-comprehensive-look-at-1x1-convolution-in-deep-learning-f6b355825578) sur le sujet, et [l'article original](https://arxiv.org/pdf/1312.4400.pdf).

### MobileNet

MobileNet est une famille de mod√®les de taille r√©duite, adapt√©s aux appareils mobiles. Utilisez-les si vous manquez de ressources et pouvez sacrifier un peu de pr√©cision. L'id√©e principale derri√®re eux est la **convolution s√©parablement en profondeur**, qui permet de repr√©senter les filtres de convolution par une composition de convolutions spatiales et de convolutions 1x1 sur les canaux de profondeur. Cela r√©duit consid√©rablement le nombre de param√®tres, rendant le r√©seau plus petit en taille et √©galement plus facile √† entra√Æner avec moins de donn√©es.

Voici [un bon article de blog sur MobileNet](https://medium.com/analytics-vidhya/image-classification-with-mobilenet-cc6fbb2cd470).

## Conclusion

Dans cette unit√©, vous avez appris le concept principal derri√®re les r√©seaux neuronaux en vision par ordinateur - les r√©seaux de convolution. Les architectures r√©elles qui alimentent la classification d'images, la d√©tection d'objets et m√™me les r√©seaux de g√©n√©ration d'images sont toutes bas√©es sur les CNN, mais avec plus de couches et quelques astuces d'entra√Ænement suppl√©mentaires.

## üöÄ D√©fi

Dans les notebooks accompagnants, il y a des notes en bas sur la fa√ßon d'obtenir une pr√©cision plus √©lev√©e. Faites quelques exp√©riences pour voir si vous pouvez atteindre une pr√©cision sup√©rieure.

## [Quiz post-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/207)

## Revue & Auto-√©tude

Bien que les CNN soient le plus souvent utilis√©s pour des t√¢ches de vision par ordinateur, ils sont g√©n√©ralement bons pour extraire des motifs de taille fixe. Par exemple, si nous traitons des sons, nous pouvons √©galement vouloir utiliser des CNN pour rechercher des motifs sp√©cifiques dans un signal audio - dans ce cas, les filtres seraient unidimensionnels (et ce CNN serait appel√© 1D-CNN). De plus, parfois, un 3D-CNN est utilis√© pour extraire des caract√©ristiques dans un espace multidimensionnel, comme certains √©v√©nements se produisant dans une vid√©o - le CNN peut capturer certains motifs de changement de caract√©ristiques au fil du temps. Faites quelques recherches et auto-√©tudes sur d'autres t√¢ches qui peuvent √™tre r√©alis√©es avec des CNN.

## [Devoir](lab/README.md)

Dans ce laboratoire, vous devez classer diff√©rentes races de chats et de chiens. Ces images sont plus complexes que le jeu de donn√©es MNIST et ont des dimensions plus √©lev√©es, et il y a plus de 10 classes.

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.