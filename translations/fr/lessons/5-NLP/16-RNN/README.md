# R√©seaux de Neurones R√©currents

## [Quiz pr√©-cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

Dans les sections pr√©c√©dentes, nous avons utilis√© des repr√©sentations s√©mantiques riches du texte et un simple classificateur lin√©aire au-dessus des embeddings. Ce que fait cette architecture, c'est capturer le sens agr√©g√© des mots dans une phrase, mais elle ne prend pas en compte l'**ordre** des mots, car l'op√©ration d'agr√©gation sur les embeddings a supprim√© cette information du texte original. √âtant donn√© que ces mod√®les ne peuvent pas mod√©liser l'ordre des mots, ils ne peuvent pas r√©soudre des t√¢ches plus complexes ou ambigu√´s telles que la g√©n√©ration de texte ou la r√©ponse √† des questions.

Pour capturer le sens d'une s√©quence de texte, nous devons utiliser une autre architecture de r√©seau de neurones, appel√©e **r√©seau de neurones r√©currents**, ou RNN. Dans un RNN, nous faisons passer notre phrase √† travers le r√©seau un symbole √† la fois, et le r√©seau produit un **√©tat**, que nous renvoyons ensuite au r√©seau avec le symbole suivant.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.fr.png)

> Image par l'auteur

√âtant donn√© la s√©quence d'entr√©e de tokens X<sub>0</sub>,...,X<sub>n</sub>, le RNN cr√©e une s√©quence de blocs de r√©seaux de neurones et entra√Æne cette s√©quence de bout en bout en utilisant la r√©tropropagation. Chaque bloc de r√©seau prend une paire (X<sub>i</sub>,S<sub>i</sub>) comme entr√©e et produit S<sub>i+1</sub> en r√©sultat. L'√©tat final S<sub>n</sub> ou (sortie Y<sub>n</sub>) passe dans un classificateur lin√©aire pour produire le r√©sultat. Tous les blocs de r√©seau partagent les m√™mes poids et sont entra√Æn√©s de bout en bout en utilisant un passage de r√©tropropagation.

Puisque les vecteurs d'√©tat S<sub>0</sub>,...,S<sub>n</sub> sont pass√©s √† travers le r√©seau, il est capable d'apprendre les d√©pendances s√©quentielles entre les mots. Par exemple, lorsque le mot *not* appara√Æt quelque part dans la s√©quence, il peut apprendre √† nier certains √©l√©ments au sein du vecteur d'√©tat, ce qui entra√Æne une n√©gation.

> ‚úÖ √âtant donn√© que les poids de tous les blocs RNN sur l'image ci-dessus sont partag√©s, la m√™me image peut √™tre repr√©sent√©e comme un seul bloc (√† droite) avec une boucle de r√©troaction r√©currente, qui renvoie l'√©tat de sortie du r√©seau √† l'entr√©e.

## Anatomie d'une cellule RNN

Voyons comment une cellule RNN simple est organis√©e. Elle accepte l'√©tat pr√©c√©dent S<sub>i-1</sub> et le symbole actuel X<sub>i</sub> comme entr√©es, et doit produire l'√©tat de sortie S<sub>i</sub> (et, parfois, nous sommes √©galement int√©ress√©s par une autre sortie Y<sub>i</sub>, comme dans le cas des r√©seaux g√©n√©ratifs).

Une cellule RNN simple poss√®de deux matrices de poids √† l'int√©rieur : l'une transforme un symbole d'entr√©e (appelons-la W), et l'autre transforme un √©tat d'entr√©e (H). Dans ce cas, la sortie du r√©seau est calcul√©e comme œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), o√π œÉ est la fonction d'activation et b est un biais suppl√©mentaire.

<img alt="Anatomie de la cellule RNN" src="images/rnn-anatomy.png" width="50%"/>

> Image par l'auteur

Dans de nombreux cas, les tokens d'entr√©e sont pass√©s par la couche d'embedding avant d'entrer dans le RNN pour r√©duire la dimensionnalit√©. Dans ce cas, si la dimension des vecteurs d'entr√©e est *emb_size*, et que le vecteur d'√©tat est *hid_size* - la taille de W est *emb_size*√ó*hid_size*, et la taille de H est *hid_size*√ó*hid_size*.

## M√©moire √† Long Terme et √† Court Terme (LSTM)

Un des principaux probl√®mes des RNN classiques est le probl√®me des **gradients qui disparaissent**. √âtant donn√© que les RNN sont entra√Æn√©s de bout en bout en un seul passage de r√©tropropagation, il a des difficult√©s √† propager l'erreur vers les premi√®res couches du r√©seau, et donc le r√©seau ne peut pas apprendre les relations entre des tokens √©loign√©s. Une des fa√ßons d'√©viter ce probl√®me est d'introduire une **gestion explicite de l'√©tat** en utilisant ce que l'on appelle des **portes**. Il existe deux architectures bien connues de ce type : **Long Short Term Memory** (LSTM) et **Gated Relay Unit** (GRU).

![Image montrant un exemple de cellule m√©moire √† long terme](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Source de l'image √† d√©terminer

Le r√©seau LSTM est organis√© de mani√®re similaire au RNN, mais il y a deux √©tats qui sont pass√©s de couche en couche : l'√©tat actuel C, et le vecteur cach√© H. √Ä chaque unit√©, le vecteur cach√© H<sub>i</sub> est concat√©n√© avec l'entr√©e X<sub>i</sub>, et ils contr√¥lent ce qui arrive √† l'√©tat C via des **portes**. Chaque porte est un r√©seau de neurones avec une activation sigmo√Øde (sortie dans la plage [0,1]), qui peut √™tre consid√©r√© comme un masque au niveau des bits lorsqu'il est multipli√© par le vecteur d'√©tat. Il y a les portes suivantes (de gauche √† droite sur l'image ci-dessus) :

* La **porte d'oubli** prend un vecteur cach√© et d√©termine quels composants du vecteur C nous devons oublier, et lesquels passer.
* La **porte d'entr√©e** prend certaines informations des vecteurs d'entr√©e et cach√©s et les ins√®re dans l'√©tat.
* La **porte de sortie** transforme l'√©tat via une couche lin√©aire avec activation *tanh*, puis s√©lectionne certains de ses composants en utilisant un vecteur cach√© H<sub>i</sub> pour produire un nouvel √©tat C<sub>i+1</sub>.

Les composants de l'√©tat C peuvent √™tre consid√©r√©s comme des indicateurs qui peuvent √™tre activ√©s ou d√©sactiv√©s. Par exemple, lorsque nous rencontrons un nom *Alice* dans la s√©quence, nous pouvons vouloir supposer qu'il fait r√©f√©rence √† un personnage f√©minin et activer l'indicateur dans l'√©tat que nous avons un nom f√©minin dans la phrase. Lorsque nous rencontrons ensuite des phrases comme *et Tom*, nous activerons l'indicateur que nous avons un nom pluriel. Ainsi, en manipulant l'√©tat, nous pouvons suppos√©ment garder une trace des propri√©t√©s grammaticales des parties de la phrase.

> ‚úÖ Une excellente ressource pour comprendre les rouages des LSTM est cet excellent article [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) par Christopher Olah.

## RNNs Bidirectionnels et Multicouches

Nous avons discut√© des r√©seaux r√©currents qui fonctionnent dans une seule direction, du d√©but d'une s√©quence √† la fin. Cela semble naturel, car cela ressemble √† la fa√ßon dont nous lisons et √©coutons la parole. Cependant, puisque dans de nombreux cas pratiques nous avons un acc√®s al√©atoire √† la s√©quence d'entr√©e, il peut √™tre judicieux d'ex√©cuter un calcul r√©current dans les deux directions. De tels r√©seaux sont appel√©s RNNs **bidirectionnels**. Lorsqu'on traite un r√©seau bidirectionnel, nous aurions besoin de deux vecteurs d'√©tat cach√©s, un pour chaque direction.

Un r√©seau r√©current, qu'il soit unidirectionnel ou bidirectionnel, capture certains motifs au sein d'une s√©quence et peut les stocker dans un vecteur d'√©tat ou les passer en sortie. Comme avec les r√©seaux convolutifs, nous pouvons construire une autre couche r√©currente au-dessus de la premi√®re pour capturer des motifs de niveau sup√©rieur et construire √† partir de motifs de bas niveau extraits par la premi√®re couche. Cela nous conduit √† la notion de **RNN multicouche** qui se compose de deux r√©seaux r√©currents ou plus, o√π la sortie de la couche pr√©c√©dente est pass√©e √† la couche suivante en tant qu'entr√©e.

![Image montrant un RNN multicouche de m√©moire √† long terme](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.fr.jpg)

*Image tir√©e [de cet excellent article](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) par Fernando L√≥pez*

## ‚úçÔ∏è Exercices : Embeddings

Continuez votre apprentissage dans les notebooks suivants :

* [RNNs avec PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs avec TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Conclusion

Dans cette unit√©, nous avons vu que les RNNs peuvent √™tre utilis√©s pour la classification de s√©quences, mais en fait, ils peuvent g√©rer de nombreuses autres t√¢ches, telles que la g√©n√©ration de texte, la traduction automatique, et plus encore. Nous consid√©rerons ces t√¢ches dans la prochaine unit√©.

## üöÄ D√©fi

Lisez quelques documents sur les LSTMs et envisagez leurs applications :

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Quiz post-cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## R√©vision & Auto-√©tude

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) par Christopher Olah.

## [Devoir : Notebooks](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.