# M√©canismes d'Attention et Transformateurs

## [Quiz pr√©-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

L'un des probl√®mes les plus importants dans le domaine du NLP est la **traduction automatique**, une t√¢che essentielle qui sous-tend des outils tels que Google Translate. Dans cette section, nous allons nous concentrer sur la traduction automatique, ou, plus g√©n√©ralement, sur toute t√¢che de *s√©quence √† s√©quence* (qui est √©galement appel√©e **transduction de phrases**).

Avec les RNN, la s√©quence √† s√©quence est mise en ≈ìuvre par deux r√©seaux r√©currents, o√π un r√©seau, l'**encodeur**, r√©duit une s√©quence d'entr√©e √† un √©tat cach√©, tandis qu'un autre r√©seau, le **d√©codeur**, d√©roule cet √©tat cach√© en un r√©sultat traduit. Il y a quelques probl√®mes avec cette approche :

* L'√©tat final du r√©seau encodeur a du mal √† se souvenir du d√©but d'une phrase, ce qui entra√Æne une mauvaise qualit√© du mod√®le pour les longues phrases.
* Tous les mots d'une s√©quence ont le m√™me impact sur le r√©sultat. En r√©alit√©, cependant, certains mots sp√©cifiques de la s√©quence d'entr√©e ont souvent plus d'impact sur les sorties s√©quentielles que d'autres.

Les **M√©canismes d'Attention** fournissent un moyen de pond√©rer l'impact contextuel de chaque vecteur d'entr√©e sur chaque pr√©diction de sortie du RNN. La mani√®re dont cela est mis en ≈ìuvre consiste √† cr√©er des raccourcis entre les √©tats interm√©diaires du RNN d'entr√©e et le RNN de sortie. De cette mani√®re, lors de la g√©n√©ration du symbole de sortie y<sub>t</sub>, nous prendrons en compte tous les √©tats cach√©s d'entr√©e h<sub>i</sub>, avec des coefficients de poids diff√©rents Œ±<sub>t,i</sub>.

![Image montrant un mod√®le encodeur/d√©codeur avec une couche d'attention additive](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.fr.png)

> Le mod√®le encodeur-d√©codeur avec m√©canisme d'attention additive dans [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cit√© dans [cet article de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

La matrice d'attention {Œ±<sub>i,j</sub>} repr√©senterait le degr√© auquel certains mots d'entr√©e jouent dans la g√©n√©ration d'un mot donn√© dans la s√©quence de sortie. Voici un exemple d'une telle matrice :

![Image montrant un alignement d'exemple trouv√© par RNNsearch-50, tir√© de Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.fr.png)

> Figure de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Les m√©canismes d'attention sont responsables de beaucoup de l'√©tat actuel ou presque actuel de l'art dans le NLP. Cependant, l'ajout d'attention augmente consid√©rablement le nombre de param√®tres du mod√®le, ce qui a conduit √† des probl√®mes d'√©chelle avec les RNN. Une contrainte cl√© de l'√©chelle des RNN est que la nature r√©currente des mod√®les rend difficile le traitement par lots et la parall√©lisation de l'entra√Ænement. Dans un RNN, chaque √©l√©ment d'une s√©quence doit √™tre trait√© dans l'ordre s√©quentiel, ce qui signifie qu'il ne peut pas √™tre facilement parall√©lis√©.

![Encodeur D√©codeur avec Attention](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figure de [Blog de Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

L'adoption des m√©canismes d'attention combin√©e √† cette contrainte a conduit √† la cr√©ation des mod√®les de transformateurs qui sont maintenant √† la pointe de la technologie que nous connaissons et utilisons aujourd'hui, tels que BERT et Open-GPT3.

## Mod√®les de Transformateurs

L'une des principales id√©es derri√®re les transformateurs est d'√©viter la nature s√©quentielle des RNN et de cr√©er un mod√®le qui est parall√©lisable pendant l'entra√Ænement. Cela est r√©alis√© en mettant en ≈ìuvre deux id√©es :

* codage positionnel
* utilisation du m√©canisme d'auto-attention pour capturer des motifs au lieu des RNN (ou CNN) (c'est pourquoi l'article qui introduit les transformateurs s'appelle *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Codage/Emballage Positionnel

L'id√©e du codage positionnel est la suivante. 
1. Lors de l'utilisation des RNN, la position relative des jetons est repr√©sent√©e par le nombre d'√©tapes, et n'a donc pas besoin d'√™tre repr√©sent√©e explicitement. 
2. Cependant, une fois que nous passons √† l'attention, nous devons conna√Ætre les positions relatives des jetons dans une s√©quence. 
3. Pour obtenir le codage positionnel, nous augmentons notre s√©quence de jetons avec une s√©quence de positions de jetons dans la s√©quence (c'est-√†-dire, une s√©quence de nombres 0,1, ...).
4. Nous m√©langeons ensuite la position du jeton avec un vecteur d'embedding de jeton. Pour transformer la position (entier) en un vecteur, nous pouvons utiliser diff√©rentes approches :

* Embedding entra√Ænable, similaire √† l'embedding de jeton. C'est l'approche que nous consid√©rons ici. Nous appliquons des couches d'embedding sur les jetons et leurs positions, ce qui donne des vecteurs d'embedding de m√™mes dimensions, que nous additionnons ensuite.
* Fonction de codage de position fixe, comme propos√© dans l'article original.

<img src="images/pos-embedding.png" width="50%"/>

> Image de l'auteur

Le r√©sultat que nous obtenons avec l'embedding positionnel int√®gre √† la fois le jeton original et sa position dans une s√©quence.

### Auto-Attention Multi-T√™te

Ensuite, nous devons capturer certains motifs au sein de notre s√©quence. Pour ce faire, les transformateurs utilisent un m√©canisme d'**auto-attention**, qui est essentiellement une attention appliqu√©e √† la m√™me s√©quence en tant qu'entr√©e et sortie. L'application de l'auto-attention nous permet de prendre en compte le **contexte** au sein de la phrase, et de voir quels mots sont inter-reli√©s. Par exemple, cela nous permet de voir quels mots sont r√©f√©renc√©s par des cor√©f√©rences, telles que *il*, et √©galement de prendre en compte le contexte :

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.fr.png)

> Image du [Blog de Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Dans les transformateurs, nous utilisons l'**Attention Multi-T√™te** afin de donner au r√©seau la capacit√© de capturer plusieurs types de d√©pendances diff√©rentes, par exemple les relations entre mots √† long terme par rapport √† celles √† court terme, les co-r√©f√©rences par rapport √† autre chose, etc.

[Notebook TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) contient plus de d√©tails sur l'impl√©mentation des couches de transformateurs.

### Attention Encodeur-D√©codeur

Dans les transformateurs, l'attention est utilis√©e √† deux endroits :

* Pour capturer des motifs au sein du texte d'entr√©e en utilisant l'auto-attention
* Pour effectuer la traduction de s√©quence - c'est la couche d'attention entre l'encodeur et le d√©codeur.

L'attention encodeur-d√©codeur est tr√®s similaire au m√©canisme d'attention utilis√© dans les RNN, comme d√©crit au d√©but de cette section. Ce diagramme anim√© explique le r√¥le de l'attention encodeur-d√©codeur.

![GIF anim√© montrant comment les √©valuations sont effectu√©es dans les mod√®les de transformateurs.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Puisque chaque position d'entr√©e est mapp√©e ind√©pendamment √† chaque position de sortie, les transformateurs peuvent mieux se parall√©liser que les RNN, ce qui permet des mod√®les de langage beaucoup plus grands et plus expressifs. Chaque t√™te d'attention peut √™tre utilis√©e pour apprendre diff√©rentes relations entre les mots, ce qui am√©liore les t√¢ches de traitement du langage naturel en aval.

## BERT

**BERT** (Repr√©sentations Encodeur Bidirectionnelles √† partir de Transformateurs) est un tr√®s grand r√©seau de transformateurs multicouches avec 12 couches pour *BERT-base*, et 24 pour *BERT-large*. Le mod√®le est d'abord pr√©-entra√Æn√© sur un grand corpus de donn√©es textuelles (WikiPedia + livres) en utilisant un entra√Ænement non supervis√© (pr√©diction de mots masqu√©s dans une phrase). Pendant le pr√©-entra√Ænement, le mod√®le absorbe des niveaux significatifs de compr√©hension linguistique qui peuvent ensuite √™tre exploit√©s avec d'autres ensembles de donn√©es en utilisant un ajustement fin. Ce processus est appel√© **apprentissage par transfert**.

![image de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.fr.png)

> Image [source](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exercices : Transformateurs

Poursuivez votre apprentissage dans les notebooks suivants :

* [Transformateurs en PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformateurs en TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Conclusion

Dans cette le√ßon, vous avez appris sur les Transformateurs et les M√©canismes d'Attention, tous des outils essentiels dans la bo√Æte √† outils du NLP. Il existe de nombreuses variations d'architectures de Transformateurs, y compris BERT, DistilBERT, BigBird, OpenGPT3 et plus encore, qui peuvent √™tre ajust√©es. Le [package HuggingFace](https://github.com/huggingface/) fournit un d√©p√¥t pour entra√Æner bon nombre de ces architectures avec PyTorch et TensorFlow.

## üöÄ D√©fi

## [Quiz post-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Revue & Auto-√©tude

* [Article de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), expliquant l'article classique [Attention is all you need](https://arxiv.org/abs/1706.03762) sur les transformateurs.
* [Une s√©rie d'articles de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sur les transformateurs, expliquant l'architecture en d√©tail.

## [Devoir](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction professionnelle par un humain est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.