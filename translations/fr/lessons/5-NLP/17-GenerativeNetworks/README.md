# R√©seaux g√©n√©ratifs

## [Quiz pr√©-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Les r√©seaux de neurones r√©currents (RNN) et leurs variantes √† cellules gateÃÅes telles que les cellules de m√©moire √† long terme et √† court terme (LSTM) et les unit√©s r√©currentes gateÃÅes (GRU) ont fourni un m√©canisme pour la mod√©lisation du langage en ce sens qu'ils peuvent apprendre l'ordre des mots et fournir des pr√©dictions pour le mot suivant dans une s√©quence. Cela nous permet d'utiliser les RNN pour des **t√¢ches g√©n√©ratives**, telles que la g√©n√©ration de texte ordinaire, la traduction automatique et m√™me la l√©gende d'images.

> ‚úÖ Pensez √† toutes les fois o√π vous avez b√©n√©fici√© de t√¢ches g√©n√©ratives telles que la compl√©tion de texte pendant que vous tapez. Faites des recherches sur vos applications pr√©f√©r√©es pour voir si elles ont utilis√© des RNN.

Dans l'architecture RNN que nous avons discut√©e dans l'unit√© pr√©c√©dente, chaque unit√© RNN produisait le prochain √©tat cach√© en sortie. Cependant, nous pouvons √©galement ajouter une autre sortie √† chaque unit√© r√©currente, ce qui nous permettrait de produire une **s√©quence** (qui est de la m√™me longueur que la s√©quence originale). De plus, nous pouvons utiliser des unit√©s RNN qui n'acceptent pas une entr√©e √† chaque √©tape, mais prennent simplement un vecteur d'√©tat initial, puis produisent une s√©quence de sorties.

Cela permet diff√©rentes architectures neuronales qui sont montr√©es dans l'image ci-dessous :

![Image montrant des motifs courants de r√©seaux de neurones r√©currents.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.fr.jpg)

> Image provenant de l'article de blog [L'efficacit√© d√©raisonnable des r√©seaux de neurones r√©currents](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) par [Andrej Karpaty](http://karpathy.github.io/)

* **Un √† un** est un r√©seau de neurones traditionnel avec une entr√©e et une sortie
* **Un √† plusieurs** est une architecture g√©n√©rative qui accepte une valeur d'entr√©e et g√©n√®re une s√©quence de valeurs de sortie. Par exemple, si nous voulons entra√Æner un r√©seau de **l√©gende d'images** qui produirait une description textuelle d'une image, nous pouvons prendre une image comme entr√©e, la passer √† travers un CNN pour obtenir son √©tat cach√©, puis faire g√©n√©rer une cha√Æne r√©currente des mots de la l√©gende un par un
* **Plusieurs √† un** correspond aux architectures RNN que nous avons d√©crites dans l'unit√© pr√©c√©dente, telles que la classification de texte
* **Plusieurs √† plusieurs**, ou **s√©quence √† s√©quence**, correspond √† des t√¢ches telles que **la traduction automatique**, o√π nous avons d'abord un RNN qui collecte toutes les informations de la s√©quence d'entr√©e dans l'√©tat cach√©, et une autre cha√Æne RNN d√©roule cet √©tat dans la s√©quence de sortie.

Dans cette unit√©, nous nous concentrerons sur des mod√®les g√©n√©ratifs simples qui nous aident √† g√©n√©rer du texte. Pour simplifier, nous utiliserons une tokenisation au niveau des caract√®res.

Nous allons entra√Æner ce RNN pour g√©n√©rer du texte √©tape par √©tape. √Ä chaque √©tape, nous prendrons une s√©quence de caract√®res de longueur `nchars` et demanderons au r√©seau de g√©n√©rer le prochain caract√®re de sortie pour chaque caract√®re d'entr√©e :

![Image montrant un exemple de g√©n√©ration RNN du mot 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.fr.png)

Lors de la g√©n√©ration de texte (durant l'inf√©rence), nous commen√ßons avec un **prompt**, qui est pass√© √† travers les cellules RNN pour g√©n√©rer son √©tat interm√©diaire, puis √† partir de cet √©tat, la g√©n√©ration commence. Nous g√©n√©rons un caract√®re √† la fois et passons l'√©tat et le caract√®re g√©n√©r√© √† une autre cellule RNN pour g√©n√©rer le suivant, jusqu'√† ce que nous g√©n√©rions suffisamment de caract√®res.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Image de l'auteur

## ‚úçÔ∏è Exercices : R√©seaux g√©n√©ratifs

Poursuivez votre apprentissage dans les notebooks suivants :

* [R√©seaux g√©n√©ratifs avec PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [R√©seaux g√©n√©ratifs avec TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## G√©n√©ration de texte douce et temp√©rature

La sortie de chaque cellule RNN est une distribution de probabilit√© de caract√®res. Si nous prenons toujours le caract√®re avec la plus haute probabilit√© comme le prochain caract√®re dans le texte g√©n√©r√©, le texte peut souvent devenir "cyclique" entre les m√™mes s√©quences de caract√®res encore et encore, comme dans cet exemple :

```
today of the second the company and a second the company ...
```

Cependant, si nous regardons la distribution de probabilit√© pour le prochain caract√®re, il se pourrait que la diff√©rence entre quelques probabilit√©s les plus √©lev√©es ne soit pas √©norme, par exemple, un caract√®re peut avoir une probabilit√© de 0,2, un autre - 0,19, etc. Par exemple, en cherchant le prochain caract√®re dans la s√©quence '*play*', le prochain caract√®re peut tout aussi bien √™tre un espace ou **e** (comme dans le mot *player*).

Cela nous am√®ne √† la conclusion qu'il n'est pas toujours "juste" de s√©lectionner le caract√®re avec une probabilit√© plus √©lev√©e, car choisir le deuxi√®me plus √©lev√© pourrait toujours nous mener √† un texte significatif. Il est plus judicieux de **pr√©lever** des caract√®res √† partir de la distribution de probabilit√© donn√©e par la sortie du r√©seau. Nous pouvons √©galement utiliser un param√®tre, **temp√©rature**, qui va aplatir la distribution de probabilit√©, dans le cas o√π nous voulons ajouter plus de randomit√©, ou la rendre plus abrupte, si nous voulons nous en tenir davantage aux caract√®res de plus haute probabilit√©.

Explorez comment cette g√©n√©ration de texte douce est mise en ≈ìuvre dans les notebooks li√©s ci-dessus.

## Conclusion

Bien que la g√©n√©ration de texte puisse √™tre utile en soi, les principaux avantages proviennent de la capacit√© √† g√©n√©rer du texte en utilisant des RNN √† partir d'un certain vecteur de caract√©ristiques initial. Par exemple, la g√©n√©ration de texte est utilis√©e dans le cadre de la traduction automatique (s√©quence √† s√©quence, dans ce cas le vecteur d'√©tat de l'*encodeur* est utilis√© pour g√©n√©rer ou *d√©coder* le message traduit), ou pour g√©n√©rer une description textuelle d'une image (dans ce cas, le vecteur de caract√©ristiques proviendrait d'un extracteur CNN).

## üöÄ D√©fi

Suivez quelques le√ßons sur Microsoft Learn sur ce sujet

* G√©n√©ration de texte avec [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz post-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Revue & Auto-apprentissage

Voici quelques articles pour approfondir vos connaissances

* Diff√©rentes approches de g√©n√©ration de texte avec Markov Chain, LSTM et GPT-2 : [article de blog](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemple de g√©n√©ration de texte dans [la documentation Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Devoir](lab/README.md)

Nous avons vu comment g√©n√©rer du texte caract√®re par caract√®re. Dans le laboratoire, vous explorerez la g√©n√©ration de texte au niveau des mots.

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue natale doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.