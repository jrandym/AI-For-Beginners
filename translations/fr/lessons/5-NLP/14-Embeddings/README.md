# Int√©grations

## [Quiz avant le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Lors de l'entra√Ænement de classificateurs bas√©s sur BoW ou TF/IDF, nous avons travaill√© avec des vecteurs de sac de mots de haute dimension de longueur `vocab_size`, et nous convertissions explicitement des vecteurs de repr√©sentation positionnelle de faible dimension en une repr√©sentation one-hot √©parse. Cependant, cette repr√©sentation one-hot n'est pas efficace en termes de m√©moire. De plus, chaque mot est trait√© ind√©pendamment des autres, c'est-√†-dire que les vecteurs encod√©s en one-hot n'expriment aucune similarit√© s√©mantique entre les mots.

L'id√©e de **l'int√©gration** est de repr√©senter les mots par des vecteurs denses de dimension inf√©rieure, qui refl√®tent d'une certaine mani√®re le sens s√©mantique d'un mot. Nous discuterons plus tard de la fa√ßon de construire des int√©grations de mots significatives, mais pour l'instant, consid√©rons simplement les int√©grations comme un moyen de r√©duire la dimensionnalit√© d'un vecteur de mots.

Ainsi, la couche d'int√©gration prendrait un mot comme entr√©e et produirait un vecteur de sortie de dimension sp√©cifi√©e `embedding_size`. En un sens, c'est tr√®s similaire √† une couche `Linear`, mais au lieu de prendre un vecteur encod√© en one-hot, elle pourra prendre un num√©ro de mot comme entr√©e, ce qui nous permet d'√©viter de cr√©er de grands vecteurs encod√©s en one-hot.

En utilisant une couche d'int√©gration comme premi√®re couche dans notre r√©seau de classificateurs, nous pouvons passer d'un mod√®le de sac de mots √† un mod√®le de **sac d'int√©grations**, o√π nous convertissons d'abord chaque mot de notre texte en l'int√©gration correspondante, puis nous calculons une fonction agr√©g√©e sur toutes ces int√©grations, telle que `sum`, `average` ou `max`.  

![Image montrant un classificateur d'int√©gration pour cinq mots en s√©quence.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.fr.png)

> Image par l'auteur

## ‚úçÔ∏è Exercices : Int√©grations

Continuez votre apprentissage dans les cahiers suivants :
* [Int√©grations avec PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Int√©grations TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Int√©grations s√©mantiques : Word2Vec

Bien que la couche d'int√©gration ait appris √† mapper les mots √† une repr√©sentation vectorielle, cette repr√©sentation n'avait pas n√©cessairement beaucoup de signification s√©mantique. Il serait souhaitable d'apprendre une repr√©sentation vectorielle telle que des mots similaires ou des synonymes correspondent √† des vecteurs qui sont proches les uns des autres en termes de distance vectorielle (par exemple, distance euclidienne).

Pour ce faire, nous devons pr√©-entra√Æner notre mod√®le d'int√©gration sur une grande collection de textes d'une mani√®re sp√©cifique. Une fa√ßon d'entra√Æner des int√©grations s√©mantiques est appel√©e [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Elle est bas√©e sur deux architectures principales utilis√©es pour produire une repr√©sentation distribu√©e des mots :

 - **Sac de mots continu** (CBoW) ‚Äî dans cette architecture, nous entra√Ænons le mod√®le √† pr√©dire un mot √† partir du contexte environnant. √âtant donn√© le ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, l'objectif du mod√®le est de pr√©dire $W_0$ √† partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Skip-gram continu** est l'oppos√© de CBoW. Le mod√®le utilise une fen√™tre de mots contextuels environnants pour pr√©dire le mot actuel.

CBoW est plus rapide, tandis que skip-gram est plus lent, mais il fait un meilleur travail de repr√©sentation des mots peu fr√©quents.

![Image montrant √† la fois les algorithmes CBoW et Skip-Gram pour convertir des mots en vecteurs.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fr.png)

> Image tir√©e [de cet article](https://arxiv.org/pdf/1301.3781.pdf)

Les int√©grations pr√©-entra√Æn√©es Word2Vec (ainsi que d'autres mod√®les similaires, comme GloVe) peuvent √©galement √™tre utilis√©es √† la place de la couche d'int√©gration dans les r√©seaux de neurones. Cependant, nous devons traiter les vocabulaires, car le vocabulaire utilis√© pour pr√©-entra√Æner Word2Vec/GloVe est susceptible de diff√©rer du vocabulaire de notre corpus de texte. Consultez les cahiers ci-dessus pour voir comment ce probl√®me peut √™tre r√©solu.

## Int√©grations contextuelles

Une limitation cl√© des repr√©sentations d'int√©gration pr√©-entra√Æn√©es traditionnelles telles que Word2Vec est le probl√®me de la d√©sambigu√Øsation du sens des mots. Bien que les int√©grations pr√©-entra√Æn√©es puissent capturer une partie du sens des mots dans un contexte, chaque signification possible d'un mot est encod√©e dans la m√™me int√©gration. Cela peut poser des probl√®mes dans les mod√®les en aval, car de nombreux mots, comme le mot 'play', ont des significations diff√©rentes selon le contexte dans lequel ils sont utilis√©s.

Par exemple, le mot 'play' dans ces deux phrases diff√©rentes a des significations assez diff√©rentes :

- Je suis all√© √† une **pi√®ce** au th√©√¢tre.
- John veut **jouer** avec ses amis.

Les int√©grations pr√©-entra√Æn√©es ci-dessus repr√©sentent ces deux significations du mot 'play' dans la m√™me int√©gration. Pour surmonter cette limitation, nous devons construire des int√©grations bas√©es sur le **mod√®le linguistique**, qui est entra√Æn√© sur un grand corpus de texte et *sait* comment les mots peuvent √™tre assembl√©s dans diff√©rents contextes. La discussion sur les int√©grations contextuelles est hors de port√©e de ce tutoriel, mais nous y reviendrons lorsque nous parlerons des mod√®les linguistiques plus tard dans le cours.

## Conclusion

Dans cette le√ßon, vous avez d√©couvert comment construire et utiliser des couches d'int√©gration dans TensorFlow et Pytorch pour mieux refl√©ter les significations s√©mantiques des mots.

## üöÄ D√©fi

Word2Vec a √©t√© utilis√© pour des applications int√©ressantes, y compris la g√©n√©ration de paroles de chansons et de po√©sie. Jetez un ≈ìil √† [cet article](https://www.politetype.com/blog/word2vec-color-poems) qui explique comment l'auteur a utilis√© Word2Vec pour g√©n√©rer de la po√©sie. Regardez √©galement [cette vid√©o de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) pour d√©couvrir une autre explication de cette technique. Ensuite, essayez d'appliquer ces techniques √† votre propre corpus de texte, peut-√™tre provenant de Kaggle.

## [Quiz apr√®s le cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Revue & Auto-apprentissage

Lisez cet article sur Word2Vec : [Estimation efficace des repr√©sentations de mots dans l'espace vectoriel](https://arxiv.org/pdf/1301.3781.pdf)

## [Devoir : Cahiers](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source autoris√©e. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.