# Repr√©sentation du Texte en Tenseurs

## [Quiz pr√©-cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Classification de Texte

Tout au long de la premi√®re partie de cette section, nous allons nous concentrer sur la t√¢che de **classification de texte**. Nous utiliserons le jeu de donn√©es [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), qui contient des articles de presse comme les suivants :

* Cat√©gorie : Sci/Tech
* Titre : Une entreprise du Kentucky remporte une subvention pour √©tudier les peptides (AP)
* Corps : AP - Une entreprise fond√©e par un chercheur en chimie √† l'Universit√© de Louisville a remport√© une subvention pour d√©velopper...

Notre objectif sera de classer l'article d'actualit√© dans l'une des cat√©gories en fonction du texte.

## Repr√©sentation du texte

Si nous voulons r√©soudre des t√¢ches de traitement du langage naturel (NLP) avec des r√©seaux neuronaux, nous avons besoin d'un moyen de repr√©senter le texte sous forme de tenseurs. Les ordinateurs repr√©sentent d√©j√† les caract√®res textuels sous forme de nombres qui se mappent √† des polices sur votre √©cran en utilisant des encodages tels que ASCII ou UTF-8.

<img alt="Image montrant un diagramme mappant un caract√®re √† une repr√©sentation ASCII et binaire" src="images/ascii-character-map.png" width="50%"/>

> [Source de l'image](https://www.seobility.net/en/wiki/ASCII)

En tant qu'√™tres humains, nous comprenons ce que chaque lettre **repr√©sente**, et comment tous les caract√®res se combinent pour former les mots d'une phrase. Cependant, les ordinateurs, √† eux seuls, n'ont pas une telle compr√©hension, et le r√©seau neuronal doit apprendre la signification pendant l'entra√Ænement.

Par cons√©quent, nous pouvons utiliser diff√©rentes approches pour repr√©senter le texte :

* **Repr√©sentation au niveau des caract√®res**, lorsque nous repr√©sentons le texte en traitant chaque caract√®re comme un nombre. √âtant donn√© que nous avons *C* caract√®res diff√©rents dans notre corpus de texte, le mot *Hello* serait repr√©sent√© par un tenseur 5x*C*. Chaque lettre correspondrait √† une colonne de tenseur en encodage one-hot.
* **Repr√©sentation au niveau des mots**, dans laquelle nous cr√©ons un **vocabulaire** de tous les mots de notre texte, puis repr√©sentons les mots en utilisant l'encodage one-hot. Cette approche est quelque peu meilleure, car chaque lettre prise isol√©ment n'a pas beaucoup de sens, et donc en utilisant des concepts s√©mantiques de niveau sup√©rieur - les mots - nous simplifions la t√¢che pour le r√©seau neuronal. Cependant, √©tant donn√© la taille importante du dictionnaire, nous devons g√©rer des tenseurs creux de haute dimension.

Quelle que soit la repr√©sentation, nous devons d'abord convertir le texte en une s√©quence de **tokens**, chaque token √©tant soit un caract√®re, un mot, ou parfois m√™me une partie d'un mot. Ensuite, nous convertissons le token en un nombre, g√©n√©ralement en utilisant un **vocabulaire**, et ce nombre peut √™tre aliment√© dans un r√©seau neuronal en utilisant l'encodage one-hot.

## N-Grams

Dans le langage naturel, le sens pr√©cis des mots ne peut √™tre d√©termin√© que dans un contexte. Par exemple, les significations de *r√©seau neuronal* et *r√©seau de p√™che* sont compl√®tement diff√©rentes. L'une des fa√ßons de prendre cela en compte est de construire notre mod√®le sur des paires de mots, en consid√©rant les paires de mots comme des tokens de vocabulaire s√©par√©s. De cette mani√®re, la phrase *J'aime aller p√™cher* sera repr√©sent√©e par la s√©quence de tokens suivante : *J'aime*, *aime aller*, *aller p√™cher*. Le probl√®me avec cette approche est que la taille du dictionnaire augmente consid√©rablement, et des combinaisons comme *aller p√™cher* et *aller faire du shopping* sont pr√©sent√©es par des tokens diff√©rents, qui ne partagent aucune similarit√© s√©mantique malgr√© le m√™me verbe.

Dans certains cas, nous pouvons envisager d'utiliser des tri-grammes -- combinaisons de trois mots -- √©galement. Ainsi, l'approche est souvent appel√©e **n-grams**. De plus, il est logique d'utiliser des n-grams avec une repr√©sentation au niveau des caract√®res, auquel cas les n-grams correspondront √† diff√©rents syllabes.

## Sac de Mots et TF/IDF

Lors de la r√©solution de t√¢ches telles que la classification de texte, nous devons √™tre en mesure de repr√©senter le texte par un vecteur de taille fixe, que nous utiliserons comme entr√©e pour le classificateur dense final. L'une des fa√ßons les plus simples de le faire est de combiner toutes les repr√©sentations de mots individuelles, par exemple en les additionnant. Si nous ajoutons les encodages one-hot de chaque mot, nous finirons par un vecteur de fr√©quences, montrant combien de fois chaque mot appara√Æt dans le texte. Cette repr√©sentation du texte est appel√©e **sac de mots** (BoW).

<img src="images/bow.png" width="90%"/>

> Image par l'auteur

Un BoW repr√©sente essentiellement quels mots apparaissent dans le texte et en quelles quantit√©s, ce qui peut effectivement √™tre une bonne indication de ce dont parle le texte. Par exemple, un article de presse sur la politique contiendra probablement des mots tels que *pr√©sident* et *pays*, tandis qu'une publication scientifique aurait des termes comme *collisionneur*, *d√©couvert*, etc. Ainsi, les fr√©quences des mots peuvent dans de nombreux cas √™tre un bon indicateur du contenu du texte.

Le probl√®me avec le BoW est que certains mots communs, tels que *et*, *est*, etc. apparaissent dans la plupart des textes, et ils ont les fr√©quences les plus √©lev√©es, masquant les mots qui sont vraiment importants. Nous pouvons r√©duire l'importance de ces mots en tenant compte de la fr√©quence √† laquelle les mots apparaissent dans l'ensemble de la collection de documents. C'est l'id√©e principale derri√®re l'approche TF/IDF, qui est abord√©e plus en d√©tail dans les notebooks joints √† cette le√ßon.

Cependant, aucune de ces approches ne peut pleinement prendre en compte la **s√©mantique** du texte. Nous avons besoin de mod√®les de r√©seaux neuronaux plus puissants pour cela, que nous discuterons plus tard dans cette section.

## ‚úçÔ∏è Exercices : Repr√©sentation du Texte

Poursuivez votre apprentissage dans les notebooks suivants :

* [Repr√©sentation du Texte avec PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Repr√©sentation du Texte avec TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Conclusion

Jusqu'√† pr√©sent, nous avons √©tudi√© des techniques qui peuvent ajouter un poids de fr√©quence √† diff√©rents mots. Cependant, elles ne peuvent pas repr√©senter le sens ou l'ordre. Comme l'a dit le c√©l√®bre linguiste J. R. Firth en 1935, "Le sens complet d'un mot est toujours contextuel, et aucune √©tude du sens en dehors du contexte ne peut √™tre prise au s√©rieux." Nous apprendrons plus tard dans le cours comment capturer les informations contextuelles √† partir du texte en utilisant la mod√©lisation du langage.

## üöÄ D√©fi

Essayez d'autres exercices en utilisant le sac de mots et diff√©rents mod√®les de donn√©es. Vous pourriez √™tre inspir√© par cette [comp√©tition sur Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz post-cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Revue & Auto-apprentissage

Pratiquez vos comp√©tences avec les embeddings de texte et les techniques de sac de mots sur [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Devoir : Notebooks](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous visons √† garantir l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.