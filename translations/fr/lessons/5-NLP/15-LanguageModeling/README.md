# Mod√©lisation du Langage

Les embeddings s√©mantiques, tels que Word2Vec et GloVe, repr√©sentent en fait une premi√®re √©tape vers la **mod√©lisation du langage** - cr√©er des mod√®les qui comprennent (ou repr√©sentent) d'une certaine mani√®re la nature du langage.

## [Quiz pr√©-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

L'id√©e principale derri√®re la mod√©lisation du langage est de les entra√Æner sur des ensembles de donn√©es non √©tiquet√©s de mani√®re non supervis√©e. Cela est important car nous disposons de quantit√©s √©normes de texte non √©tiquet√©, tandis que la quantit√© de texte √©tiquet√© serait toujours limit√©e par l'effort que nous pouvons consacrer √† l'√©tiquetage. Le plus souvent, nous pouvons construire des mod√®les de langage capables de **pr√©dire les mots manquants** dans le texte, car il est facile de masquer un mot al√©atoire dans le texte et de l'utiliser comme √©chantillon d'entra√Ænement.

## Entra√Ænement des Embeddings

Dans nos exemples pr√©c√©dents, nous avons utilis√© des embeddings s√©mantiques pr√©-entra√Æn√©s, mais il est int√©ressant de voir comment ces embeddings peuvent √™tre entra√Æn√©s. Plusieurs id√©es possibles peuvent √™tre utilis√©es :

* Mod√©lisation du langage **N-Gram**, lorsque nous pr√©disons un jeton en regardant N jetons pr√©c√©dents (N-gram)
* **Continuous Bag-of-Words** (CBoW), lorsque nous pr√©disons le jeton du milieu $W_0$ dans une s√©quence de jetons $W_{-N}$, ..., $W_N$.
* **Skip-gram**, o√π nous pr√©disons un ensemble de jetons voisins {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} √† partir du jeton du milieu $W_0$.

![image provenant d'un article sur la conversion de mots en vecteurs](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.fr.png)

> Image tir√©e [de cet article](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Notebooks d'Exemple : Entra√Ænement du mod√®le CBoW

Poursuivez votre apprentissage dans les notebooks suivants :

* [Entra√Ænement de CBoW Word2Vec avec TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Entra√Ænement de CBoW Word2Vec avec PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Conclusion

Dans la le√ßon pr√©c√©dente, nous avons vu que les embeddings de mots fonctionnent comme par magie ! Maintenant, nous savons que l'entra√Ænement des embeddings de mots n'est pas une t√¢che tr√®s complexe, et nous devrions √™tre en mesure d'entra√Æner nos propres embeddings de mots pour un texte sp√©cifique √† un domaine si n√©cessaire.

## [Quiz post-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Revue & Auto-apprentissage

* [Tutoriel officiel PyTorch sur la Mod√©lisation du Langage](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutoriel officiel TensorFlow sur l'entra√Ænement du mod√®le Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* L'utilisation du cadre **gensim** pour entra√Æner les embeddings les plus couramment utilis√©s en quelques lignes de code est d√©crite [dans cette documentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Devoir : Entra√Æner un Mod√®le Skip-Gram](lab/README.md)

Dans le laboratoire, nous vous mettons au d√©fi de modifier le code de cette le√ßon pour entra√Æner un mod√®le skip-gram au lieu de CBoW. [Lisez les d√©tails](lab/README.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous visons √† garantir l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue natale doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction professionnelle par un humain est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.