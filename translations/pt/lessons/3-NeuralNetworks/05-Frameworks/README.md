# Estruturas de Redes Neurais

Como j√° aprendemos, para treinar redes neurais de forma eficiente, precisamos fazer duas coisas:

* Operar em tensores, por exemplo, multiplicar, adicionar e calcular algumas fun√ß√µes como sigmoid ou softmax
* Calcular gradientes de todas as express√µes, a fim de realizar a otimiza√ß√£o por descida de gradiente

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Enquanto a biblioteca `numpy` pode fazer a primeira parte, precisamos de algum mecanismo para calcular gradientes. Em [nossa estrutura](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) que desenvolvemos na se√ß√£o anterior, tivemos que programar manualmente todas as fun√ß√µes derivadas dentro do m√©todo `backward`, que realiza a retropropaga√ß√£o. Idealmente, uma estrutura deveria nos dar a oportunidade de calcular gradientes de *qualquer express√£o* que pudermos definir.

Outra coisa importante √© poder realizar c√°lculos em GPU, ou em qualquer outra unidade de computa√ß√£o especializada, como [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). O treinamento de redes neurais profundas requer *muitos* c√°lculos, e poder paralelizar esses c√°lculos em GPUs √© muito importante.

> ‚úÖ O termo 'paralelizar' significa distribuir os c√°lculos entre v√°rios dispositivos.

Atualmente, as duas estruturas de rede neural mais populares s√£o: [TensorFlow](http://TensorFlow.org) e [PyTorch](https://pytorch.org/). Ambas fornecem uma API de baixo n√≠vel para operar com tensores tanto na CPU quanto na GPU. Acima da API de baixo n√≠vel, tamb√©m existe uma API de n√≠vel superior, chamada [Keras](https://keras.io/) e [PyTorch Lightning](https://pytorchlightning.ai/) respectivamente.

API de Baixo N√≠vel | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
API de Alto N√≠vel| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**APIs de baixo n√≠vel** em ambas as estruturas permitem que voc√™ construa os chamados **gr√°ficos computacionais**. Este gr√°fico define como calcular a sa√≠da (geralmente a fun√ß√£o de perda) com par√¢metros de entrada dados, e pode ser enviado para computa√ß√£o na GPU, se estiver dispon√≠vel. Existem fun√ß√µes para diferenciar esse gr√°fico computacional e calcular gradientes, que podem ser usados para otimizar os par√¢metros do modelo.

**APIs de alto n√≠vel** consideram as redes neurais como uma **sequ√™ncia de camadas**, tornando a constru√ß√£o da maioria das redes neurais muito mais f√°cil. O treinamento do modelo geralmente requer a prepara√ß√£o dos dados e, em seguida, a chamada de uma fun√ß√£o `fit` para realizar o trabalho.

A API de alto n√≠vel permite que voc√™ construa redes neurais t√≠picas muito rapidamente, sem se preocupar com muitos detalhes. Ao mesmo tempo, a API de baixo n√≠vel oferece muito mais controle sobre o processo de treinamento, e, portanto, √© muito utilizada em pesquisas, quando se lida com novas arquiteturas de redes neurais.

√â tamb√©m importante entender que voc√™ pode usar ambas as APIs juntas, por exemplo, voc√™ pode desenvolver sua pr√≥pria arquitetura de camada de rede usando a API de baixo n√≠vel e, em seguida, us√°-la dentro da rede maior constru√≠da e treinada com a API de alto n√≠vel. Ou voc√™ pode definir uma rede usando a API de alto n√≠vel como uma sequ√™ncia de camadas e, em seguida, usar seu pr√≥prio loop de treinamento de baixo n√≠vel para realizar a otimiza√ß√£o. Ambas as APIs usam os mesmos conceitos b√°sicos subjacentes e foram projetadas para funcionar bem juntas.

## Aprendizado

Neste curso, oferecemos a maior parte do conte√∫do tanto para PyTorch quanto para TensorFlow. Voc√™ pode escolher sua estrutura preferida e apenas passar pelos notebooks correspondentes. Se voc√™ n√£o tem certeza de qual estrutura escolher, leia algumas discuss√µes na internet sobre **PyTorch vs. TensorFlow**. Voc√™ tamb√©m pode dar uma olhada em ambas as estruturas para ter uma melhor compreens√£o.

Onde for poss√≠vel, usaremos APIs de Alto N√≠vel por simplicidade. No entanto, acreditamos que √© importante entender como as redes neurais funcionam desde o in√≠cio, portanto, no come√ßo, come√ßamos trabalhando com a API de baixo n√≠vel e tensores. No entanto, se voc√™ deseja avan√ßar rapidamente e n√£o quer gastar muito tempo aprendendo esses detalhes, pode pular essas partes e ir direto para os notebooks da API de alto n√≠vel.

## ‚úçÔ∏è Exerc√≠cios: Estruturas

Continue seu aprendizado nos seguintes notebooks:

API de Baixo N√≠vel | [Notebook TensorFlow+Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
API de Alto N√≠vel| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Ap√≥s dominar as estruturas, vamos recapitular a no√ß√£o de overfitting.

# Overfitting

Overfitting √© um conceito extremamente importante em aprendizado de m√°quina, e √© muito importante compreend√™-lo corretamente!

Considere o seguinte problema de aproxima√ß√£o de 5 pontos (representados por `x` nos gr√°ficos abaixo):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.pt.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.pt.jpg)
-------------------------|--------------------------
**Modelo linear, 2 par√¢metros** | **Modelo n√£o linear, 7 par√¢metros**
Erro de treinamento = 5.3 | Erro de valida√ß√£o = 0
Erro de valida√ß√£o = 5.1 | Erro de valida√ß√£o = 20

* √Ä esquerda, vemos uma boa aproxima√ß√£o em linha reta. Como o n√∫mero de par√¢metros √© adequado, o modelo capta a ideia por tr√°s da distribui√ß√£o dos pontos corretamente.
* √Ä direita, o modelo √© poderoso demais. Como temos apenas 5 pontos e o modelo possui 7 par√¢metros, ele pode se ajustar de tal forma que passe por todos os pontos, fazendo com que o erro de treinamento seja 0. No entanto, isso impede que o modelo entenda o padr√£o correto por tr√°s dos dados, portanto, o erro de valida√ß√£o √© muito alto.

√â muito importante encontrar um equil√≠brio correto entre a riqueza do modelo (n√∫mero de par√¢metros) e o n√∫mero de amostras de treinamento.

## Por que o overfitting ocorre

  * Dados de treinamento insuficientes
  * Modelo muito poderoso
  * Muito ru√≠do nos dados de entrada

## Como detectar overfitting

Como voc√™ pode ver no gr√°fico acima, o overfitting pode ser detectado por um erro de treinamento muito baixo e um erro de valida√ß√£o alto. Normalmente, durante o treinamento, veremos tanto os erros de treinamento quanto de valida√ß√£o come√ßando a diminuir, e ent√£o em algum momento o erro de valida√ß√£o pode parar de diminuir e come√ßar a aumentar. Isso ser√° um sinal de overfitting e um indicativo de que provavelmente devemos parar o treinamento neste ponto (ou pelo menos fazer uma c√≥pia do modelo).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.pt.png)

## Como prevenir o overfitting

Se voc√™ perceber que o overfitting est√° ocorrendo, voc√™ pode fazer uma das seguintes a√ß√µes:

 * Aumentar a quantidade de dados de treinamento
 * Diminuir a complexidade do modelo
 * Usar alguma [t√©cnica de regulariza√ß√£o](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), como [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), que consideraremos mais tarde.

## Overfitting e a Troca de Vi√©s-Vari√¢ncia

Overfitting √©, na verdade, um caso de um problema mais gen√©rico em estat√≠sticas chamado [Troca de Vi√©s-Vari√¢ncia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Se considerarmos as poss√≠veis fontes de erro em nosso modelo, podemos ver dois tipos de erros:

* **Erros de vi√©s** s√£o causados pelo nosso algoritmo n√£o conseguir capturar corretamente a rela√ß√£o entre os dados de treinamento. Isso pode resultar do fato de que nosso modelo n√£o √© poderoso o suficiente (**underfitting**).
* **Erros de vari√¢ncia**, que s√£o causados pelo modelo aproximando o ru√≠do nos dados de entrada em vez de uma rela√ß√£o significativa (**overfitting**).

Durante o treinamento, o erro de vi√©s diminui (√† medida que nosso modelo aprende a aproximar os dados), e o erro de vari√¢ncia aumenta. √â importante parar o treinamento - seja manualmente (quando detectamos overfitting) ou automaticamente (introduzindo regulariza√ß√£o) - para evitar o overfitting.

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre as diferen√ßas entre as v√°rias APIs para os dois frameworks de IA mais populares, TensorFlow e PyTorch. Al√©m disso, voc√™ aprendeu sobre um t√≥pico muito importante, o overfitting.

## üöÄ Desafio

Nos notebooks acompanhantes, voc√™ encontrar√° 'tarefas' na parte inferior; trabalhe atrav√©s dos notebooks e complete as tarefas.

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Revis√£o e Autoestudo

Fa√ßa uma pesquisa sobre os seguintes t√≥picos:

- TensorFlow
- PyTorch
- Overfitting

Pergunte a si mesmo as seguintes quest√µes:

- Qual √© a diferen√ßa entre TensorFlow e PyTorch?
- Qual √© a diferen√ßa entre overfitting e underfitting?

## [Tarefa](lab/README.md)

Neste laborat√≥rio, voc√™ √© solicitado a resolver dois problemas de classifica√ß√£o usando redes totalmente conectadas de camadas √∫nica e m√∫ltipla, usando PyTorch ou TensorFlow.

* [Instru√ß√µes](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes err√¥neas decorrentes do uso desta tradu√ß√£o.