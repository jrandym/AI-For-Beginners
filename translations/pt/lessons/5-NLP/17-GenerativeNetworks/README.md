# Redes generativas

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Redes Neurais Recorrentes (RNNs) e suas variantes com c√©lulas portadas, como C√©lulas de Mem√≥ria de Longo e Curto Prazo (LSTMs) e Unidades Recorrentes Portadas (GRUs), fornecem um mecanismo para modelagem de linguagem, pois podem aprender a ordem das palavras e fazer previs√µes para a pr√≥xima palavra em uma sequ√™ncia. Isso nos permite usar RNNs para **tarefas generativas**, como gera√ß√£o de texto comum, tradu√ß√£o autom√°tica e at√© mesmo legendagem de imagens.

> ‚úÖ Pense em todas as vezes que voc√™ se beneficiou de tarefas generativas, como a conclus√£o de texto enquanto digita. Fa√ßa uma pesquisa sobre seus aplicativos favoritos para ver se eles utilizaram RNNs.

Na arquitetura de RNN que discutimos na unidade anterior, cada unidade RNN produzia o pr√≥ximo estado oculto como uma sa√≠da. No entanto, tamb√©m podemos adicionar outra sa√≠da a cada unidade recorrente, o que nos permitiria gerar uma **sequ√™ncia** (que tem o mesmo comprimento da sequ√™ncia original). Al√©m disso, podemos usar unidades RNN que n√£o aceitam uma entrada em cada passo, e apenas pegam algum vetor de estado inicial, e ent√£o produzem uma sequ√™ncia de sa√≠das.

Isso permite diferentes arquiteturas neurais que s√£o mostradas na imagem abaixo:

![Imagem mostrando padr√µes comuns de redes neurais recorrentes.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.pt.jpg)

> Imagem do post do blog [Efic√°cia Irresist√≠vel das Redes Neurais Recorrentes](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) de [Andrej Karpaty](http://karpathy.github.io/)

* **Um-para-um** √© uma rede neural tradicional com uma entrada e uma sa√≠da
* **Um-para-muitos** √© uma arquitetura generativa que aceita um valor de entrada e gera uma sequ√™ncia de valores de sa√≠da. Por exemplo, se quisermos treinar uma rede de **legendagem de imagens** que produza uma descri√ß√£o textual de uma imagem, podemos usar uma imagem como entrada, pass√°-la por uma CNN para obter seu estado oculto, e ent√£o ter uma cadeia recorrente gerando a legenda palavra por palavra
* **Muitos-para-um** corresponde √†s arquiteturas RNN que descrevemos na unidade anterior, como a classifica√ß√£o de texto
* **Muitos-para-muitos**, ou **sequ√™ncia-para-sequ√™ncia**, corresponde a tarefas como **tradu√ß√£o autom√°tica**, onde temos a primeira RNN coletando todas as informa√ß√µes da sequ√™ncia de entrada no estado oculto, e outra cadeia RNN desenrolando esse estado na sequ√™ncia de sa√≠da.

Nesta unidade, vamos nos concentrar em modelos generativos simples que nos ajudam a gerar texto. Para simplificar, usaremos tokeniza√ß√£o em n√≠vel de caractere.

Vamos treinar esta RNN para gerar texto passo a passo. Em cada passo, pegaremos uma sequ√™ncia de caracteres de comprimento `nchars` e pediremos √† rede para gerar o pr√≥ximo caractere de sa√≠da para cada caractere de entrada:

![Imagem mostrando um exemplo de gera√ß√£o RNN da palavra 'OL√Å'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.pt.png)

Ao gerar texto (durante a infer√™ncia), come√ßamos com algum **prompt**, que √© passado pelas c√©lulas RNN para gerar seu estado intermedi√°rio, e ent√£o a partir desse estado a gera√ß√£o come√ßa. Geramos um caractere de cada vez e passamos o estado e o caractere gerado para outra c√©lula RNN para gerar o pr√≥ximo, at√© gerarmos caracteres suficientes.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Redes Generativas

Continue seu aprendizado nos seguintes notebooks:

* [Redes Generativas com PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Redes Generativas com TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Gera√ß√£o de texto suave e temperatura

A sa√≠da de cada c√©lula RNN √© uma distribui√ß√£o de probabilidade de caracteres. Se sempre pegarmos o caractere com a maior probabilidade como o pr√≥ximo caractere no texto gerado, o texto pode frequentemente "circular" entre as mesmas sequ√™ncias de caracteres repetidamente, como neste exemplo:

```
today of the second the company and a second the company ...
```

No entanto, se olharmos para a distribui√ß√£o de probabilidade do pr√≥ximo caractere, pode ser que a diferen√ßa entre algumas das maiores probabilidades n√£o seja enorme, por exemplo, um caractere pode ter probabilidade 0.2, outro - 0.19, etc. Por exemplo, ao procurar o pr√≥ximo caractere na sequ√™ncia '*play*', o pr√≥ximo caractere pode ser igualmente um espa√ßo ou **e** (como na palavra *player*).

Isso nos leva √† conclus√£o de que n√£o √© sempre "justo" selecionar o caractere com a maior probabilidade, porque escolher o segundo mais alto ainda pode nos levar a um texto significativo. √â mais s√°bio **amostrar** caracteres da distribui√ß√£o de probabilidade dada pela sa√≠da da rede. Tamb√©m podemos usar um par√¢metro, **temperatura**, que ir√° achatar a distribui√ß√£o de probabilidade, caso queiramos adicionar mais aleatoriedade, ou torn√°-la mais acentuada, se quisermos nos apegar mais aos caracteres de maior probabilidade.

Explore como essa gera√ß√£o de texto suave √© implementada nos notebooks vinculados acima.

## Conclus√£o

Embora a gera√ß√£o de texto possa ser √∫til por si s√≥, os principais benef√≠cios v√™m da capacidade de gerar texto usando RNNs a partir de algum vetor de caracter√≠sticas inicial. Por exemplo, a gera√ß√£o de texto √© usada como parte da tradu√ß√£o autom√°tica (sequ√™ncia-para-sequ√™ncia, neste caso o vetor de estado do *codificador* √© usado para gerar ou *decodificar* a mensagem traduzida), ou gerando uma descri√ß√£o textual de uma imagem (caso em que o vetor de caracter√≠sticas viria do extrator CNN).

## üöÄ Desafio

Fa√ßa algumas li√ß√µes no Microsoft Learn sobre este t√≥pico

* Gera√ß√£o de Texto com [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Revis√£o & Estudo Aut√¥nomo

Aqui est√£o alguns artigos para expandir seu conhecimento

* Diferentes abordagens para gera√ß√£o de texto com Cadeia de Markov, LSTM e GPT-2: [post do blog](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exemplo de gera√ß√£o de texto na [documenta√ß√£o do Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Tarefa](lab/README.md)

Vimos como gerar texto caractere por caractere. No laborat√≥rio, voc√™ explorar√° a gera√ß√£o de texto em n√≠vel de palavra.

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido usando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional feita por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.