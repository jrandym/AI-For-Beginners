# Representando Texto como Tensores

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Classifica√ß√£o de Texto

Na primeira parte desta se√ß√£o, vamos nos concentrar na tarefa de **classifica√ß√£o de texto**. Usaremos o conjunto de dados [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset), que cont√©m artigos de not√≠cias como os seguintes:

* Categoria: Sci/Tech
* T√≠tulo: Ky. Company Wins Grant to Study Peptides (AP)
* Corpo: AP - Uma empresa fundada por um pesquisador de qu√≠mica na Universidade de Louisville ganhou uma bolsa para desenvolver...

Nosso objetivo ser√° classificar o item de not√≠cia em uma das categorias com base no texto.

## Representando texto

Se quisermos resolver tarefas de Processamento de Linguagem Natural (NLP) com redes neurais, precisamos de alguma forma de representar texto como tensores. Os computadores j√° representam caracteres textuais como n√∫meros que mapeiam para fontes na sua tela usando codifica√ß√µes como ASCII ou UTF-8.

<img alt="Imagem mostrando diagrama mapeando um caractere para uma representa√ß√£o ASCII e bin√°ria" src="images/ascii-character-map.png" width="50%"/>

> [Fonte da imagem](https://www.seobility.net/en/wiki/ASCII)

Como humanos, entendemos o que cada letra **representa** e como todos os caracteres se juntam para formar as palavras de uma frase. No entanto, os computadores, por si s√≥, n√£o t√™m essa compreens√£o, e a rede neural precisa aprender o significado durante o treinamento.

Portanto, podemos usar diferentes abordagens ao representar texto:

* **Representa√ß√£o em n√≠vel de caractere**, quando representamos o texto tratando cada caractere como um n√∫mero. Dado que temos *C* caracteres diferentes em nosso corpus de texto, a palavra *Hello* seria representada por um tensor 5x*C*. Cada letra corresponderia a uma coluna de tensor em codifica√ß√£o one-hot.
* **Representa√ß√£o em n√≠vel de palavra**, na qual criamos um **vocabul√°rio** de todas as palavras em nosso texto e, em seguida, representamos as palavras usando codifica√ß√£o one-hot. Essa abordagem √© de certa forma melhor, porque cada letra isoladamente n√£o tem muito significado e, assim, ao usar conceitos sem√¢nticos de n√≠vel superior - palavras - simplificamos a tarefa para a rede neural. No entanto, dado o grande tamanho do dicion√°rio, precisamos lidar com tensores esparsos de alta dimens√£o.

Independentemente da representa√ß√£o, primeiro precisamos converter o texto em uma sequ√™ncia de **tokens**, sendo que um token pode ser um caractere, uma palavra ou, √†s vezes, at√© mesmo parte de uma palavra. Em seguida, convertemos o token em um n√∫mero, tipicamente usando **vocabul√°rio**, e esse n√∫mero pode ser alimentado em uma rede neural usando codifica√ß√£o one-hot.

## N-Gramas

Na linguagem natural, o significado preciso das palavras s√≥ pode ser determinado no contexto. Por exemplo, os significados de *rede neural* e *rede de pesca* s√£o completamente diferentes. Uma das maneiras de levar isso em conta √© construir nosso modelo com pares de palavras, considerando pares de palavras como tokens de vocabul√°rio separados. Dessa forma, a frase *Eu gosto de pescar* ser√° representada pela seguinte sequ√™ncia de tokens: *Eu gosto*, *gosto de*, *de pescar*. O problema com essa abordagem √© que o tamanho do dicion√°rio cresce significativamente, e combina√ß√µes como *ir pescar* e *ir √†s compras* s√£o apresentadas por tokens diferentes, que n√£o compartilham nenhuma semelhan√ßa sem√¢ntica, apesar do mesmo verbo.

Em alguns casos, tamb√©m podemos considerar o uso de tri-gramas -- combina√ß√µes de tr√™s palavras --. Assim, a abordagem √© frequentemente chamada de **n-gramas**. Al√©m disso, faz sentido usar n-gramas com representa√ß√£o em n√≠vel de caractere, nesse caso os n-gramas corresponder√£o aproximadamente a diferentes s√≠labas.

## Bag-of-Words e TF/IDF

Ao resolver tarefas como a classifica√ß√£o de texto, precisamos ser capazes de representar o texto por um vetor de tamanho fixo, que usaremos como entrada para o classificador denso final. Uma das maneiras mais simples de fazer isso √© combinar todas as representa√ß√µes individuais de palavras, por exemplo, somando-as. Se adicionarmos as codifica√ß√µes one-hot de cada palavra, terminaremos com um vetor de frequ√™ncias, mostrando quantas vezes cada palavra aparece dentro do texto. Essa representa√ß√£o de texto √© chamada de **bag of words** (BoW).

<img src="images/bow.png" width="90%"/>

> Imagem do autor

Um BoW representa essencialmente quais palavras aparecem no texto e em quais quantidades, o que pode de fato ser uma boa indica√ß√£o do que o texto trata. Por exemplo, um artigo de not√≠cias sobre pol√≠tica provavelmente conter√° palavras como *presidente* e *pa√≠s*, enquanto uma publica√ß√£o cient√≠fica teria algo como *colisor*, *descoberto*, etc. Assim, as frequ√™ncias das palavras podem, em muitos casos, ser um bom indicador do conte√∫do do texto.

O problema com o BoW √© que certas palavras comuns, como *e*, *√©*, etc., aparecem na maioria dos textos e t√™m as maiores frequ√™ncias, ofuscando as palavras que s√£o realmente importantes. Podemos diminuir a import√¢ncia dessas palavras levando em conta a frequ√™ncia com que as palavras ocorrem em toda a cole√ß√£o de documentos. Essa √© a ideia principal por tr√°s da abordagem TF/IDF, que √© abordada com mais detalhes nos notebooks anexados a esta li√ß√£o.

No entanto, nenhuma dessas abordagens pode levar plenamente em conta a **sem√¢ntica** do texto. Precisamos de modelos de redes neurais mais poderosos para fazer isso, que discutiremos mais adiante nesta se√ß√£o.

## ‚úçÔ∏è Exerc√≠cios: Representa√ß√£o de Texto

Continue seu aprendizado nos seguintes notebooks:

* [Representa√ß√£o de Texto com PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Representa√ß√£o de Texto com TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Conclus√£o

At√© agora, estudamos t√©cnicas que podem adicionar peso de frequ√™ncia a diferentes palavras. No entanto, elas n√£o conseguem representar significado ou ordem. Como disse o famoso linguista J. R. Firth em 1935: "O significado completo de uma palavra √© sempre contextual, e nenhum estudo de significado separado do contexto pode ser levado a s√©rio." Aprenderemos mais tarde no curso como capturar informa√ß√µes contextuais do texto usando modelagem de linguagem.

## üöÄ Desafio

Tente alguns outros exerc√≠cios usando bag-of-words e diferentes modelos de dados. Voc√™ pode se inspirar nesta [competi√ß√£o no Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## Revis√£o & Autoestudo

Pratique suas habilidades com embeddings de texto e t√©cnicas de bag-of-words em [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Tarefa: Notebooks](assignment.md)

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.