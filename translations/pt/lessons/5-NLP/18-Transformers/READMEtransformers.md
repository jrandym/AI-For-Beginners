# Mecanismos de Aten√ß√£o e Transformers

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

Um dos problemas mais importantes no dom√≠nio do PLN √© **tradu√ß√£o autom√°tica**, uma tarefa essencial que fundamenta ferramentas como o Google Translate. Nesta se√ß√£o, focaremos na tradu√ß√£o autom√°tica ou, de forma mais geral, em qualquer tarefa *sequ√™ncia-para-sequ√™ncia* (que tamb√©m √© chamada de **transdu√ß√£o de senten√ßas**).

Com RNNs, a sequ√™ncia-para-sequ√™ncia √© implementada por duas redes recorrentes, onde uma rede, o **codificador**, colapsa uma sequ√™ncia de entrada em um estado oculto, enquanto outra rede, o **decodificador**, desdobra esse estado oculto em um resultado traduzido. Existem alguns problemas com essa abordagem:

* O estado final da rede codificadora tem dificuldade em lembrar o in√≠cio de uma senten√ßa, causando assim uma qualidade ruim do modelo para senten√ßas longas.
* Todas as palavras em uma sequ√™ncia t√™m o mesmo impacto no resultado. Na realidade, no entanto, palavras espec√≠ficas na sequ√™ncia de entrada frequentemente t√™m mais impacto nas sa√≠das sequenciais do que outras.

**Mecanismos de Aten√ß√£o** fornecem um meio de ponderar o impacto contextual de cada vetor de entrada em cada previs√£o de sa√≠da da RNN. A forma como √© implementado √© criando atalhos entre estados intermedi√°rios da RNN de entrada e da RNN de sa√≠da. Dessa maneira, ao gerar o s√≠mbolo de sa√≠da y<sub>t</sub>, levaremos em conta todos os estados ocultos de entrada h<sub>i</sub>, com diferentes coeficientes de peso Œ±<sub>t,i</sub>.

![Imagem mostrando um modelo codificador/decodificador com uma camada de aten√ß√£o aditiva](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.pt.png)

> O modelo codificador-decodificador com mecanismo de aten√ß√£o aditiva em [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), citado a partir [deste post de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

A matriz de aten√ß√£o {Œ±<sub>i,j</sub>} representaria o grau em que certas palavras de entrada desempenham na gera√ß√£o de uma palavra espec√≠fica na sequ√™ncia de sa√≠da. Abaixo est√° um exemplo de tal matriz:

![Imagem mostrando um alinhamento de exemplo encontrado por RNNsearch-50, tirada de Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.pt.png)

> Figura de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Os mecanismos de aten√ß√£o s√£o respons√°veis por grande parte do estado da arte atual ou quase atual em PLN. No entanto, adicionar aten√ß√£o aumenta significativamente o n√∫mero de par√¢metros do modelo, o que levou a problemas de escalabilidade com RNNs. Uma restri√ß√£o chave da escalabilidade das RNNs √© que a natureza recorrente dos modelos torna desafiador agrupar e paralelizar o treinamento. Em uma RNN, cada elemento de uma sequ√™ncia precisa ser processado em ordem sequencial, o que significa que n√£o pode ser facilmente paralelizado.

![Codificador Decodificador com Aten√ß√£o](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figura do [Blog do Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

A ado√ß√£o de mecanismos de aten√ß√£o combinada com essa restri√ß√£o levou √† cria√ß√£o dos agora modelos Transformer de Estado da Arte que conhecemos e usamos hoje, como BERT e Open-GPT3.

## Modelos Transformer

Uma das principais ideias por tr√°s dos transformers √© evitar a natureza sequencial das RNNs e criar um modelo que seja paraleliz√°vel durante o treinamento. Isso √© alcan√ßado implementando duas ideias:

* codifica√ß√£o posicional
* uso de mecanismo de autoaten√ß√£o para capturar padr√µes em vez de RNNs (ou CNNs) (√© por isso que o artigo que introduz os transformers √© chamado *[Aten√ß√£o √© tudo o que voc√™ precisa](https://arxiv.org/abs/1706.03762)*)

### Codifica√ß√£o/Embutimento Posicional

A ideia da codifica√ß√£o posicional √© a seguinte. 
1. Ao usar RNNs, a posi√ß√£o relativa dos tokens √© representada pelo n√∫mero de etapas e, portanto, n√£o precisa ser explicitamente representada. 
2. No entanto, uma vez que mudamos para aten√ß√£o, precisamos saber as posi√ß√µes relativas dos tokens dentro de uma sequ√™ncia. 
3. Para obter a codifica√ß√£o posicional, aumentamos nossa sequ√™ncia de tokens com uma sequ√™ncia de posi√ß√µes de tokens na sequ√™ncia (ou seja, uma sequ√™ncia de n√∫meros 0, 1, ...).
4. Em seguida, misturamos a posi√ß√£o do token com um vetor de embutimento de token. Para transformar a posi√ß√£o (inteiro) em um vetor, podemos usar diferentes abordagens:

* Embutimento trein√°vel, semelhante ao embutimento de token. Esta √© a abordagem que consideramos aqui. Aplicamos camadas de embutimento tanto em tokens quanto em suas posi√ß√µes, resultando em vetores de embutimento das mesmas dimens√µes, que ent√£o somamos.
* Fun√ß√£o de codifica√ß√£o de posi√ß√£o fixa, conforme proposto no artigo original.

<img src="images/pos-embedding.png" width="50%"/>

> Imagem do autor

O resultado que obtemos com o embutimento posicional incorpora tanto o token original quanto sua posi√ß√£o dentro de uma sequ√™ncia.

### Autoaten√ß√£o Multi-Cabe√ßa

Em seguida, precisamos capturar alguns padr√µes dentro da nossa sequ√™ncia. Para fazer isso, os transformers usam um mecanismo de **autoaten√ß√£o**, que √© essencialmente aten√ß√£o aplicada √† mesma sequ√™ncia como entrada e sa√≠da. A aplica√ß√£o de autoaten√ß√£o nos permite levar em conta o **contexto** dentro da senten√ßa e ver quais palavras est√£o inter-relacionadas. Por exemplo, isso nos permite ver quais palavras s√£o referidas por co-refer√™ncias, como *isso*, e tamb√©m levar o contexto em considera√ß√£o:

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.pt.png)

> Imagem do [Blog do Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Nos transformers, usamos **Aten√ß√£o Multi-Cabe√ßa** para dar ao modelo o poder de capturar v√°rios tipos diferentes de depend√™ncias, por exemplo, rela√ß√µes de palavras de longo prazo versus curto prazo, co-refer√™ncia versus algo mais, etc.

[Notebook TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) cont√©m mais detalhes sobre a implementa√ß√£o das camadas de transformer.

### Aten√ß√£o Codificador-Decodificador

Nos transformers, a aten√ß√£o √© usada em dois lugares:

* Para capturar padr√µes dentro do texto de entrada usando autoaten√ß√£o
* Para realizar tradu√ß√£o de sequ√™ncia - √© a camada de aten√ß√£o entre codificador e decodificador.

A aten√ß√£o codificador-decodificador √© muito semelhante ao mecanismo de aten√ß√£o usado em RNNs, como descrito no in√≠cio desta se√ß√£o. Este diagrama animado explica o papel da aten√ß√£o codificador-decodificador.

![GIF animado mostrando como as avalia√ß√µes s√£o realizadas em modelos transformer.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Como cada posi√ß√£o de entrada √© mapeada independentemente para cada posi√ß√£o de sa√≠da, os transformers podem ser paralelizados melhor do que as RNNs, o que permite modelos de linguagem muito maiores e mais expressivos. Cada cabe√ßa de aten√ß√£o pode ser usada para aprender diferentes rela√ß√µes entre palavras que melhoram as tarefas de Processamento de Linguagem Natural a jusante.

## BERT

**BERT** (Representa√ß√µes de Codificador Bidirecional de Transformers) √© uma rede transformer multilayer muito grande com 12 camadas para *BERT-base* e 24 para *BERT-large*. O modelo √© primeiro pr√©-treinado em um grande corpus de dados textuais (WikiPedia + livros) usando treinamento n√£o supervisionado (previs√£o de palavras mascaradas em uma senten√ßa). Durante o pr√©-treinamento, o modelo absorve n√≠veis significativos de compreens√£o da linguagem, que podem ser aproveitados com outros conjuntos de dados usando ajuste fino. Este processo √© chamado de **aprendizado por transfer√™ncia**.

![imagem de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.pt.png)

> Imagem [fonte](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exerc√≠cios: Transformers

Continue seu aprendizado nos seguintes notebooks:

* [Transformers em PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformers em TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre Transformers e Mecanismos de Aten√ß√£o, todas ferramentas essenciais no conjunto de ferramentas de PLN. Existem muitas varia√ß√µes das arquiteturas Transformer, incluindo BERT, DistilBERT, BigBird, OpenGPT3 e mais, que podem ser ajustadas. O [pacote HuggingFace](https://github.com/huggingface/) fornece um reposit√≥rio para treinar muitas dessas arquiteturas com PyTorch e TensorFlow.

## üöÄ Desafio

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## Revis√£o & Autoestudo

* [Post de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), explicando o cl√°ssico artigo [Aten√ß√£o √© tudo o que voc√™ precisa](https://arxiv.org/abs/1706.03762) sobre transformers.
* [Uma s√©rie de posts de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sobre transformers, explicando a arquitetura em detalhes.

## [Tarefa](assignment.md)

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido usando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional por um humano. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas que possam surgir do uso desta tradu√ß√£o.