# Embeddings

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Ao treinar classificadores baseados em BoW ou TF/IDF, operamos em vetores de bag-of-words de alta dimens√£o com comprimento `vocab_size`, e est√°vamos convertendo explicitamente de vetores de representa√ß√£o posicional de baixa dimens√£o para uma representa√ß√£o one-hot esparsa. No entanto, essa representa√ß√£o one-hot n√£o √© eficiente em termos de mem√≥ria. Al√©m disso, cada palavra √© tratada de forma independente, ou seja, vetores codificados em one-hot n√£o expressam nenhuma similaridade sem√¢ntica entre as palavras.

A ideia de **embedding** √© representar palavras por vetores densos de menor dimens√£o, que de alguma forma refletem o significado sem√¢ntico de uma palavra. Mais adiante, discutiremos como construir embeddings de palavras significativos, mas por enquanto, vamos pensar nos embeddings como uma forma de reduzir a dimensionalidade de um vetor de palavras.

Assim, a camada de embedding receberia uma palavra como entrada e produziria um vetor de sa√≠da de tamanho especificado `embedding_size`. De certa forma, √© muito semelhante a uma camada `Linear`, mas em vez de receber um vetor codificado em one-hot, ela poder√° receber um n√∫mero de palavra como entrada, permitindo-nos evitar a cria√ß√£o de grandes vetores codificados em one-hot.

Ao usar uma camada de embedding como a primeira camada em nossa rede de classificadores, podemos mudar de um modelo de bag-of-words para um modelo de **embedding bag**, onde primeiro convertemos cada palavra em nosso texto em seu correspondente embedding e, em seguida, calculamos alguma fun√ß√£o agregada sobre todos esses embeddings, como `sum`, `average` ou `max`.  

![Imagem mostrando um classificador de embedding para cinco palavras sequenciais.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.pt.png)

> Imagem do autor

## ‚úçÔ∏è Exerc√≠cios: Embeddings

Continue seu aprendizado nos seguintes notebooks:
* [Embeddings com PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Embeddings TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Embeddings Sem√¢nticos: Word2Vec

Enquanto a camada de embedding aprendeu a mapear palavras para representa√ß√£o vetorial, essa representa√ß√£o n√£o necessariamente tinha muito significado sem√¢ntico. Seria interessante aprender uma representa√ß√£o vetorial de modo que palavras semelhantes ou sin√¥nimos corresponderiam a vetores que est√£o pr√≥ximos uns dos outros em termos de alguma dist√¢ncia vetorial (por exemplo, dist√¢ncia euclidiana).

Para isso, precisamos pr√©-treinar nosso modelo de embedding em uma grande cole√ß√£o de textos de uma maneira espec√≠fica. Uma forma de treinar embeddings sem√¢nticos √© chamada de [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). √â baseada em duas arquiteturas principais que s√£o usadas para produzir uma representa√ß√£o distribu√≠da de palavras:

 - **Continuous bag-of-words** (CBoW) ‚Äî nesta arquitetura, treinamos o modelo para prever uma palavra a partir do contexto ao seu redor. Dado o ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, o objetivo do modelo √© prever $W_0$ a partir de $(W_{-2},W_{-1},W_1,W_2)$.
 - **Continuous skip-gram** √© o oposto do CBoW. O modelo usa uma janela de palavras de contexto ao redor para prever a palavra atual.

CBoW √© mais r√°pido, enquanto skip-gram √© mais lento, mas faz um trabalho melhor ao representar palavras infrequentes.

![Imagem mostrando os algoritmos CBoW e Skip-Gram para converter palavras em vetores.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.pt.png)

> Imagem deste [artigo](https://arxiv.org/pdf/1301.3781.pdf)

Os embeddings pr√©-treinados do Word2Vec (assim como outros modelos similares, como o GloVe) tamb√©m podem ser usados no lugar da camada de embedding em redes neurais. No entanto, precisamos lidar com vocabul√°rios, pois o vocabul√°rio usado para pr√©-treinar o Word2Vec/GloVe provavelmente difere do vocabul√°rio em nosso corpus de texto. D√™ uma olhada nos Notebooks acima para ver como esse problema pode ser resolvido.

## Embeddings Contextuais

Uma limita√ß√£o chave das representa√ß√µes de embedding pr√©-treinadas tradicionais, como o Word2Vec, √© o problema da desambigua√ß√£o de sentido das palavras. Embora os embeddings pr√©-treinados possam capturar parte do significado das palavras em contexto, cada poss√≠vel significado de uma palavra √© codificado no mesmo embedding. Isso pode causar problemas em modelos posteriores, uma vez que muitas palavras, como a palavra 'play', t√™m significados diferentes dependendo do contexto em que s√£o usadas.

Por exemplo, a palavra 'play' nas duas frases diferentes tem significados bastante distintos:

- Eu fui a uma **pe√ßa** no teatro.
- John quer **brincar** com seus amigos.

Os embeddings pr√©-treinados acima representam ambos os significados da palavra 'play' no mesmo embedding. Para superar essa limita√ß√£o, precisamos construir embeddings baseados no **modelo de linguagem**, que √© treinado em um grande corpus de texto e *sabe* como as palavras podem ser combinadas em diferentes contextos. Discutir embeddings contextuais est√° fora do escopo deste tutorial, mas voltaremos a eles ao falar sobre modelos de linguagem mais adiante no curso.

## Conclus√£o

Nesta li√ß√£o, voc√™ descobriu como construir e usar camadas de embedding no TensorFlow e Pytorch para refletir melhor os significados sem√¢nticos das palavras.

## üöÄ Desafio

O Word2Vec tem sido usado para algumas aplica√ß√µes interessantes, incluindo a gera√ß√£o de letras de m√∫sicas e poesias. D√™ uma olhada neste [artigo](https://www.politetype.com/blog/word2vec-color-poems) que explica como o autor usou o Word2Vec para gerar poesia. Assista tamb√©m a [este v√≠deo de Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) para descobrir uma explica√ß√£o diferente dessa t√©cnica. Em seguida, tente aplicar essas t√©cnicas ao seu pr√≥prio corpus de texto, talvez proveniente do Kaggle.

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Revis√£o & Autoestudo

Leia este artigo sobre Word2Vec: [Estimativa Eficiente de Representa√ß√µes de Palavras em Espa√ßo Vetorial](https://arxiv.org/pdf/1301.3781.pdf)

## [Tarefa: Notebooks](assignment.md)

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido usando servi√ßos de tradu√ß√£o baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional feita por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes err√¥neas decorrentes do uso desta tradu√ß√£o.