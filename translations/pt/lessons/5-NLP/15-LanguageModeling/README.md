# Modelagem de Linguagem

Embutimentos sem√¢nticos, como Word2Vec e GloVe, s√£o na verdade um primeiro passo em dire√ß√£o √† **modelagem de linguagem** - criar modelos que de alguma forma *entendem* (ou *representam*) a natureza da linguagem.

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

A ideia principal por tr√°s da modelagem de linguagem √© trein√°-los em conjuntos de dados n√£o rotulados de maneira n√£o supervisionada. Isso √© importante porque temos grandes quantidades de texto n√£o rotulado dispon√≠veis, enquanto a quantidade de texto rotulado sempre seria limitada pela quantidade de esfor√ßo que podemos gastar na rotulagem. Na maioria das vezes, podemos construir modelos de linguagem que podem **prever palavras ausentes** no texto, porque √© f√°cil mascarar uma palavra aleat√≥ria no texto e us√°-la como uma amostra de treinamento.

## Treinamento de Embutimentos

Em nossos exemplos anteriores, usamos embutimentos sem√¢nticos pr√©-treinados, mas √© interessante ver como esses embutimentos podem ser treinados. Existem v√°rias ideias poss√≠veis que podem ser usadas:

* Modelagem de linguagem **N-Gram**, quando prevemos um token olhando para N tokens anteriores (N-gram)
* **Continuous Bag-of-Words** (CBoW), quando prevemos o token do meio $W_0$ em uma sequ√™ncia de tokens $W_{-N}$, ..., $W_N$.
* **Skip-gram**, onde prevemos um conjunto de tokens vizinhos {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} a partir do token do meio $W_0$.

![imagem do artigo sobre convers√£o de palavras em vetores](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.pt.png)

> Imagem do [este artigo](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Notebooks de Exemplo: Treinando o modelo CBoW

Continue seu aprendizado nos seguintes notebooks:

* [Treinando CBoW Word2Vec com TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Treinando CBoW Word2Vec com PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Conclus√£o

Na li√ß√£o anterior, vimos que os embutimentos de palavras funcionam como m√°gica! Agora sabemos que treinar embutimentos de palavras n√£o √© uma tarefa muito complexa, e devemos ser capazes de treinar nossos pr√≥prios embutimentos de palavras para texto espec√≠fico de dom√≠nio, se necess√°rio.

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Revis√£o & Estudo Aut√¥nomo

* [Tutorial oficial do PyTorch sobre Modelagem de Linguagem](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Tutorial oficial do TensorFlow sobre treinamento de modelo Word2Vec](https://www.TensorFlow.org/tutorials/text/word2vec).
* Usar o framework **gensim** para treinar os embutimentos mais comumente usados em algumas linhas de c√≥digo √© descrito [nesta documenta√ß√£o](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## üöÄ [Tarefa: Treinar Modelo Skip-Gram](lab/README.md)

No laborat√≥rio, desafiamos voc√™ a modificar o c√≥digo desta li√ß√£o para treinar o modelo skip-gram em vez do CBoW. [Leia os detalhes](lab/README.md)

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em sua l√≠ngua nativa deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes err√¥neas decorrentes do uso desta tradu√ß√£o.