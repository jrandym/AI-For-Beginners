# Aprendizado por Refor√ßo Profundo

O aprendizado por refor√ßo (RL) √© visto como um dos paradigmas b√°sicos de aprendizado de m√°quina, ao lado do aprendizado supervisionado e do aprendizado n√£o supervisionado. Enquanto no aprendizado supervisionado confiamos em um conjunto de dados com resultados conhecidos, o RL √© baseado em **aprender fazendo**. Por exemplo, quando vemos um jogo de computador pela primeira vez, come√ßamos a jogar, mesmo sem conhecer as regras, e logo conseguimos melhorar nossas habilidades apenas pelo processo de jogar e ajustar nosso comportamento.

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Para realizar RL, precisamos de:

* Um **ambiente** ou **simulador** que estabelece as regras do jogo. Devemos ser capazes de executar os experimentos no simulador e observar os resultados.
* Alguma **fun√ß√£o de recompensa**, que indica qu√£o bem-sucedido foi nosso experimento. No caso de aprender a jogar um jogo de computador, a recompensa seria nossa pontua√ß√£o final.

Com base na fun√ß√£o de recompensa, devemos ser capazes de ajustar nosso comportamento e melhorar nossas habilidades, para que na pr√≥xima vez joguemos melhor. A principal diferen√ßa entre outros tipos de aprendizado de m√°quina e RL √© que, no RL, normalmente n√£o sabemos se ganhamos ou perdemos at√© terminarmos o jogo. Assim, n√£o podemos dizer se um determinado movimento √© bom ou n√£o - recebemos uma recompensa apenas ao final do jogo.

Durante o RL, normalmente realizamos muitos experimentos. Durante cada experimento, precisamos equilibrar entre seguir a estrat√©gia √≥tima que aprendemos at√© agora (**explora√ß√£o**) e explorar novos estados poss√≠veis (**explora√ß√£o**).

## OpenAI Gym

Uma √≥tima ferramenta para RL √© o [OpenAI Gym](https://gym.openai.com/) - um **ambiente de simula√ß√£o**, que pode simular muitos ambientes diferentes, desde jogos Atari at√© a f√≠sica por tr√°s do equil√≠brio de um poste. √â um dos ambientes de simula√ß√£o mais populares para treinar algoritmos de aprendizado por refor√ßo, e √© mantido pela [OpenAI](https://openai.com/).

> **Nota**: Voc√™ pode ver todos os ambientes dispon√≠veis no OpenAI Gym [aqui](https://gym.openai.com/envs/#classic_control).

## Equil√≠brio do CartPole

Voc√™ provavelmente j√° viu dispositivos modernos de equil√≠brio, como o *Segway* ou *Gyroscooters*. Eles conseguem se equilibrar automaticamente ajustando suas rodas em resposta a um sinal de um aceler√¥metro ou girosc√≥pio. Nesta se√ß√£o, aprenderemos como resolver um problema semelhante - equilibrar um poste. √â semelhante a uma situa√ß√£o em que um artista de circo precisa equilibrar um poste em sua m√£o - mas esse equil√≠brio de poste ocorre apenas em 1D.

Uma vers√£o simplificada do equil√≠brio √© conhecida como o problema **CartPole**. No mundo do cartpole, temos um deslizante horizontal que pode se mover para a esquerda ou para a direita, e o objetivo √© equilibrar um poste vertical em cima do deslizante enquanto ele se move.

<img alt="um cartpole" src="images/cartpole.png" width="200"/>

Para criar e usar esse ambiente, precisamos de algumas linhas de c√≥digo Python:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Cada ambiente pode ser acessado exatamente da mesma forma:
* `env.reset` starts a new experiment
* `env.step` realiza um passo de simula√ß√£o. Ele recebe uma **a√ß√£o** do **espa√ßo de a√ß√µes** e retorna uma **observa√ß√£o** (do espa√ßo de observa√ß√£o), bem como uma recompensa e um sinal de termina√ß√£o.

No exemplo acima, realizamos uma a√ß√£o aleat√≥ria a cada passo, raz√£o pela qual a vida do experimento √© muito curta:

![cartpole sem equil√≠brio](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

O objetivo de um algoritmo de RL √© treinar um modelo - a chamada **pol√≠tica** œÄ - que retornar√° a a√ß√£o em resposta a um determinado estado. Tamb√©m podemos considerar a pol√≠tica como probabil√≠stica, ou seja, para qualquer estado *s* e a√ß√£o *a*, ela retornar√° a probabilidade œÄ(*a*|*s*) de que devemos tomar *a* no estado *s*.

## Algoritmo de Gradientes de Pol√≠tica

A maneira mais √≥bvia de modelar uma pol√≠tica √© criando uma rede neural que receber√° estados como entrada e retornar√° as a√ß√µes correspondentes (ou melhor, as probabilidades de todas as a√ß√µes). De certa forma, seria semelhante a uma tarefa de classifica√ß√£o normal, com uma grande diferen√ßa - n√£o sabemos de antem√£o quais a√ß√µes devemos tomar em cada um dos passos.

A ideia aqui √© estimar essas probabilidades. Constru√≠mos um vetor de **recompensas cumulativas** que mostra nossa recompensa total em cada passo do experimento. Tamb√©m aplicamos **desconto de recompensa** multiplicando recompensas anteriores por algum coeficiente Œ≥=0.99, a fim de diminuir o papel das recompensas anteriores. Em seguida, refor√ßamos aqueles passos ao longo do caminho do experimento que geram recompensas maiores.

> Aprenda mais sobre o algoritmo de Gradiente de Pol√≠tica e veja-o em a√ß√£o no [notebook de exemplo](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algoritmo Ator-Cr√≠tico

Uma vers√£o aprimorada da abordagem de Gradientes de Pol√≠tica √© chamada de **Ator-Cr√≠tico**. A ideia principal por tr√°s disso √© que a rede neural seria treinada para retornar duas coisas:

* A pol√≠tica, que determina qual a√ß√£o tomar. Esta parte √© chamada de **ator**.
* A estimativa da recompensa total que podemos esperar obter neste estado - esta parte √© chamada de **cr√≠tico**.

De certa forma, essa arquitetura se assemelha a um [GAN](../../4-ComputerVision/10-GANs/README.md), onde temos duas redes que s√£o treinadas uma contra a outra. No modelo ator-cr√≠tico, o ator prop√µe a a√ß√£o que precisamos tomar, e o cr√≠tico tenta ser cr√≠tico e estimar o resultado. No entanto, nosso objetivo √© treinar essas redes em un√≠ssono.

Como sabemos tanto as recompensas cumulativas reais quanto os resultados retornados pelo cr√≠tico durante o experimento, √© relativamente f√°cil construir uma fun√ß√£o de perda que minimize a diferen√ßa entre elas. Isso nos dar√° a **perda do cr√≠tico**. Podemos calcular a **perda do ator** usando a mesma abordagem que no algoritmo de gradiente de pol√≠tica.

Depois de executar um desses algoritmos, podemos esperar que nosso CartPole se comporte assim:

![um cartpole equilibrado](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Exerc√≠cios: Gradientes de Pol√≠tica e RL Ator-Cr√≠tico

Continue seu aprendizado nos seguintes notebooks:

* [RL em TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL em PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Outras Tarefas de RL

O Aprendizado por Refor√ßo atualmente √© um campo de pesquisa em r√°pido crescimento. Alguns exemplos interessantes de aprendizado por refor√ßo s√£o:

* Ensinar um computador a jogar **Jogos Atari**. A parte desafiadora desse problema √© que n√£o temos um estado simples representado como um vetor, mas sim uma captura de tela - e precisamos usar a CNN para converter essa imagem de tela em um vetor de caracter√≠sticas, ou para extrair informa√ß√µes de recompensa. Jogos Atari est√£o dispon√≠veis no Gym.
* Ensinar um computador a jogar jogos de tabuleiro, como Xadrez e Go. Recentemente, programas de ponta como **Alpha Zero** foram treinados do zero por dois agentes jogando um contra o outro e melhorando a cada passo.
* Na ind√∫stria, o RL √© usado para criar sistemas de controle a partir de simula√ß√£o. Um servi√ßo chamado [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) √© especificamente projetado para isso.

## Conclus√£o

Agora aprendemos como treinar agentes para alcan√ßar bons resultados apenas fornecendo uma fun√ß√£o de recompensa que define o estado desejado do jogo e dando a eles a oportunidade de explorar inteligentemente o espa√ßo de busca. Tentamos com sucesso dois algoritmos e alcan√ßamos um bom resultado em um per√≠odo relativamente curto de tempo. No entanto, isso √© apenas o come√ßo de sua jornada no RL, e voc√™ definitivamente deve considerar fazer um curso separado se quiser se aprofundar mais.

## üöÄ Desafio

Explore as aplica√ß√µes listadas na se√ß√£o 'Outras Tarefas de RL' e tente implementar uma!

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Revis√£o e Estudo Aut√¥nomo

Aprenda mais sobre aprendizado por refor√ßo cl√°ssico em nosso [Curr√≠culo de Aprendizado de M√°quina para Iniciantes](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Assista a [este √≥timo v√≠deo](https://www.youtube.com/watch?v=qv6UVOQ0F44) que fala sobre como um computador pode aprender a jogar Super Mario.

## Tarefa: [Treine um Carro de Montanha](lab/README.md)

Seu objetivo durante esta tarefa ser√° treinar um ambiente diferente do Gym - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional por um humano. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes erradas decorrentes do uso desta tradu√ß√£o.