# 딥러닝 훈련 팁

신경망이 깊어짐에 따라 훈련 과정은 점점 더 어려워집니다. 주요 문제 중 하나는 이른바 [소실 기울기](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) 또는 [폭발 기울기](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.)입니다. [이 글](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11)은 이러한 문제에 대한 좋은 소개를 제공합니다.

딥 네트워크의 훈련을 더 효율적으로 만들기 위해 사용할 수 있는 몇 가지 기술이 있습니다.

## 값 유지하기

수치 계산을 더 안정적으로 만들기 위해, 신경망 내의 모든 값이 일반적으로 [-1..1] 또는 [0..1] 범위 내에 있도록 하는 것이 중요합니다. 이는 엄격한 요구 사항은 아니지만, 부동 소수점 계산의 특성상 서로 다른 크기의 값들은 함께 정확하게 조작될 수 없습니다. 예를 들어, 10<sup>-10</sup>과 10<sup>10</sup>을 더하면 10<sup>10</sup>이 나올 가능성이 높습니다. 작은 값이 큰 값과 같은 차원으로 "변환"되기 때문에, 가수 부분이 손실됩니다.

대부분의 활성화 함수는 [-1..1] 주변에서 비선형성을 가지므로, 모든 입력 데이터를 [-1..1] 또는 [0..1] 범위로 스케일링하는 것이 합리적입니다.

## 초기 가중치 초기화

이상적으로, 네트워크 레이어를 통과한 후 값들이 동일한 범위에 있도록 하고자 합니다. 따라서 가중치를 초기화할 때 값의 분포를 유지하는 것이 중요합니다.

정규 분포 **N(0,1)**는 좋은 아이디어가 아닙니다. 왜냐하면 *n* 개의 입력이 있을 경우, 출력의 표준 편차는 *n*이 되고, 값들이 [0..1] 범위를 벗어날 가능성이 높기 때문입니다.

다음과 같은 초기화 방법이 자주 사용됩니다:

 * 균등 분포 -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/√n_in)**는 평균이 0이고 표준 편차가 1인 입력에 대해 같은 평균/표준 편차가 유지되도록 보장합니다.
 * **N(0,√2/(n_in+n_out))** -- 이른바 **자비에 초기화** (`glorot`)로, 전방 및 후방 전파 동안 신호를 범위 내에 유지하는 데 도움이 됩니다.

## 배치 정규화

적절한 가중치 초기화가 이루어지더라도, 훈련 중에 가중치가 임의로 커지거나 작아질 수 있으며, 이로 인해 신호가 적절한 범위를 벗어날 수 있습니다. 우리는 **정규화** 기술 중 하나를 사용하여 신호를 다시 가져올 수 있습니다. 여러 가지 방법이 있지만 (가중치 정규화, 레이어 정규화 등), 가장 많이 사용되는 것은 배치 정규화입니다.

**배치 정규화**의 아이디어는 미니배치 전반의 모든 값을 고려하고, 이러한 값에 기반하여 정규화를 수행하는 것입니다 (즉, 평균을 빼고 표준 편차로 나누기). 이는 가중치를 적용한 후 활성화 함수 이전에 이 정규화를 수행하는 네트워크 레이어로 구현됩니다. 결과적으로 우리는 더 높은 최종 정확도와 더 빠른 훈련을 경험할 가능성이 높습니다.

여기 배치 정규화에 대한 [원본 논문](https://arxiv.org/pdf/1502.03167.pdf), [위키피디아의 설명](https://en.wikipedia.org/wiki/Batch_normalization), 그리고 [좋은 소개 블로그 포스트](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (그리고 [러시아어로 된 글](https://habrahabr.ru/post/309302/))이 있습니다.

## 드롭아웃

**드롭아웃**은 훈련 중에 일정 비율의 임의의 뉴런을 제거하는 흥미로운 기술입니다. 이는 제거할 뉴런의 비율(일반적으로 10%-50%)이라는 하나의 매개변수를 가진 레이어로 구현되며, 훈련 중에는 입력 벡터의 임의의 요소를 0으로 만들고 다음 레이어로 전달합니다.

이것이 이상한 아이디어처럼 들릴 수 있지만, [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb) 노트북에서 MNIST 숫자 분류기 훈련에 대한 드롭아웃의 효과를 확인할 수 있습니다. 이는 훈련을 가속화하고 더 적은 훈련 에포크로 더 높은 정확도를 달성할 수 있게 해줍니다.

이 효과는 여러 가지 방법으로 설명될 수 있습니다:

 * 모델에 대한 임의의 충격 요인으로 간주될 수 있으며, 이는 최적화를 지역 최소값에서 벗어나게 합니다.
 * *암묵적인 모델 평균화*로 간주될 수 있습니다. 드롭아웃 동안 약간 다른 모델을 훈련하고 있다고 볼 수 있습니다.

> *어떤 사람들은 술에 취한 사람이 무언가를 배우려고 할 때, 다음 날 아침에 더 잘 기억한다고 말합니다. 이는 일부 기능이 제대로 작동하지 않는 뉴런이 의미를 이해하기 위해 더 잘 적응하려고 하기 때문이라고 합니다. 우리는 이것이 사실인지 실험해본 적이 없습니다.*

## 과적합 방지

딥러닝에서 매우 중요한 측면 중 하나는 [과적합](../../3-NeuralNetworks/05-Frameworks/Overfitting.md)을 방지할 수 있는 능력입니다. 매우 강력한 신경망 모델을 사용하는 것이 매력적일 수 있지만, 항상 모델 매개변수의 수와 훈련 샘플의 수를 균형 있게 맞춰야 합니다.

> 우리가 이전에 소개한 [과적합](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) 개념을 이해하고 있는지 확인하세요!

과적합을 방지하는 방법에는 여러 가지가 있습니다:

 * 조기 중단 -- 검증 세트에서 오류를 지속적으로 모니터링하고, 검증 오류가 증가하기 시작하면 훈련을 중단합니다.
 * 명시적 가중치 감소 / 정규화 -- 높은 절대값의 가중치에 대해 손실 함수에 추가적인 패널티를 추가하여 모델이 매우 불안정한 결과를 얻지 못하도록 합니다.
 * 모델 평균화 -- 여러 모델을 훈련하고 결과를 평균화합니다. 이는 분산을 최소화하는 데 도움이 됩니다.
 * 드롭아웃 (암묵적 모델 평균화)

## 옵티마이저 / 훈련 알고리즘

훈련의 또 다른 중요한 측면은 좋은 훈련 알고리즘을 선택하는 것입니다. 고전적인 **경량 경사 하강법**은 합리적인 선택이지만, 때때로 너무 느리거나 다른 문제를 일으킬 수 있습니다.

딥러닝에서는 **확률적 경량 경사 하강법**(SGD)을 사용합니다. 이는 훈련 세트에서 임의로 선택된 미니배치에 적용된 경량 경사 하강법입니다. 가중치는 다음 공식을 사용하여 조정됩니다:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### 모멘텀

**모멘텀 SGD**에서는 이전 단계의 기울기 일부를 유지합니다. 이는 우리가 관성으로 어딘가 이동할 때, 다른 방향으로 주먹을 맞으면 우리의 궤적이 즉시 바뀌지 않고 원래 움직임의 일부를 유지하는 것과 유사합니다. 여기서 우리는 *속도*를 나타내기 위해 또 다른 벡터 v를 도입합니다:

* v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
* w<sup>t+1</sup> = w<sup>t</sup> + v<sup>t+1</sup>

여기서 매개변수 γ는 우리가 관성을 얼마나 고려하는지를 나타냅니다: γ=0은 고전적인 SGD에 해당하고, γ=1은 순수한 운동 방정식입니다.

### Adam, Adagrad 등

각 레이어에서 신호를 어떤 행렬 W<sub>i</sub>로 곱하기 때문에 ||W<sub>i</sub>||에 따라 기울기가 감소하거나 0에 가까워지거나 무한히 증가할 수 있습니다. 이것이 바로 폭발/소실 기울기 문제의 본질입니다.

이 문제에 대한 해결책 중 하나는 방정식에서 기울기의 방향만 사용하고 절대값은 무시하는 것입니다. 즉,

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), 여기서 ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

이 알고리즘은 **Adagrad**라고 불립니다. 같은 아이디어를 사용하는 다른 알고리즘으로는 **RMSProp**, **Adam**이 있습니다.

> **Adam**은 많은 응용 프로그램에 대해 매우 효율적인 알고리즘으로 간주되므로, 어떤 것을 사용할지 확신이 없다면 Adam을 사용하세요.

### 기울기 클리핑

기울기 클리핑은 위의 아이디어를 확장한 것입니다. ||∇ℒ|| ≤ θ인 경우, 우리는 가중치 최적화에서 원래 기울기를 고려하고, ||∇ℒ|| > θ인 경우에는 기울기를 그 노름으로 나눕니다. 여기서 θ는 매개변수이며, 대부분의 경우 θ=1 또는 θ=10을 사용할 수 있습니다.

### 학습률 감소

훈련의 성공은 종종 학습률 매개변수 η에 달려 있습니다. η의 큰 값이 더 빠른 훈련을 초래한다고 가정하는 것이 합리적입니다. 이는 일반적으로 훈련 초기에 원하는 것이며, 이후에는 더 작은 η 값이 네트워크를 미세 조정하는 데 도움이 됩니다. 따라서 대부분의 경우 훈련 과정에서 η를 감소시키고자 합니다.

이는 각 훈련 에포크 후 η에 어떤 수(예: 0.98)를 곱하거나 더 복잡한 **학습률 스케줄**을 사용하여 수행할 수 있습니다.

## 다양한 네트워크 아키텍처

문제에 적합한 네트워크 아키텍처를 선택하는 것은 까다로울 수 있습니다. 일반적으로 우리는 특정 작업(또는 유사한 작업)에 대해 효과가 입증된 아키텍처를 선택합니다. 여기 [컴퓨터 비전을 위한 신경망 아키텍처에 대한 좋은 개요](https://www.topbots.com/a-brief-history-of-neural-network-architectures/)가 있습니다.

> 우리가 가진 훈련 샘플의 수에 대해 충분히 강력한 아키텍처를 선택하는 것이 중요합니다. 너무 강력한 모델을 선택하면 [과적합](../../3-NeuralNetworks/05-Frameworks/Overfitting.md)으로 이어질 수 있습니다.

또한, 필요한 복잡성에 자동으로 조정되는 아키텍처를 사용하는 것도 좋은 방법입니다. 어느 정도까지는 **ResNet** 아키텍처와 **Inception**이 자가 조정됩니다. [컴퓨터 비전 아키텍처에 대한 더 많은 정보](../07-ConvNets/CNN_Architectures.md)도 참고하세요.

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있음을 유의하시기 바랍니다. 원본 문서는 해당 언어로 작성된 권위 있는 자료로 간주되어야 합니다. 중요한 정보에 대해서는 전문적인 인간 번역을 권장합니다. 이 번역의 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 우리는 책임을 지지 않습니다.