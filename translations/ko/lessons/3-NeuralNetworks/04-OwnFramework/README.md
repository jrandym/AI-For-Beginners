# 신경망 소개: 다층 퍼셉트론

이전 섹션에서는 가장 간단한 신경망 모델인 단층 퍼셉트론, 즉 선형 이진 분류 모델에 대해 배웠습니다.

이번 섹션에서는 이 모델을 보다 유연한 프레임워크로 확장하여 다음과 같은 작업을 수행할 수 있도록 합니다:

* 이진 분류 외에 **다중 클래스 분류** 수행
* 분류 외에 **회귀 문제** 해결
* 선형적으로 분리할 수 없는 클래스 분리

또한, 다양한 신경망 아키텍처를 구성할 수 있는 자체 모듈형 프레임워크를 Python으로 개발할 것입니다.

## [강의 전 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## 머신러닝의 정형화

머신러닝 문제를 정형화하는 것부터 시작해 보겠습니다. 훈련 데이터셋 **X**와 레이블 **Y**가 있다고 가정하고, 가장 정확한 예측을 할 수 있는 모델 *f*를 구축해야 합니다. 예측의 품질은 **손실 함수** ℒ로 측정됩니다. 다음의 손실 함수들이 자주 사용됩니다:

* 회귀 문제의 경우, 숫자를 예측해야 할 때는 **절대 오차** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| 또는 **제곱 오차** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>를 사용할 수 있습니다.
* 분류의 경우, **0-1 손실** (모델의 **정확도**와 본질적으로 동일) 또는 **로지스틱 손실**을 사용합니다.

단층 퍼셉트론의 경우, 함수 *f*는 선형 함수 *f(x)=wx+b*로 정의되었습니다 (여기서 *w*는 가중치 행렬, *x*는 입력 특성의 벡터, *b*는 바이어스 벡터입니다). 다양한 신경망 아키텍처의 경우, 이 함수는 더 복잡한 형태를 취할 수 있습니다.

> 분류의 경우, 네트워크 출력으로 해당 클래스의 확률을 얻는 것이 종종 바람직합니다. 임의의 숫자를 확률로 변환하기 위해 (예: 출력을 정규화하기 위해) 우리는 종종 **소프트맥스** 함수 σ를 사용하며, 함수 *f*는 *f(x)=σ(wx+b)*로 변환됩니다.

위의 *f* 정의에서 *w*와 *b*는 **매개변수** θ=⟨*w,b*⟩라고 불립니다. 데이터셋 ⟨**X**,**Y**⟩가 주어지면, 우리는 매개변수 θ의 함수로 전체 데이터셋에 대한 전체 오류를 계산할 수 있습니다.

> ✅ **신경망 훈련의 목표는 매개변수 θ를 변화시켜 오류를 최소화하는 것입니다.**

## 경량 경량화 최적화

함수 최적화에 대한 잘 알려진 방법이 **경량 경량화**입니다. 아이디어는 손실 함수에 대한 매개변수의 도함수(다차원 경우 **기울기**)를 계산하고, 오류가 감소하도록 매개변수를 변화시키는 것입니다. 이는 다음과 같이 정형화할 수 있습니다:

* 매개변수를 무작위 값 w<sup>(0)</sup>, b<sup>(0)</sup>로 초기화합니다.
* 다음 단계를 여러 번 반복합니다:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

훈련 중에 최적화 단계는 전체 데이터셋을 고려하여 계산되어야 합니다 (손실은 모든 훈련 샘플을 통해 합산하여 계산된다는 점을 기억하세요). 그러나 실제로 우리는 **미니배치**라고 불리는 데이터셋의 작은 부분을 가져와 데이터의 하위 집합을 기반으로 기울기를 계산합니다. 매번 하위 집합이 무작위로 선택되기 때문에, 이러한 방법을 **확률적 경량 경량화** (SGD)라고 부릅니다.

## 다층 퍼셉트론과 역전파

위에서 본 단층 네트워크는 선형적으로 분리 가능한 클래스를 분류할 수 있습니다. 보다 풍부한 모델을 구축하기 위해 네트워크의 여러 층을 결합할 수 있습니다. 수학적으로 이는 함수 *f*가 더 복잡한 형태를 가지며 여러 단계로 계산된다는 것을 의미합니다:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

여기서, α는 **비선형 활성화 함수**, σ는 소프트맥스 함수이며, 매개변수 θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>입니다.

경량 경량화 알고리즘은 동일하게 유지되지만, 기울기를 계산하는 것이 더 어려워질 것입니다. 연쇄 미분 법칙을 고려하면 다음과 같이 도함수를 계산할 수 있습니다:

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 연쇄 미분 법칙은 손실 함수의 도함수를 매개변수에 대해 계산하는 데 사용됩니다.

모든 표현의 가장 왼쪽 부분이 동일하므로, 우리는 손실 함수에서 시작하여 계산 그래프를 "역방향"으로 따라가며 도함수를 효과적으로 계산할 수 있습니다. 따라서 다층 퍼셉트론 훈련 방법을 **역전파** 또는 '백프롭'이라고 부릅니다.

<img alt="compute graph" src="images/ComputeGraphGrad.png"/>

> TODO: 이미지 출처

> ✅ 우리는 노트북 예제에서 역전파를 훨씬 더 자세히 다룰 것입니다.  

## 결론

이번 수업에서는 자체 신경망 라이브러리를 구축하고, 이를 사용하여 간단한 2차원 분류 작업을 수행했습니다.

## 🚀 도전 과제

동봉된 노트북에서 다층 퍼셉트론을 구축하고 훈련하기 위한 자체 프레임워크를 구현할 것입니다. 현대 신경망이 어떻게 작동하는지 자세히 살펴볼 수 있습니다.

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) 노트북으로 진행하여 작업을 수행하세요.

## [강의 후 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## 복습 및 자기 학습

역전파는 AI와 ML에서 사용되는 일반적인 알고리즘으로, [자세히 공부할 가치가 있습니다](https://wikipedia.org/wiki/Backpropagation).

## [과제](lab/README.md)

이번 실습에서는 이번 수업에서 구축한 프레임워크를 사용하여 MNIST 손글씨 숫자 분류 문제를 해결해야 합니다.

* [지침](lab/README.md)
* [노트북](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서는 해당 언어로 작성된 권위 있는 자료로 간주되어야 합니다. 중요한 정보에 대해서는 전문 인간 번역을 권장합니다. 이 번역의 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 우리는 책임을 지지 않습니다.