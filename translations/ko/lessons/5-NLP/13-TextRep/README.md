# 텍스트를 텐서로 표현하기

## [강의 전 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## 텍스트 분류

이 섹션의 첫 번째 부분에서는 **텍스트 분류** 작업에 집중할 것입니다. 우리는 다음과 같은 뉴스 기사를 포함하는 [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) 데이터셋을 사용할 것입니다:

* 카테고리: Sci/Tech
* 제목: Ky. Company Wins Grant to Study Peptides (AP)
* 본문: AP - 루이빌 대학교의 화학 연구원이 설립한 회사가 펩타이드 연구를 위한 보조금을 받았습니다...

우리의 목표는 텍스트를 기반으로 뉴스 항목을 카테고리 중 하나로 분류하는 것입니다.

## 텍스트 표현하기

신경망을 사용하여 자연어 처리(NLP) 작업을 해결하려면 텍스트를 텐서로 표현할 방법이 필요합니다. 컴퓨터는 이미 ASCII 또는 UTF-8과 같은 인코딩을 사용하여 텍스트 문자를 숫자로 변환하여 화면의 글꼴에 매핑합니다.

<img alt="문자를 ASCII 및 이진 표현으로 매핑하는 다이어그램을 보여주는 이미지" src="images/ascii-character-map.png" width="50%"/>

> [이미지 출처](https://www.seobility.net/en/wiki/ASCII)

인간으로서 우리는 각 글자가 **무엇을 나타내는지** 이해하고, 모든 문자가 어떻게 결합되어 문장의 단어를 형성하는지 이해합니다. 그러나 컴퓨터는 스스로 이러한 이해를 갖고 있지 않으며, 신경망은 훈련 중에 의미를 배워야 합니다.

따라서 텍스트를 표현할 때 다양한 접근 방식을 사용할 수 있습니다:

* **문자 수준 표현**: 각 문자를 숫자로 취급하여 텍스트를 표현합니다. 텍스트 말뭉치에 *C* 개의 서로 다른 문자가 있다고 가정할 때, 단어 *Hello*는 5x*C* 텐서로 표현됩니다. 각 글자는 원-핫 인코딩의 텐서 열에 해당합니다.
* **단어 수준 표현**: 텍스트의 모든 단어로 **어휘**를 만들고, 그런 다음 원-핫 인코딩을 사용하여 단어를 표현합니다. 이 접근 방식은 단어가 더 높은 수준의 의미 개념을 사용하므로 신경망의 작업을 단순화하기 때문에 더 나은 방법입니다. 그러나 큰 사전 크기로 인해 고차원 희소 텐서를 다루어야 합니다.

표현 방식에 관계없이 먼저 텍스트를 **토큰**의 시퀀스로 변환해야 합니다. 하나의 토큰은 문자, 단어 또는 때로는 단어의 일부일 수 있습니다. 그런 다음 토큰을 숫자로 변환하며, 일반적으로 **어휘**를 사용하여 이 숫자를 신경망에 원-핫 인코딩을 통해 입력할 수 있습니다.

## N-그램

자연어에서 단어의 정확한 의미는 문맥에 따라 결정됩니다. 예를 들어, *neural network*와 *fishing network*의 의미는 완전히 다릅니다. 이를 고려하는 한 가지 방법은 단어 쌍을 사용하여 모델을 구축하고, 단어 쌍을 별도의 어휘 토큰으로 간주하는 것입니다. 이렇게 하면 문장 *I like to go fishing*은 다음과 같은 토큰 시퀀스로 표현됩니다: *I like*, *like to*, *to go*, *go fishing*. 이 접근 방식의 문제는 사전 크기가 크게 증가하고, *go fishing*과 *go shopping*과 같은 조합이 서로 다른 토큰으로 표현되지만, 동일한 동사를 사용함에도 불구하고 의미적 유사성이 없다는 것입니다.

경우에 따라 세 단어의 조합인 삼그램(tri-grams)도 고려할 수 있습니다. 따라서 이 접근 방식은 종종 **n-그램**이라고 불립니다. 또한 문자 수준 표현에서 n-그램을 사용하는 것이 의미가 있으며, 이 경우 n-그램은 대략적으로 서로 다른 음절에 해당합니다.

## 단어 가방 및 TF/IDF

텍스트 분류와 같은 작업을 해결할 때, 우리는 텍스트를 고정 크기 벡터로 표현할 수 있어야 하며, 이를 최종 밀집 분류기의 입력으로 사용할 것입니다. 이를 수행하는 가장 간단한 방법 중 하나는 모든 개별 단어 표현을 결합하는 것입니다. 예를 들어, 각 단어의 원-핫 인코딩을 추가하면 각 단어가 텍스트 내에서 얼마나 자주 나타나는지를 보여주는 빈도 벡터를 얻을 수 있습니다. 이러한 텍스트 표현을 **단어 가방**(BoW)이라고 합니다.

<img src="images/bow.png" width="90%"/>

> 저자 제공 이미지

BoW는 본질적으로 텍스트에서 어떤 단어가 어떤 양으로 나타나는지를 나타내며, 이는 텍스트의 주제를 잘 나타낼 수 있습니다. 예를 들어, 정치에 관한 뉴스 기사는 *president*와 *country*와 같은 단어를 포함할 가능성이 높고, 과학 출판물은 *collider*, *discovered*와 같은 단어를 포함할 것입니다. 따라서 단어 빈도는 텍스트 내용의 좋은 지표가 될 수 있습니다.

BoW의 문제는 *and*, *is*와 같은 특정 일반 단어가 대부분의 텍스트에 나타나며, 이들이 가장 높은 빈도를 가지므로 실제로 중요한 단어를 가리는 것입니다. 이러한 단어의 중요성을 낮추기 위해 전체 문서 집합에서 단어가 나타나는 빈도를 고려할 수 있습니다. 이는 TF/IDF 접근 방식의 주요 아이디어이며, 이 수업에 첨부된 노트북에서 더 자세히 다룰 것입니다.

그러나 이러한 접근 방식은 텍스트의 **의미**를 완전히 고려할 수 없습니다. 이를 위해서는 더 강력한 신경망 모델이 필요하며, 이는 이 섹션 후반부에서 논의할 것입니다.

## ✍️ 연습: 텍스트 표현

다음 노트북에서 학습을 계속하세요:

* [PyTorch를 사용한 텍스트 표현](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [TensorFlow를 사용한 텍스트 표현](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## 결론

지금까지 우리는 서로 다른 단어에 빈도 가중치를 추가할 수 있는 기술을 연구했습니다. 그러나 이들은 의미나 순서를 표현할 수 없습니다. 유명한 언어학자 J. R. Firth가 1935년에 말했듯이, "단어의 완전한 의미는 항상 문맥에 따라 다르며, 문맥과 분리된 의미에 대한 연구는 진지하게 받아들여질 수 없다." 우리는 나중에 언어 모델링을 사용하여 텍스트에서 문맥 정보를 캡처하는 방법을 배울 것입니다.

## 🚀 도전

단어 가방 및 다양한 데이터 모델을 사용하여 다른 연습 문제를 시도해 보세요. 이 [Kaggle 대회](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)에서 영감을 받을 수 있습니다.

## [강의 후 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## 리뷰 및 자기 학습

[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)에서 텍스트 임베딩 및 단어 가방 기술로 기술을 연습하세요.

## [과제: 노트북](assignment.md)

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있음을 유의하시기 바랍니다. 원본 문서는 해당 언어에서 권위 있는 출처로 간주되어야 합니다. 중요한 정보에 대해서는 전문 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 우리는 책임을 지지 않습니다.