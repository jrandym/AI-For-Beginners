# 임베딩

## [강의 전 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

BoW 또는 TF/IDF 기반의 분류기를 훈련할 때, 우리는 길이가 `vocab_size`인 고차원 단어 가방 벡터에서 작업했으며, 저차원 위치 표현 벡터를 희소한 원-핫 표현으로 명시적으로 변환하고 있었습니다. 그러나 이 원-핫 표현은 메모리 효율적이지 않습니다. 게다가 각 단어는 서로 독립적으로 처리되므로, 즉 원-핫 인코딩된 벡터는 단어 간의 의미적 유사성을 표현하지 않습니다.

**임베딩**의 아이디어는 단어를 저차원 밀집 벡터로 표현하여 단어의 의미를 반영하는 것입니다. 의미 있는 단어 임베딩을 구축하는 방법에 대해서는 나중에 논의하겠지만, 지금은 임베딩을 단어 벡터의 차원을 낮추는 방법으로 생각해 봅시다.

따라서 임베딩 레이어는 단어를 입력으로 받아서 지정된 `embedding_size`의 출력 벡터를 생성합니다. 어떤 면에서는 `Linear` 레이어와 매우 유사하지만, 원-핫 인코딩된 벡터를 사용하는 대신 단어 번호를 입력으로 받아 대규모 원-핫 인코딩 벡터 생성을 피할 수 있습니다.

우리의 분류기 네트워크에서 임베딩 레이어를 첫 번째 레이어로 사용함으로써, 우리는 단어 가방 모델에서 **임베딩 가방** 모델로 전환할 수 있습니다. 여기서 우리는 텍스트의 각 단어를 해당 임베딩으로 변환한 다음, `sum`, `average` 또는 `max`와 같은 모든 임베딩에 대해 집계 함수를 계산합니다.

![다섯 개의 연속 단어에 대한 임베딩 분류기를 보여주는 이미지.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.ko.png)

> 저자 제공 이미지

## ✍️ 연습: 임베딩

다음 노트북에서 학습을 계속하세요:
* [PyTorch로 임베딩하기](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow로 임베딩하기](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## 의미 임베딩: Word2Vec

임베딩 레이어가 단어를 벡터 표현으로 매핑하는 방법을 학습하는 동안, 이 표현이 반드시 많은 의미적 의미를 가지고 있는 것은 아닙니다. 유사한 단어 또는 동의어가 어떤 벡터 거리(예: 유클리드 거리) 측면에서 서로 가까운 벡터에 대응하도록 벡터 표현을 학습하는 것이 좋습니다.

이를 위해, 우리는 특정 방식으로 대규모 텍스트 모음에서 임베딩 모델을 사전 훈련해야 합니다. 의미 임베딩을 훈련하는 한 가지 방법은 [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)이라고 불립니다. 이는 단어의 분산 표현을 생성하기 위해 사용되는 두 가지 주요 아키텍처를 기반으로 합니다:

 - **연속 단어 가방** (CBoW) — 이 아키텍처에서는 모델이 주변 맥락에서 단어를 예측하도록 훈련합니다. ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$가 주어지면, 모델의 목표는 $(W_{-2},W_{-1},W_1,W_2)$에서 $W_0$를 예측하는 것입니다.
 - **연속 스킵그램**은 CBoW와 반대입니다. 모델은 현재 단어를 예측하기 위해 주변 맥락 단어의 윈도우를 사용합니다.

CBoW는 더 빠르지만, 스킵그램은 더 느리지만 드문 단어를 더 잘 표현합니다.

![단어를 벡터로 변환하는 CBoW와 스킵그램 알고리즘을 보여주는 이미지.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.ko.png)

> [이 논문](https://arxiv.org/pdf/1301.3781.pdf)에서 가져온 이미지

Word2Vec로 사전 훈련된 임베딩(및 GloVe와 같은 다른 유사한 모델)은 신경망의 임베딩 레이어 대신 사용할 수도 있습니다. 그러나 우리는 어휘 문제를 다루어야 합니다. 왜냐하면 Word2Vec/GloVe를 사전 훈련하는 데 사용된 어휘가 우리의 텍스트 코퍼스의 어휘와 다를 가능성이 높기 때문입니다. 이 문제를 해결하는 방법을 알아보려면 위의 노트북을 참조하세요.

## 맥락 임베딩

Word2Vec와 같은 전통적인 사전 훈련된 임베딩 표현의 주요 한계 중 하나는 단어 의미의 모호성 문제입니다. 사전 훈련된 임베딩은 문맥에서 단어의 의미를 어느 정도 포착할 수 있지만, 단어의 모든 가능한 의미가 동일한 임베딩에 인코딩됩니다. 이는 'play'와 같은 많은 단어가 사용되는 맥락에 따라 다른 의미를 가지기 때문에 하위 모델에서 문제를 일으킬 수 있습니다.

예를 들어, 'play'라는 단어는 다음 두 문장에서 상당히 다른 의미를 가지고 있습니다:

- 나는 극장에서 **연극**을 보러 갔다.
- 존은 친구들과 **놀고** 싶어한다.

위의 사전 훈련된 임베딩은 'play'라는 단어의 두 가지 의미를 동일한 임베딩으로 표현합니다. 이러한 한계를 극복하기 위해, 우리는 대규모 텍스트 코퍼스를 기반으로 한 **언어 모델**에 따라 임베딩을 구축해야 하며, 이는 단어가 다양한 맥락에서 어떻게 결합될 수 있는지를 *알고* 있습니다. 맥락 임베딩에 대한 논의는 이 튜토리얼의 범위를 벗어나지만, 나중에 언어 모델에 대해 이야기할 때 다시 돌아올 것입니다.

## 결론

이번 수업에서는 TensorFlow와 PyTorch에서 임베딩 레이어를 구축하고 사용하는 방법을 배워 단어의 의미를 더 잘 반영할 수 있음을 발견했습니다.

## 🚀 도전

Word2Vec는 노래 가사와 시를 생성하는 등 흥미로운 응용 프로그램에 사용되었습니다. 저자가 Word2Vec를 사용하여 시를 생성하는 과정을 설명하는 [이 기사](https://www.politetype.com/blog/word2vec-color-poems)를 살펴보세요. 또한 [Dan Shiffmann의 이 동영상](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain)을 시청하여 이 기술에 대한 다른 설명을 알아보세요. 그런 다음 이러한 기술을 Kaggle에서 가져온 자신의 텍스트 코퍼스에 적용해 보세요.

## [강의 후 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## 복습 및 자기 학습

Word2Vec에 대한 이 논문을 읽어보세요: [벡터 공간에서 단어 표현의 효율적인 추정](https://arxiv.org/pdf/1301.3781.pdf)

## [과제: 노트북](assignment.md)

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 있을 수 있음을 유의해 주시기 바랍니다. 원본 문서는 해당 언어로 된 권위 있는 출처로 간주되어야 합니다. 중요한 정보에 대해서는 전문 인간 번역을 권장합니다. 이 번역을 사용하여 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.