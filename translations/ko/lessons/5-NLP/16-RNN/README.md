# 순환 신경망

## [강의 전 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

이전 섹션에서는 텍스트의 풍부한 의미 표현을 사용하고, 임베딩 위에 간단한 선형 분류기를 적용했습니다. 이 아키텍처의 목적은 문장 내 단어들의 집합된 의미를 포착하는 것이지만, 단어의 **순서**를 고려하지 않습니다. 왜냐하면 임베딩 위에서의 집계 작업이 원본 텍스트에서 이 정보를 제거했기 때문입니다. 이러한 모델은 단어의 순서를 모델링할 수 없기 때문에, 텍스트 생성이나 질문 응답과 같은 더 복잡하거나 모호한 작업을 해결할 수 없습니다.

텍스트 시퀀스의 의미를 포착하기 위해서는 **순환 신경망**(RNN)이라는 다른 신경망 아키텍처를 사용해야 합니다. RNN에서는 문장을 한 번에 하나의 기호로 네트워크를 통과시키고, 네트워크는 어떤 **상태**를 생성하며, 이를 다음 기호와 함께 다시 네트워크에 전달합니다.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.ko.png)

> 저자 제공 이미지

입력 토큰 시퀀스 X<sub>0</sub>,...,X<sub>n</sub>가 주어지면, RNN은 신경망 블록의 시퀀스를 생성하고, 이 시퀀스를 백프로파게이션을 사용하여 끝에서 끝까지 학습합니다. 각 네트워크 블록은 입력으로 (X<sub>i</sub>, S<sub>i</sub>) 쌍을 받고, 결과로 S<sub>i+1</sub>을 생성합니다. 최종 상태 S<sub>n</sub> 또는 (출력 Y<sub>n</sub>)는 결과를 생성하기 위해 선형 분류기로 전달됩니다. 모든 네트워크 블록은 동일한 가중치를 공유하며, 하나의 백프로파게이션 패스를 사용하여 끝에서 끝까지 학습됩니다.

상태 벡터 S<sub>0</sub>,...,S<sub>n</sub>가 네트워크를 통과하므로, 단어 간의 순차적 의존성을 학습할 수 있습니다. 예를 들어, 시퀀스의 어딘가에 단어 *not*이 나타나면, 특정 요소를 부정하도록 상태 벡터 내에서 학습할 수 있습니다.

> ✅ 위 그림에서 모든 RNN 블록의 가중치가 공유되므로, 동일한 그림은 출력 상태를 네트워크의 입력으로 다시 전달하는 순환 피드백 루프가 있는 하나의 블록(오른쪽)으로 표현될 수 있습니다.

## RNN 셀의 구조

간단한 RNN 셀의 구조를 살펴보겠습니다. 이 셀은 이전 상태 S<sub>i-1</sub>와 현재 기호 X<sub>i</sub>를 입력으로 받아들여, 출력 상태 S<sub>i</sub>를 생성해야 합니다(때때로 생성 네트워크의 경우처럼 다른 출력 Y<sub>i</sub>에도 관심이 있을 수 있습니다).

간단한 RNN 셀 내부에는 두 개의 가중치 행렬이 있습니다: 하나는 입력 기호를 변환하고(이를 W라고 부르겠습니다), 다른 하나는 입력 상태를 변환합니다(H). 이 경우 네트워크의 출력은 σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b)로 계산되며, 여기서 σ는 활성화 함수이고 b는 추가적인 편향입니다.

<img alt="RNN 셀 구조" src="images/rnn-anatomy.png" width="50%"/>

> 저자 제공 이미지

많은 경우, 입력 토큰은 RNN에 들어가기 전에 임베딩 레이어를 통과하여 차원을 줄입니다. 이 경우 입력 벡터의 차원이 *emb_size*이고 상태 벡터의 차원이 *hid_size*인 경우, W의 크기는 *emb_size*×*hid_size*이고 H의 크기는 *hid_size*×*hid_size*입니다.

## 장기 단기 기억(LSTM)

고전적인 RNN의 주요 문제 중 하나는 이른바 **소실 기울기** 문제입니다. RNN은 하나의 백프로파게이션 패스에서 끝에서 끝까지 학습되기 때문에, 네트워크의 첫 번째 레이어로 오류를 전파하는 데 어려움이 있으며, 따라서 네트워크는 먼 토큰 간의 관계를 학습할 수 없습니다. 이 문제를 피하는 한 가지 방법은 이른바 **게이트**를 사용하여 **명시적인 상태 관리**를 도입하는 것입니다. 이러한 종류의 두 가지 잘 알려진 아키텍처가 있습니다: **장기 단기 기억**(LSTM)과 **게이티드 릴레이 유닛**(GRU).

![장기 단기 기억 셀의 예시를 보여주는 이미지](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> 이미지 출처 TBD

LSTM 네트워크는 RNN과 유사한 방식으로 구성되지만, 레이어 간에 전달되는 두 가지 상태가 있습니다: 실제 상태 C와 숨겨진 벡터 H입니다. 각 유닛에서 숨겨진 벡터 H<sub>i</sub>는 입력 X<sub>i</sub>와 연결되어 C 상태에 어떤 일이 발생하는지를 제어합니다. 각 게이트는 시그모이드 활성화가 있는 신경망(출력이 [0,1] 범위)에 해당하며, 상태 벡터와 곱해질 때 비트 마스크로 생각할 수 있습니다. 다음은 위 그림에서 왼쪽에서 오른쪽으로의 게이트입니다:

* **망각 게이트**는 숨겨진 벡터를 받아 C 벡터의 어떤 구성 요소를 잊어야 할지, 어떤 구성 요소를 통과시켜야 할지를 결정합니다.
* **입력 게이트**는 입력과 숨겨진 벡터에서 일부 정보를 받아 상태에 삽입합니다.
* **출력 게이트**는 상태를 *tanh* 활성화가 있는 선형 레이어를 통해 변환한 후, 숨겨진 벡터 H<sub>i</sub>를 사용하여 일부 구성 요소를 선택하여 새로운 상태 C<sub>i+1</sub>를 생성합니다.

상태 C의 구성 요소는 켜고 끌 수 있는 플래그로 생각할 수 있습니다. 예를 들어, 시퀀스에서 이름 *Alice*를 만나면, 이는 여성 캐릭터를 가리킨다고 가정할 수 있으며, 문장 내에 여성 명사가 있다는 플래그를 세울 수 있습니다. 이후 *and Tom*이라는 구문을 만나면, 우리는 복수 명사가 있다는 플래그를 세울 것입니다. 따라서 상태를 조작함으로써 문장 부분의 문법적 특성을 추적할 수 있습니다.

> ✅ LSTM의 내부를 이해하는 데 훌륭한 자료는 Christopher Olah의 이 훌륭한 기사 [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)입니다.

## 양방향 및 다층 RNN

우리는 시퀀스의 시작에서 끝으로 한 방향으로 작동하는 순환 네트워크에 대해 논의했습니다. 이는 우리가 읽고 듣는 방식과 유사하므로 자연스럽게 보입니다. 그러나 많은 실제 경우에서 입력 시퀀스에 무작위 접근이 가능하므로, 양방향으로 순환 계산을 수행하는 것이 의미가 있을 수 있습니다. 이러한 네트워크를 **양방향** RNN이라고 합니다. 양방향 네트워크를 다룰 때는 각 방향에 대해 두 개의 숨겨진 상태 벡터가 필요합니다.

순환 네트워크는 단방향이든 양방향이든 시퀀스 내의 특정 패턴을 포착하고 이를 상태 벡터에 저장하거나 출력을 통해 전달할 수 있습니다. 합성곱 네트워크와 마찬가지로, 우리는 첫 번째 레이어에서 추출한 저수준 패턴을 기반으로 더 높은 수준의 패턴을 포착하기 위해 첫 번째 레이어 위에 또 다른 순환 레이어를 구축할 수 있습니다. 이는 두 개 이상의 순환 네트워크로 구성된 **다층 RNN**의 개념으로 이어지며, 이전 레이어의 출력이 다음 레이어의 입력으로 전달됩니다.

![다층 장기 단기 기억 RNN을 보여주는 이미지](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.ko.jpg)

*그림 출처: [이 훌륭한 게시물](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) by Fernando López*

## ✍️ 연습: 임베딩

다음 노트북에서 학습을 계속하세요:

* [PyTorch로 RNN](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow로 RNN](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## 결론

이번 단원에서는 RNN이 시퀀스 분류에 사용될 수 있지만, 사실 텍스트 생성, 기계 번역 등 더 많은 작업을 처리할 수 있음을 알게 되었습니다. 다음 단원에서는 이러한 작업을 다룰 것입니다.

## 🚀 도전 과제

LSTM에 대한 문헌을 읽고 그 응용 프로그램을 고려해 보세요:

- [그리드 장기 단기 기억](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [강의 후 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## 복습 및 자기 학습

- [LSTM 네트워크 이해하기](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.

## [과제: 노트북](assignment.md)

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있음을 유의하시기 바랍니다. 원본 문서는 원어로 된 권위 있는 자료로 간주되어야 합니다. 중요한 정보에 대해서는 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 저희는 책임을 지지 않습니다.