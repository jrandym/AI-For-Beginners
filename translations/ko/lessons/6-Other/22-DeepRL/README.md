# 심층 강화 학습

강화 학습(RL)은 감독 학습 및 비감독 학습과 함께 기본적인 기계 학습 패러다임 중 하나로 여겨집니다. 감독 학습에서는 결과가 알려진 데이터셋에 의존하지만, RL은 **행동을 통해 배우는** 데 기반을 두고 있습니다. 예를 들어, 우리가 컴퓨터 게임을 처음 접했을 때, 규칙을 알지 못한 채로 게임을 시작하고, 곧 게임을 하며 행동을 조정하는 과정만으로도 우리의 기술을 향상시킬 수 있습니다.

## [강의 전 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL을 수행하기 위해서는 다음이 필요합니다:

* 게임의 규칙을 설정하는 **환경** 또는 **시뮬레이터**. 우리는 시뮬레이터에서 실험을 실행하고 결과를 관찰할 수 있어야 합니다.
* 우리의 실험이 얼마나 성공적이었는지를 나타내는 **보상 함수**. 컴퓨터 게임을 배우는 경우, 보상은 우리의 최종 점수가 됩니다.

보상 함수에 따라 우리는 행동을 조정하고 기술을 향상시킬 수 있어야 하며, 다음 번에는 더 나은 플레이를 할 수 있습니다. 다른 유형의 기계 학습과 RL의 주요 차이점은 RL에서는 게임이 끝날 때까지 우리가 이겼는지 졌는지 알 수 없다는 것입니다. 따라서 특정 행동이 좋거나 나쁘다고 단독으로 판단할 수 없으며, 게임의 끝에서만 보상을 받습니다.

RL 동안 우리는 일반적으로 많은 실험을 수행합니다. 각 실험에서 우리는 지금까지 배운 최적의 전략을 따르는 것(**착취**)과 새로운 가능한 상태를 탐색하는 것(**탐색**) 사이에서 균형을 맞춰야 합니다.

## OpenAI Gym

RL을 위한 훌륭한 도구는 [OpenAI Gym](https://gym.openai.com/)입니다. 이는 아타리 게임부터 폴 밸런싱의 물리학에 이르기까지 다양한 환경을 시뮬레이션할 수 있는 **시뮬레이션 환경**입니다. 이는 강화 학습 알고리즘을 훈련시키기 위한 가장 인기 있는 시뮬레이션 환경 중 하나이며, [OpenAI](https://openai.com/)에서 유지 관리됩니다.

> **참고**: OpenAI Gym에서 사용 가능한 모든 환경은 [여기](https://gym.openai.com/envs/#classic_control)에서 확인할 수 있습니다.

## CartPole 균형 잡기

여러분은 아마 *세그웨이*나 *자이로스쿠터*와 같은 현대의 균형 장치를 모두 보았을 것입니다. 이들은 가속도계나 자이로스코프의 신호에 따라 바퀴를 조정하여 자동으로 균형을 잡을 수 있습니다. 이 섹션에서는 유사한 문제인 폴 균형 잡기를 배우게 됩니다. 이는 서커스 공연자가 손에 막대를 균형 잡아야 하는 상황과 비슷하지만, 이 폴 균형 잡기는 1D에서만 발생합니다.

균형 잡기의 단순화된 버전은 **CartPole** 문제로 알려져 있습니다. CartPole 세계에서는 왼쪽이나 오른쪽으로 이동할 수 있는 수평 슬라이더가 있으며, 목표는 슬라이더 위에 수직 막대를 균형 잡는 것입니다.

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

이 환경을 생성하고 사용하기 위해서는 몇 줄의 파이썬 코드가 필요합니다:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

각 환경은 정확히 동일한 방법으로 접근할 수 있습니다:
* `env.reset` starts a new experiment
* `env.step`는 시뮬레이션 단계를 수행합니다. 이는 **행동 공간**에서 **행동**을 받고, **관찰 공간**에서 **관찰**을 반환하며, 보상과 종료 플래그도 반환합니다.

위의 예에서는 각 단계에서 무작위 행동을 수행하므로 실험의 수명이 매우 짧습니다:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

RL 알고리즘의 목표는 주어진 상태에 대한 반응으로 행동을 반환하는 모델, 즉 **정책** π를 훈련하는 것입니다. 정책을 확률적으로 고려할 수도 있습니다. 예를 들어, 어떤 상태 *s*와 행동 *a*에 대해 *s* 상태에서 *a*를 취할 확률 π(*a*|*s*)를 반환합니다.

## 정책 경량화 알고리즘

정책을 모델링하는 가장 명백한 방법은 상태를 입력으로 받고 해당 행동(또는 모든 행동의 확률)을 반환하는 신경망을 만드는 것입니다. 어떤 면에서는 이것이 일반적인 분류 작업과 유사하지만, 주요 차이점은 각 단계에서 어떤 행동을 취해야 할지 미리 알 수 없다는 것입니다.

여기서의 아이디어는 이러한 확률을 추정하는 것입니다. 우리는 실험의 각 단계에서 우리의 총 보상을 보여주는 **누적 보상** 벡터를 구축합니다. 또한 이전 보상에 어떤 계수 γ=0.99를 곱하여 **보상 할인**을 적용하여 이전 보상의 역할을 줄입니다. 그런 다음, 더 큰 보상을 가져오는 실험 경로를 따라 이러한 단계를 강화합니다.

> 정책 경량화 알고리즘에 대해 더 배우고 [예제 노트북](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)에서 이를 확인하세요.

## 액터-크리틱 알고리즘

정책 경량화 접근 방식의 개선된 버전은 **액터-크리틱**이라고 합니다. 그 뒤에 있는 주요 아이디어는 신경망이 두 가지를 반환하도록 훈련된다는 것입니다:

* 어떤 행동을 취할지를 결정하는 정책. 이 부분을 **액터**라고 합니다.
* 이 상태에서 우리가 기대할 수 있는 총 보상의 추정치 - 이 부분을 **크리틱**이라고 합니다.

어떤 면에서는 이 아키텍처가 [GAN](../../4-ComputerVision/10-GANs/README.md)과 유사하며, 여기서는 서로 대립되는 두 개의 네트워크가 있습니다. 액터-크리틱 모델에서 액터는 우리가 취해야 할 행동을 제안하고, 크리틱은 비판적으로 결과를 추정하려고 합니다. 그러나 우리의 목표는 이러한 네트워크를 동시에 훈련하는 것입니다.

우리는 실험 동안 실제 누적 보상과 크리틱이 반환한 결과를 모두 알고 있기 때문에, 이 둘 간의 차이를 최소화하는 손실 함수를 만드는 것이 상대적으로 쉽습니다. 이것이 **크리틱 손실**입니다. **액터 손실**은 정책 경량화 알고리즘과 동일한 접근 방식을 사용하여 계산할 수 있습니다.

이 알고리즘 중 하나를 실행한 후, 우리의 CartPole이 이렇게 행동할 것으로 기대할 수 있습니다:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ 연습: 정책 경량화 및 액터-크리틱 RL

다음 노트북에서 학습을 계속하세요:

* [TensorFlow에서의 RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch에서의 RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## 기타 RL 작업

현재 강화 학습은 빠르게 성장하는 연구 분야입니다. 강화 학습의 흥미로운 몇 가지 예시는 다음과 같습니다:

* 컴퓨터가 **아타리 게임**을 플레이하도록 가르치는 것. 이 문제에서 도전적인 부분은 단순한 벡터로 표현된 상태가 아니라 스크린샷이라는 것입니다. 우리는 CNN을 사용하여 이 화면 이미지를 피처 벡터로 변환하거나 보상 정보를 추출해야 합니다. 아타리 게임은 Gym에서 사용할 수 있습니다.
* 컴퓨터가 체스와 바둑과 같은 보드 게임을 플레이하도록 가르치는 것. 최근 **Alpha Zero**와 같은 최첨단 프로그램은 서로 대결하는 두 에이전트에 의해 처음부터 훈련되었으며, 각 단계에서 개선되었습니다.
* 산업에서는 RL을 사용하여 시뮬레이션에서 제어 시스템을 생성합니다. [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste)라는 서비스는 이를 위해 특별히 설계되었습니다.

## 결론

이제 우리는 에이전트를 훈련시켜 게임의 원하는 상태를 정의하는 보상 함수를 제공하고, 탐색 공간을 지능적으로 탐색할 기회를 제공함으로써 좋은 결과를 얻는 방법을 배웠습니다. 우리는 두 가지 알고리즘을 성공적으로 시도했고, 비교적 짧은 시간 안에 좋은 결과를 얻었습니다. 그러나 이는 RL에 대한 여러분의 여정의 시작일 뿐이며, 더 깊이 파고들고 싶다면 별도의 과정을 수강하는 것을 고려해야 합니다.

## 🚀 도전

'기타 RL 작업' 섹션에 나열된 응용 프로그램을 탐색하고 하나를 구현해 보세요!

## [강의 후 퀴즈](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## 복습 및 자습

우리의 [초보자를 위한 기계 학습 커리큘럼](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)에서 고전적인 강화 학습에 대해 더 배우세요.

[이 훌륭한 비디오](https://www.youtube.com/watch?v=qv6UVOQ0F44)를 시청하여 컴퓨터가 슈퍼 마리오를 플레이하는 방법에 대해 알아보세요.

## 과제: [산악차 훈련](lab/README.md)

이번 과제의 목표는 다른 Gym 환경인 [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)를 훈련하는 것입니다.

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서는 해당 언어로 작성된 권위 있는 출처로 간주되어야 합니다. 중요한 정보에 대해서는 전문 인간 번역을 권장합니다. 이 번역의 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 우리는 책임을 지지 않습니다.