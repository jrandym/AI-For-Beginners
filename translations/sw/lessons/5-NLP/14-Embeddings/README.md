# Inb√§ddningar

## [F√∂r-lektion quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

N√§r vi tr√§nade klassificerare baserade p√• BoW eller TF/IDF, arbetade vi med h√∂gdimensionella bag-of-words-vektorer med l√§ngd `vocab_size`, och vi konverterade uttryckligen fr√•n l√•gdimensionella positionsrepresentationsvektorer till glesa en-hot-representationer. Denna en-hot-representation √§r dock inte minneseffektiv. Dessutom behandlas varje ord oberoende av varandra, dvs. en-hot-kodade vektorer uttrycker ingen semantisk likhet mellan orden.

Id√©n med **inb√§ddning** √§r att representera ord med l√§gre dimensionella t√§ta vektorer, som p√• n√•got s√§tt √•terspeglar det semantiska betydelsen av ett ord. Vi kommer senare att diskutera hur man bygger meningsfulla ordinb√§ddningar, men f√∂r nu kan vi t√§nka p√• inb√§ddningar som ett s√§tt att s√§nka dimensionaliteten av en ordvektor.

S√• inb√§ddningslagret skulle ta ett ord som indata och producera en utdata-vektor av specificerad `embedding_size`. P√• ett s√§tt √§r det mycket likt ett `Linear`-lager, men ist√§llet f√∂r att ta en en-hot-kodad vektor, kan det ta ett ordnummer som indata, vilket g√∂r att vi kan undvika att skapa stora en-hot-kodade vektorer.

Genom att anv√§nda ett inb√§ddningslager som det f√∂rsta lagret i v√•rt klassificeringsn√§tverk kan vi v√§xla fr√•n en bag-of-words till **inb√§ddningsbag**-modell, d√§r vi f√∂rst konverterar varje ord i v√•r text till motsvarande inb√§ddning och sedan ber√§knar en viss aggregatfunktion √∂ver alla dessa inb√§ddningar, s√•som `sum`, `average` eller `max`.

![Bild som visar en inb√§ddningsklassificerare f√∂r fem sekvensord.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.sw.png)

> Bild av f√∂rfattaren

## ‚úçÔ∏è √ñvningar: Inb√§ddningar

Forts√§tt ditt l√§rande i f√∂ljande anteckningsblock:
* [Inb√§ddningar med PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Inb√§ddningar TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Semantiska Inb√§ddningar: Word2Vec

Medan inb√§ddningslagret l√§rde sig att kartl√§gga ord till vektorrepresentation, hade denna representation dock inte n√∂dv√§ndigtvis mycket semantisk betydelse. Det skulle vara bra att l√§ra sig en vektorrepresentation s√• att liknande ord eller synonymer motsvarar vektorer som ligger n√§ra varandra i termer av viss vektordistans (t.ex. euklidisk distans).

F√∂r att g√∂ra detta beh√∂ver vi f√∂rtr√§nar v√•r inb√§ddningsmodell p√• en stor samling text p√• ett specifikt s√§tt. Ett s√§tt att tr√§na semantiska inb√§ddningar kallas [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Det baseras p√• tv√• huvudarkitekturer som anv√§nds f√∂r att producera en distribuerad representation av ord:

 - **Kontinuerlig bag-of-words** (CBoW) ‚Äî i denna arkitektur tr√§nar vi modellen f√∂r att f√∂ruts√§ga ett ord fr√•n omgivande kontext. Givet ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, √§r m√•let f√∂r modellen att f√∂ruts√§ga $W_0$ fr√•n $(W_{-2},W_{-1},W_1,W_2)$.
 - **Kontinuerlig skip-gram** √§r motsatsen till CBoW. Modellen anv√§nder omgivande f√∂nster av kontextord f√∂r att f√∂ruts√§ga det aktuella ordet.

CBoW √§r snabbare, medan skip-gram √§r l√•ngsammare, men g√∂r ett b√§ttre jobb med att representera s√§llsynta ord.

![Bild som visar b√•de CBoW och Skip-Gram-algoritmer f√∂r att konvertera ord till vektorer.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.sw.png)

> Bild fr√•n [denna artikel](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec f√∂rtr√§nade inb√§ddningar (s√•v√§l som andra liknande modeller, s√•som GloVe) kan ocks√• anv√§ndas ist√§llet f√∂r inb√§ddningslagret i neurala n√§tverk. Men vi beh√∂ver hantera vokabul√§r, eftersom vokabul√§ren som anv√§nds f√∂r att f√∂rtr√§na Word2Vec/GloVe sannolikt skiljer sig fr√•n vokabul√§ren i v√•r textkorpus. Titta p√• ovanst√•ende anteckningsblock f√∂r att se hur detta problem kan l√∂sas.

## Kontextuella Inb√§ddningar

En viktig begr√§nsning av traditionella f√∂rtr√§nade inb√§ddningsrepresentationer som Word2Vec √§r problemet med ordsensdisambiguering. Medan f√∂rtr√§nade inb√§ddningar kan f√•nga en del av betydelsen av ord i kontext, kodas varje m√∂jlig betydelse av ett ord in i samma inb√§ddning. Detta kan orsaka problem i nedstr√∂msmodeller, eftersom m√•nga ord, s√•som ordet 'play', har olika betydelser beroende p√• den kontext de anv√§nds i.

Till exempel har ordet 'play' i dessa tv√• olika meningar ganska olika betydelser:

- Jag gick p√• en **teater**.
- John vill **leka** med sina v√§nner.

De f√∂rtr√§nade inb√§ddningarna ovan representerar b√•da dessa betydelser av ordet 'play' i samma inb√§ddning. F√∂r att √∂vervinna denna begr√§nsning beh√∂ver vi bygga inb√§ddningar baserade p√• **spr√•kmodellen**, som tr√§nas p√• en stor textkorpus och *vet* hur ord kan s√§ttas ihop i olika kontexter. Att diskutera kontextuella inb√§ddningar ligger utanf√∂r ramen f√∂r denna handledning, men vi kommer att √•terkomma till dem n√§r vi pratar om spr√•kmodeller senare i kursen.

## Slutsats

I denna lektion uppt√§ckte du hur man bygger och anv√§nder inb√§ddningslager i TensorFlow och Pytorch f√∂r att b√§ttre √•terspegla de semantiska betydelserna av ord.

## üöÄ Utmaning

Word2Vec har anv√§nts f√∂r n√•gra intressanta till√§mpningar, inklusive att generera l√•ttexter och poesi. Ta en titt p√• [denna artikel](https://www.politetype.com/blog/word2vec-color-poems) som g√•r igenom hur f√∂rfattaren anv√§nde Word2Vec f√∂r att generera poesi. Titta ocks√• p√• [denna video av Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) f√∂r att uppt√§cka en annan f√∂rklaring av denna teknik. F√∂rs√∂k sedan att till√§mpa dessa tekniker p√• din egen textkorpus, kanske h√§mtad fr√•n Kaggle.

## [Efter-lektion quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## Granskning & Sj√§lvstudie

L√§s igenom denna artikel om Word2Vec: [Effektiv uppskattning av ordrepresentationer i vektorutrymme](https://arxiv.org/pdf/1301.3781.pdf)

## [Uppgift: Anteckningsblock](assignment.md)

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av maskinbaserade AI-√∂vers√§ttningstj√§nster. √Ñven om vi str√§var efter noggrannhet, v√§nligen var medveten om att automatiska √∂vers√§ttningar kan inneh√•lla fel eller oegentligheter. Det ursprungliga dokumentet p√• sitt modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller felaktiga tolkningar som uppst√•r till f√∂ljd av anv√§ndningen av denna √∂vers√§ttning.