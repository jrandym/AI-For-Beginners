# Generativa n√§tverk

## [F√∂rl√§sningsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

√Öterkommande neurala n√§tverk (RNNs) och deras varianter med gated celler, s√•som Long Short Term Memory Cells (LSTMs) och Gated Recurrent Units (GRUs), erbjuder en mekanism f√∂r spr√•kmodellering genom att de kan l√§ra sig ordningsf√∂ljd och ge f√∂ruts√§gelser f√∂r n√§sta ord i en sekvens. Detta g√∂r att vi kan anv√§nda RNNs f√∂r **generativa uppgifter**, som vanlig textgenerering, maskin√∂vers√§ttning och till och med bildbeskrivning.

> ‚úÖ T√§nk p√• alla g√•nger du har haft nytta av generativa uppgifter som textkomplettering medan du skriver. G√∂r lite forskning om dina favoritapplikationer f√∂r att se om de utnyttjar RNNs.

I RNN-arkitekturen vi diskuterade i den f√∂reg√•ende enheten, producerade varje RNN-enhet n√§sta dolda tillst√•nd som en utdata. Vi kan dock ocks√• l√§gga till en annan utdata till varje √•terkommande enhet, vilket skulle till√•ta oss att producera en **sekvens** (som √§r lika l√•ng som den ursprungliga sekvensen). Dessutom kan vi anv√§nda RNN-enheter som inte tar emot en ing√•ng vid varje steg, utan ist√§llet tar en initial tillst√•ndsvektor och sedan producerar en sekvens av utdata.

Detta m√∂jligg√∂r olika neurala arkitekturer som visas i bilden nedan:

![Bild som visar vanliga m√∂nster av √•terkommande neurala n√§tverk.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.sw.jpg)

> Bild fr√•n blogginl√§gget [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) av [Andrej Karpaty](http://karpathy.github.io/)

* **En-till-en** √§r ett traditionellt neuralt n√§tverk med en ing√•ng och en utg√•ng
* **En-till-m√•nga** √§r en generativ arkitektur som accepterar ett ing√•ngsv√§rde och genererar en sekvens av utg√•ngsv√§rden. Till exempel, om vi vill tr√§na ett **bildbeskrivnings** n√§tverk som skulle producera en textuell beskrivning av en bild, kan vi anv√§nda en bild som ing√•ng, skicka den genom en CNN f√∂r att f√• dess dolda tillst√•nd, och sedan l√•ta en √•terkommande kedja generera beskrivningen ord f√∂r ord
* **M√•nga-till-en** motsvarar de RNN-arkitekturer vi beskrev i den f√∂reg√•ende enheten, s√•som textklassificering
* **M√•nga-till-m√•nga**, eller **sekvens-till-sekvens** motsvarar uppgifter som **maskin√∂vers√§ttning**, d√§r vi f√∂rst har en RNN som samlar all information fr√•n ing√•ngssekvensen till det dolda tillst√•ndet, och en annan RNN-kedja utvecklar detta tillst√•nd till utg√•ngssekvensen.

I denna enhet kommer vi att fokusera p√• enkla generativa modeller som hj√§lper oss att generera text. F√∂r enkelhetens skull kommer vi att anv√§nda tecken-niv√• tokenisering.

Vi kommer att tr√§na denna RNN f√∂r att generera text steg f√∂r steg. Vid varje steg kommer vi att ta en sekvens av tecken av l√§ngd `nchars`, och be n√§tverket att generera n√§sta utdata-tecken f√∂r varje ing√•ngstecken:

![Bild som visar ett exempel p√• RNN-generering av ordet 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.sw.png)

N√§r vi genererar text (under inferens), b√∂rjar vi med en viss **prompt**, som skickas genom RNN-celler f√∂r att generera sitt mellanliggande tillst√•nd, och sedan b√∂rjar generationen fr√•n detta tillst√•nd. Vi genererar ett tecken i taget och skickar tillst√•ndet och det genererade tecknet till en annan RNN-cell f√∂r att generera n√§sta, tills vi har genererat tillr√§ckligt med tecken.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Bild av f√∂rfattaren

## ‚úçÔ∏è √ñvningar: Generativa N√§tverk

Forts√§tt din inl√§rning i f√∂ljande anteckningsblock:

* [Generativa N√§tverk med PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Generativa N√§tverk med TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Mjuk textgenerering och temperatur

Utdata fr√•n varje RNN-cell √§r en sannolikhetsf√∂rdelning av tecken. Om vi alltid tar tecknet med den h√∂gsta sannolikheten som n√§sta tecken i den genererade texten, kan texten ofta bli "cyklad" mellan samma teckensekvenser om och om igen, som i detta exempel:

```
today of the second the company and a second the company ...
```

Men om vi tittar p√• sannolikhetsf√∂rdelningen f√∂r n√§sta tecken, kan det h√§nda att skillnaden mellan n√•gra av de h√∂gsta sannolikheterna inte √§r stor, t.ex. ett tecken kan ha sannolikheten 0.2, ett annat - 0.19, osv. Till exempel, n√§r vi letar efter n√§sta tecken i sekvensen '*play*', kan n√§sta tecken lika g√§rna vara antingen ett mellanslag eller **e** (som i ordet *player*).

Detta leder oss till slutsatsen att det inte alltid √§r "r√§ttvist" att v√§lja tecknet med h√∂gre sannolikhet, eftersom valet av det n√§st h√∂gsta fortfarande kan leda oss till meningsfull text. Det √§r klokare att **prova** tecken fr√•n sannolikhetsf√∂rdelningen som ges av n√§tverksutdata. Vi kan ocks√• anv√§nda en parameter, **temperatur**, som kommer att platta ut sannolikhetsf√∂rdelningen, om vi vill l√§gga till mer slumpm√§ssighet, eller g√∂ra den brantare, om vi vill h√•lla oss mer till tecknen med h√∂gsta sannolikhet.

Utforska hur denna mjuka textgenerering implementeras i anteckningsblocken l√§nkade ovan.

## Slutsats

√Ñven om textgenerering kan vara anv√§ndbar i sig, kommer de st√∂rsta f√∂rdelarna fr√•n f√∂rm√•gan att generera text med hj√§lp av RNNs fr√•n en viss initial funktionsvektor. Till exempel anv√§nds textgenerering som en del av maskin√∂vers√§ttning (sekvens-till-sekvens, i detta fall anv√§nds tillst√•ndsvektorn fr√•n *encoder* f√∂r att generera eller *avkoda* det √∂versatta meddelandet), eller f√∂r att generera en textuell beskrivning av en bild (i vilket fall funktionsvektorn skulle komma fr√•n CNN-extraktorn).

## üöÄ Utmaning

Ta n√•gra lektioner p√• Microsoft Learn om detta √§mne

* Textgenerering med [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Efterl√§sningsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Granskning & Sj√§lvstudie

H√§r √§r n√•gra artiklar f√∂r att ut√∂ka din kunskap

* Olika tillv√§gag√•ngss√§tt f√∂r textgenerering med Markov Chain, LSTM och GPT-2: [blogginl√§gg](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Exempel p√• textgenerering i [Keras-dokumentationen](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Uppgift](lab/README.md)

Vi har sett hur man genererar text tecken f√∂r tecken. I labbet kommer du att utforska textgenerering p√• ordniv√•.

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av maskinbaserade AI-√∂vers√§ttningstj√§nster. √Ñven om vi str√§var efter noggrannhet, v√§nligen var medveten om att automatiska √∂vers√§ttningar kan inneh√•lla fel eller oegentligheter. Det ursprungliga dokumentet p√• sitt modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller felaktiga tolkningar som uppst√•r till f√∂ljd av anv√§ndningen av denna √∂vers√§ttning.