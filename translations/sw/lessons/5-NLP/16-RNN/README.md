# √Öterkommande Neurala N√§tverk

## [F√∂r-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

I tidigare avsnitt har vi anv√§nt rika semantiska representationer av text och en enkel linj√§r klassificerare ovanp√• inb√§ddningarna. Vad denna arkitektur g√∂r √§r att f√•nga den aggregerade betydelsen av ord i en mening, men den tar inte h√§nsyn till **ordningen** av orden, eftersom aggregationsoperationen ovanp√• inb√§ddningarna tog bort denna information fr√•n den ursprungliga texten. Eftersom dessa modeller inte kan modellera ordning av ord, kan de inte l√∂sa mer komplexa eller tvetydiga uppgifter som textgenerering eller fr√•gesvar.

F√∂r att f√•nga betydelsen av textsekvenser beh√∂ver vi anv√§nda en annan neurala n√§tverksarkitektur, som kallas **√•terkommande neurala n√§tverk**, eller RNN. I RNN passerar vi v√•r mening genom n√§tverket ett symbol i taget, och n√§tverket producerar ett **tillst√•nd**, som vi sedan skickar tillbaka till n√§tverket med n√§sta symbol.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.sw.png)

> Bild av f√∂rfattaren

Givet inmatningssekvensen av token X<sub>0</sub>,...,X<sub>n</sub>, skapar RNN en sekvens av neurala n√§tverksblock och tr√§nar denna sekvens end-to-end med hj√§lp av backpropagation. Varje n√§tverksblock tar ett par (X<sub>i</sub>,S<sub>i</sub>) som inmatning och producerar S<sub>i+1</sub> som resultat. Det slutliga tillst√•ndet S<sub>n</sub> eller (utg√•ng Y<sub>n</sub>) g√•r in i en linj√§r klassificerare f√∂r att producera resultatet. Alla n√§tverksblock delar samma vikter och tr√§nas end-to-end med en backpropagation-pass.

Eftersom tillst√•ndsvektorer S<sub>0</sub>,...,S<sub>n</sub> passerar genom n√§tverket, kan det l√§ra sig de sekventiella beroendena mellan orden. Till exempel, n√§r ordet *not* dyker upp n√•gonstans i sekvensen, kan det l√§ra sig att neka vissa element inom tillst√•ndsvektorn, vilket resulterar i negation.

> ‚úÖ Eftersom vikterna f√∂r alla RNN-block p√• bilden ovan √§r delade, kan samma bild representeras som ett block (till h√∂ger) med en √•terkommande feedback-loop, som skickar utg√•ngstillst√•ndet fr√•n n√§tverket tillbaka till inmatningen.

## Anatomiska av en RNN Cell

L√•t oss se hur en enkel RNN-cell √§r organiserad. Den tar emot det f√∂reg√•ende tillst√•ndet S<sub>i-1</sub> och den aktuella symbolen X<sub>i</sub> som inmatningar, och m√•ste producera utg√•ngstillst√•ndet S<sub>i</sub> (och ibland √§r vi ocks√• intresserade av n√•gon annan utg√•ng Y<sub>i</sub>, som i fallet med generativa n√§tverk).

En enkel RNN-cell har tv√• viktmatriser inuti: en som transformerar en inmatningssymbol (l√•t oss kalla den W), och en annan som transformerar ett inmatningstillst√•nd (H). I det h√§r fallet ber√§knas n√§tverkets utg√•ng som œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b), d√§r œÉ √§r aktiveringsfunktionen och b √§r ytterligare bias.

<img alt="RNN Cell Anatom" src="images/rnn-anatomy.png" width="50%"/>

> Bild av f√∂rfattaren

I m√•nga fall passerar inmatningstokens genom inb√§ddningslagret innan de g√•r in i RNN f√∂r att s√§nka dimensionaliteten. I det h√§r fallet, om dimensionen av inmatningsvektorerna √§r *emb_size*, och tillst√•ndsvektorn √§r *hid_size* - storleken p√• W √§r *emb_size*√ó*hid_size*, och storleken p√• H √§r *hid_size*√ó*hid_size*.

## L√•ng Korttidsminne (LSTM)

Ett av de st√∂rsta problemen med klassiska RNN:er √§r det s√• kallade **f√∂rsvinnande gradienter** problemet. Eftersom RNN:er tr√§nas end-to-end i en backpropagation-pass har det sv√•rt att sprida fel till de f√∂rsta lagren av n√§tverket, och d√§rmed kan n√§tverket inte l√§ra sig relationer mellan avl√§gsna token. Ett av s√§tten att undvika detta problem √§r att inf√∂ra **explisit tillst√•ndshantering** genom att anv√§nda s√• kallade **portar**. Det finns tv√• v√§lk√§nda arkitekturer av denna typ: **Long Short Term Memory** (LSTM) och **Gated Relay Unit** (GRU).

![Bild som visar ett exempel p√• en l√•ng korttidsminnescell](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Bildk√§lla TBD

LSTM-n√§tverket √§r organiserat p√• ett s√§tt som liknar RNN, men det finns tv√• tillst√•nd som passerar fr√•n lager till lager: det aktuella tillst√•ndet C och den dolda vektorn H. Vid varje enhet sammanfogas den dolda vektorn H<sub>i</sub> med inmatningen X<sub>i</sub>, och de kontrollerar vad som h√§nder med tillst√•ndet C via **portar**. Varje port √§r ett neuralt n√§tverk med sigmoidaktivering (utg√•ng i intervallet [0,1]), vilket kan ses som en bitmask n√§r den multipliceras med tillst√•ndsvektorn. Det finns f√∂ljande portar (fr√•n v√§nster till h√∂ger p√• bilden ovan):

* **Gl√∂mskeporten** tar en dold vektor och avg√∂r vilka komponenter av vektorn C vi beh√∂ver gl√∂mma, och vilka som ska passera.
* **Inmatningsporten** tar viss information fr√•n inmatnings- och dolda vektorer och s√§tter in den i tillst√•ndet.
* **Utg√•ngsporten** transformerar tillst√•ndet via ett linj√§rt lager med *tanh*-aktivering, och v√§ljer sedan n√•gra av sina komponenter med hj√§lp av en dold vektor H<sub>i</sub> f√∂r att producera ett nytt tillst√•nd C<sub>i+1</sub>.

Komponenter av tillst√•ndet C kan ses som vissa flaggor som kan sl√•s p√• och av. Till exempel, n√§r vi st√∂ter p√• ett namn *Alice* i sekvensen, kan vi vilja anta att det h√§nvisar till en kvinnlig karakt√§r, och h√∂ja flaggan i tillst√•ndet att vi har ett kvinnligt substantiv i meningen. N√§r vi vidare st√∂ter p√• fraserna *and Tom*, kommer vi att h√∂ja flaggan att vi har ett plural substantiv. Genom att manipulera tillst√•ndet kan vi p√• s√• s√§tt h√•lla reda p√• de grammatiska egenskaperna hos meningsdelar.

> ‚úÖ En utm√§rkt resurs f√∂r att f√∂rst√• internals av LSTM √§r denna fantastiska artikel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## Bidirektionella och Flerlager RNN:er

Vi har diskuterat √•terkommande n√§tverk som fungerar i en riktning, fr√•n b√∂rjan av en sekvens till slutet. Det verkar naturligt, eftersom det liknar hur vi l√§ser och lyssnar p√• tal. Men eftersom vi i m√•nga praktiska fall har slumpm√§ssig √•tkomst till inmatningssekvensen, kan det vara meningsfullt att k√∂ra √•terkommande ber√§kningar i b√•da riktningarna. S√•dana n√§tverk kallas **bidirektionella** RNN:er. N√§r vi hanterar ett bidirektionellt n√§tverk, beh√∂ver vi tv√• dolda tillst√•ndsvektorer, en f√∂r varje riktning.

Ett √•terkommande n√§tverk, antingen en-riktat eller bidirektionellt, f√•ngar vissa m√∂nster inom en sekvens och kan lagra dem i en tillst√•ndsvektor eller skicka dem till utg√•ngen. Precis som med konvolutionella n√§tverk kan vi bygga ett annat √•terkommande lager ovanp√• det f√∂rsta f√∂r att f√•nga h√∂gre niv√•m√∂nster och bygga fr√•n l√•gniv√•m√∂nster som extraherats av det f√∂rsta lagret. Detta leder oss till begreppet **flerlager RNN**, som best√•r av tv√• eller fler √•terkommande n√§tverk, d√§r utg√•ngen fr√•n det f√∂reg√•ende lagret skickas till n√§sta lager som inmatning.

![Bild som visar en flerlagers l√•ng-korttidsminnes-RNN](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.sw.jpg)

*Bild fr√•n [detta underbara inl√§gg](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) av Fernando L√≥pez*

## ‚úçÔ∏è √ñvningar: Inb√§ddningar

Forts√§tt din inl√§rning i f√∂ljande anteckningsb√∂cker:

* [RNNs med PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs med TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Slutsats

I denna enhet har vi sett att RNN:er kan anv√§ndas f√∂r sekvensklassificering, men i sj√§lva verket kan de hantera m√•nga fler uppgifter, s√•som textgenerering, maskin√∂vers√§ttning och mer. Vi kommer att √∂verv√§ga dessa uppgifter i n√§sta enhet.

## üöÄ Utmaning

L√§s igenom viss litteratur om LSTM och √∂verv√§g deras till√§mpningar:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Granskning & Sj√§lvstudie

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) av Christopher Olah.

## [Uppgift: Anteckningsb√∂cker](assignment.md)

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av maskinbaserade AI-√∂vers√§ttningstj√§nster. √Ñven om vi str√§var efter noggrannhet, var medveten om att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• sitt modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi tar inget ansvar f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r fr√•n anv√§ndningen av denna √∂vers√§ttning.