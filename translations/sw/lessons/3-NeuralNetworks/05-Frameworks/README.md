# Neural Network Frameworks

Som vi redan har l√§rt oss, f√∂r att effektivt kunna tr√§na neurala n√§tverk beh√∂ver vi g√∂ra tv√• saker:

* Att arbeta med tensorer, t.ex. att multiplicera, addera och ber√§kna vissa funktioner som sigmoid eller softmax
* Att ber√§kna gradienter f√∂r alla uttryck, f√∂r att kunna utf√∂ra gradientnedstigning

## [F√∂r-lektion quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Medan `numpy` biblioteket kan hantera den f√∂rsta delen, beh√∂ver vi n√•gon mekanism f√∂r att ber√§kna gradienter. I [v√•rt ramverk](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) som vi utvecklade i f√∂reg√•ende avsnitt var vi tvungna att manuellt programmera alla derivatfunktioner inuti `backward` metoden, som g√∂r backpropagation. Idealiskt sett b√∂r ett ramverk ge oss m√∂jlighet att ber√§kna gradienter av *vilket uttryck som helst* som vi kan definiera.

En annan viktig aspekt √§r att kunna utf√∂ra ber√§kningar p√• GPU, eller andra specialiserade ber√§kningsenheter, s√•som [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). Tr√§ning av djupa neurala n√§tverk kr√§ver *mycket* ber√§kningar, och att kunna parallellisera dessa ber√§kningar p√• GPU:er √§r mycket viktigt.

> ‚úÖ Termen 'parallellisera' betyder att f√∂rdela ber√§kningarna √∂ver flera enheter.

F√∂r n√§rvarande √§r de tv√• mest popul√§ra neurala ramverken: [TensorFlow](http://TensorFlow.org) och [PyTorch](https://pytorch.org/). B√•da erbjuder ett l√•gniv√•-API f√∂r att arbeta med tensorer b√•de p√• CPU och GPU. Ovanp√• l√•gniv√•-API:et finns ocks√• ett h√∂gre API, som kallas [Keras](https://keras.io/) och [PyTorch Lightning](https://pytorchlightning.ai/) respektive.

L√•gniv√•-API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
H√∂gniv√•-API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**L√•gniv√•-API:er** i b√•da ramverken l√•ter dig bygga s√• kallade **ber√§kningsgrafer**. Denna graf definierar hur man ber√§knar utdata (vanligtvis f√∂rlustfunktionen) med givna indata, och kan skickas f√∂r ber√§kning p√• GPU, om det √§r tillg√§ngligt. Det finns funktioner f√∂r att differentiera denna ber√§kningsgraf och ber√§kna gradienter, som sedan kan anv√§ndas f√∂r att optimera modellparametrar.

**H√∂gniv√•-API:er** betraktar i stort sett neurala n√§tverk som en **sekvens av lager**, och g√∂r konstruktionen av de flesta neurala n√§tverk mycket enklare. Att tr√§na modellen kr√§ver vanligtvis att man f√∂rbereder data och sedan anropar en `fit` funktion f√∂r att utf√∂ra jobbet.

H√∂gniv√•-API:et g√∂r att du snabbt kan konstruera typiska neurala n√§tverk utan att beh√∂va oroa dig f√∂r m√•nga detaljer. Samtidigt erbjuder l√•gniv√•-API:t mycket mer kontroll √∂ver tr√§ningsprocessen, och d√§rf√∂r anv√§nds de ofta inom forskningen, n√§r man hanterar nya arkitekturer f√∂r neurala n√§tverk.

Det √§r ocks√• viktigt att f√∂rst√• att du kan anv√§nda b√•da API:erna tillsammans, t.ex. kan du utveckla din egen n√§tverkslagerarkitektur med l√•gniv√•-API:t, och sedan anv√§nda den i det st√∂rre n√§tverk som konstruerats och tr√§nats med h√∂gniv√•-API:t. Eller s√• kan du definiera ett n√§tverk med h√∂gniv√•-API:t som en sekvens av lager, och sedan anv√§nda din egen l√•gniv√• tr√§ningsloop f√∂r att utf√∂ra optimering. B√•da API:erna anv√§nder samma grundl√§ggande koncept och √§r designade f√∂r att fungera bra tillsammans.

## L√§rande

I den h√§r kursen erbjuder vi det mesta av inneh√•llet b√•de f√∂r PyTorch och TensorFlow. Du kan v√§lja ditt f√∂redragna ramverk och bara g√• igenom motsvarande anteckningar. Om du √§r os√§ker p√• vilket ramverk du ska v√§lja, l√§s n√•gra diskussioner p√• internet ang√•ende **PyTorch vs. TensorFlow**. Du kan ocks√• titta p√• b√•da ramverken f√∂r att f√• en b√§ttre f√∂rst√•else.

D√§r det √§r m√∂jligt kommer vi att anv√§nda H√∂gniv√•-API:er f√∂r enkelhetens skull. Men vi anser att det √§r viktigt att f√∂rst√• hur neurala n√§tverk fungerar fr√•n grunden, s√• i b√∂rjan b√∂rjar vi med att arbeta med l√•gniv√•-API och tensorer. Men om du vill komma ig√•ng snabbt och inte vill spendera mycket tid p√• att l√§ra dig dessa detaljer, kan du hoppa √∂ver dem och g√• direkt till h√∂gniv√•-API anteckningarna.

## ‚úçÔ∏è √ñvningar: Ramverk

Forts√§tt ditt l√§rande i f√∂ljande anteckningar:

L√•gniv√•-API | [TensorFlow+Keras Anteckning](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
H√∂gniv√•-API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Efter att ha bem√§strat ramverken, l√•t oss sammanfatta begreppet √∂veranpassning.

# √ñveranpassning

√ñveranpassning √§r ett extremt viktigt begrepp inom maskininl√§rning, och det √§r mycket viktigt att f√• det r√§tt!

√ñverv√§g f√∂ljande problem med att approximera 5 punkter (representerade av `x` p√• graferna nedan):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.sw.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.sw.jpg)
-------------------------|--------------------------
**Linj√§r modell, 2 parametrar** | **Icke-linj√§r modell, 7 parametrar**
Tr√§ningsfel = 5.3 | Tr√§ningsfel = 0
Valideringsfel = 5.1 | Valideringsfel = 20

* Till v√§nster ser vi en bra rak linje approximation. Eftersom antalet parametrar √§r adekvat, f√∂rst√•r modellen id√©n bakom punktf√∂rdelningen korrekt.
* Till h√∂ger √§r modellen f√∂r kraftfull. Eftersom vi bara har 5 punkter och modellen har 7 parametrar, kan den justera sig p√• ett s√§tt som g√∂r att den g√•r genom alla punkter, vilket g√∂r tr√§ningsfelet till 0. Men detta hindrar modellen fr√•n att f√∂rst√• det korrekta m√∂nstret bakom datan, vilket g√∂r att valideringsfelet blir mycket h√∂gt.

Det √§r mycket viktigt att hitta en korrekt balans mellan modellens rikedom (antal parametrar) och antalet tr√§ningsprover.

## Varf√∂r √∂veranpassning intr√§ffar

  * Inte tillr√§ckligt med tr√§ningsdata
  * F√∂r kraftfull modell
  * F√∂r mycket brus i indata

## Hur man uppt√§cker √∂veranpassning

Som du kan se fr√•n grafen ovan kan √∂veranpassning uppt√§ckas genom ett mycket l√•gt tr√§ningsfel och ett h√∂gt valideringsfel. Normalt under tr√§ning kommer vi att se b√•de tr√§nings- och valideringsfel b√∂rja minska, och sedan vid en viss punkt kan valideringsfelet sluta minska och b√∂rja √∂ka. Detta kommer att vara ett tecken p√• √∂veranpassning, och en indikator p√• att vi troligen b√∂r sluta tr√§na vid denna punkt (eller √•tminstone g√∂ra en snapshot av modellen).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.sw.png)

## Hur man f√∂rhindrar √∂veranpassning

Om du ser att √∂veranpassning intr√§ffar kan du g√∂ra n√•got av f√∂ljande:

 * √ñka m√§ngden tr√§ningsdata
 * Minska modellens komplexitet
 * Anv√§nda n√•gon [regulariseringsteknik](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), s√•som [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), som vi kommer att √∂verv√§ga senare.

## √ñveranpassning och Bias-Varians Tradeoff

√ñveranpassning √§r faktiskt ett fall av ett mer generellt problem inom statistik som kallas [Bias-Varians Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Om vi √∂verv√§ger de m√∂jliga k√§llorna till fel i v√•r modell kan vi se tv√• typer av fel:

* **Biasfel** orsakas av att v√•r algoritm inte kan f√•nga relationen mellan tr√§ningsdata korrekt. Det kan bero p√• att v√•r modell inte √§r tillr√§ckligt kraftfull (**underanpassning**).
* **Variansfel**, som orsakas av att modellen approximera brus i indata ist√§llet f√∂r meningsfulla relationer (**√∂veranpassning**).

Under tr√§ning minskar biasfelet (n√§r v√•r modell l√§r sig att approximera datan), och variansfelet √∂kar. Det √§r viktigt att stoppa tr√§ningen - antingen manuellt (n√§r vi uppt√§cker √∂veranpassning) eller automatiskt (genom att inf√∂ra regularisering) - f√∂r att f√∂rhindra √∂veranpassning.

## Slutsats

I den h√§r lektionen l√§rde du dig om skillnaderna mellan de olika API:erna f√∂r de tv√• mest popul√§ra AI-ramverken, TensorFlow och PyTorch. Dessutom l√§rde du dig om ett mycket viktigt √§mne, √∂veranpassning.

## üöÄ Utmaning

I de medf√∂ljande anteckningarna hittar du 'uppgifter' l√§ngst ner; arbeta igenom anteckningarna och slutf√∂r uppgifterna.

## [Efter-lektion quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Granskning & Sj√§lvstudie

G√∂r lite forskning om f√∂ljande √§mnen:

- TensorFlow
- PyTorch
- √ñveranpassning

St√§ll dig sj√§lv f√∂ljande fr√•gor:

- Vad √§r skillnaden mellan TensorFlow och PyTorch?
- Vad √§r skillnaden mellan √∂veranpassning och underanpassning?

## [Uppgift](lab/README.md)

I detta laboratorium ombeds du att l√∂sa tv√• klassificeringsproblem med hj√§lp av enkel- och flerlagerade fullt anslutna n√§tverk med PyTorch eller TensorFlow.

* [Instruktioner](lab/README.md)
* [Anteckning](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av maskinbaserade AI-√∂vers√§ttningstj√§nster. √Ñven om vi str√§var efter noggrannhet, var medveten om att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• sitt modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r fr√•n anv√§ndningen av denna √∂vers√§ttning.