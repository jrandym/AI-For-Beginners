# Introduktion till Neurala N√§tverk. Flerlagers Perceptron

I den f√∂reg√•ende sektionen l√§rde du dig om den enklaste modellen f√∂r neurala n√§tverk - enlagers perceptron, en linj√§r klassificeringsmodell f√∂r tv√• klasser.

I denna sektion kommer vi att utvidga denna modell till ett mer flexibelt ramverk, vilket g√∂r att vi kan:

* utf√∂ra **flerklassklassificering** ut√∂ver tv√•klasstillst√•nd
* l√∂sa **regressionsproblem** ut√∂ver klassificering
* separera klasser som inte √§r linj√§rt separerbara

Vi kommer ocks√• att utveckla v√•rt eget modul√§ra ramverk i Python som g√∂r att vi kan konstruera olika arkitekturer f√∂r neurala n√§tverk.

## [F√∂r-l√§rare quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalisering av Maskininl√§rning

L√•t oss b√∂rja med att formalisera problemet inom Maskininl√§rning. Anta att vi har en tr√§ningsdataset **X** med etiketter **Y**, och vi beh√∂ver bygga en modell *f* som kommer att ge de mest exakta f√∂ruts√§gelserna. Kvaliteten p√• f√∂ruts√§gelserna m√§ts av **F√∂rlustfunktion** ‚Ñí. F√∂ljande f√∂rlustfunktioner anv√§nds ofta:

* F√∂r regressionsproblem, n√§r vi beh√∂ver f√∂ruts√§ga ett tal, kan vi anv√§nda **absolut fel** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadrerat fel** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* F√∂r klassificering anv√§nder vi **0-1 f√∂rlust** (som i huvudsak √§r detsamma som **noggrannheten** hos modellen), eller **logistisk f√∂rlust**.

F√∂r en enlags perceptron definierades funktionen *f* som en linj√§r funktion *f(x)=wx+b* (h√§r √§r *w* viktmatrisen, *x* √§r vektorn av ing√•ngsegenskaper, och *b* √§r biasvektorn). F√∂r olika arkitekturer av neurala n√§tverk kan denna funktion anta en mer komplex form.

> I fallet med klassificering √§r det ofta √∂nskv√§rt att f√• sannolikheter f√∂r motsvarande klasser som n√§tverksutg√•ng. F√∂r att konvertera godtyckliga tal till sannolikheter (t.ex. f√∂r att normalisera utg√•ngen) anv√§nder vi ofta **softmax** funktionen œÉ, och funktionen *f* blir *f(x)=œÉ(wx+b)*

I definitionen av *f* ovan kallas *w* och *b* f√∂r **parametrar** Œ∏=‚ü®*w,b*‚ü©. Givet datasetet ‚ü®**X**,**Y**‚ü© kan vi ber√§kna ett totalt fel p√• hela datasetet som en funktion av parametrarna Œ∏.

> ‚úÖ **M√•let med tr√§ningen av det neurala n√§tverket √§r att minimera felet genom att variera parametrarna Œ∏**

## Gradientnedstigning Optimering

Det finns en v√§lk√§nd metod f√∂r funktionsoptimering som kallas **gradientnedstigning**. Id√©n √§r att vi kan ber√§kna en derivata (i flerdimensionellt fall kallad **gradient**) av f√∂rlustfunktionen med avseende p√• parametrarna, och variera parametrarna p√• ett s√§tt s√• att felet minskar. Detta kan formaliseras som f√∂ljer:

* Initiera parametrar med n√•gra slumpm√§ssiga v√§rden w<sup>(0)</sup>, b<sup>(0)</sup>
* Upprepa f√∂ljande steg m√•nga g√•nger:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

Under tr√§ningen f√∂rv√§ntas optimeringsstegen ber√§knas med h√§nsyn till hela datasetet (kom ih√•g att f√∂rlusten ber√§knas som en summa √∂ver alla tr√§ningsprover). Men i verkligheten tar vi sm√• portioner av datasetet som kallas **minibatcher**, och ber√§knar gradienter baserat p√• en delm√§ngd av data. Eftersom delm√§ngden tas slumpm√§ssigt varje g√•ng, kallas denna metod **stokastisk gradientnedstigning** (SGD).

## Flerlagers Perceptron och Backpropagation

En enlags n√§tverk, som vi har sett ovan, kan klassificera linj√§rt separerbara klasser. F√∂r att bygga en rikare modell kan vi kombinera flera lager av n√§tverket. Matematiskt skulle det inneb√§ra att funktionen *f* skulle ha en mer komplex form och ber√§knas i flera steg:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

H√§r √§r Œ± en **icke-linj√§r aktiveringsfunktion**, œÉ √§r en softmax-funktion, och parametrarna Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>* >.

Gradientnedstigningsalgoritmen skulle f√∂rbli densamma, men det skulle bli sv√•rare att ber√§kna gradienter. Givet kedjederiveringsregeln kan vi ber√§kna derivator som:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ Kedjederiveringsregeln anv√§nds f√∂r att ber√§kna derivator av f√∂rlustfunktionen med avseende p√• parametrar.

Observera att den v√§nstra delen av alla dessa uttryck √§r densamma, och vi kan d√§rf√∂r effektivt ber√§kna derivator som b√∂rjar fr√•n f√∂rlustfunktionen och g√•r "bak√•t" genom den ber√§kningsm√§ssiga grafen. D√§rf√∂r kallas metoden f√∂r att tr√§na en flerlagers perceptron f√∂r **backpropagation**, eller 'backprop'.

<img alt="ber√§kningsgraf" src="images/ComputeGraphGrad.png"/>

> TODO: bildcitat

> ‚úÖ Vi kommer att t√§cka backprop i mycket mer detalj i v√•rt notebook-exempel.

## Slutsats

I denna lektion har vi byggt v√•rt eget bibliotek f√∂r neurala n√§tverk, och vi har anv√§nt det f√∂r en enkel tv√•dimensionell klassificeringsuppgift.

## üöÄ Utmaning

I den medf√∂ljande notebooken kommer du att implementera ditt eget ramverk f√∂r att bygga och tr√§na flerlagers perceptron. Du kommer att kunna se i detalj hur moderna neurala n√§tverk fungerar.

G√• till [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) notebooken och arbeta igenom den.

## [Efter-l√§rare quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Granskning & Sj√§lvstudie

Backpropagation √§r en vanlig algoritm som anv√§nds inom AI och ML, v√§rd att studera [i mer detalj](https://wikipedia.org/wiki/Backpropagation)

## [Uppgift](lab/README.md)

I detta labb ombeds du att anv√§nda det ramverk du konstruerade i denna lektion f√∂r att l√∂sa klassificeringen av handskrivna siffror fr√•n MNIST.

* [Instruktioner](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av maskinbaserade AI-√∂vers√§ttningstj√§nster. √Ñven om vi str√§var efter noggrannhet, v√§nligen var medveten om att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller inkonsekvenser. Det ursprungliga dokumentet p√• sitt modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r till f√∂ljd av anv√§ndningen av denna √∂vers√§ttning.