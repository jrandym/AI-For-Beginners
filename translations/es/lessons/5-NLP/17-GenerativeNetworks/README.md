# Redes generativas

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Las Redes Neuronales Recurrentes (RNNs) y sus variantes con puertas, como las Celdas de Memoria a Largo y Corto Plazo (LSTMs) y las Unidades Recurrentes con Puertas (GRUs), proporcionaron un mecanismo para el modelado del lenguaje en el que pueden aprender el orden de las palabras y ofrecer predicciones para la siguiente palabra en una secuencia. Esto nos permite usar RNNs para **tareas generativas**, como la generaci√≥n de texto ordinario, la traducci√≥n autom√°tica e incluso la creaci√≥n de descripciones para im√°genes.

> ‚úÖ Piensa en todas las ocasiones en que te has beneficiado de tareas generativas, como la finalizaci√≥n de texto mientras escribes. Investiga tus aplicaciones favoritas para ver si aprovecharon RNNs.

En la arquitectura de RNN que discutimos en la unidad anterior, cada unidad RNN produc√≠a el siguiente estado oculto como salida. Sin embargo, tambi√©n podemos a√±adir otra salida a cada unidad recurrente, lo que nos permitir√≠a generar una **secuencia** (que es igual en longitud a la secuencia original). Adem√°s, podemos utilizar unidades RNN que no aceptan una entrada en cada paso, y simplemente toman un vector de estado inicial y luego producen una secuencia de salidas.

Esto permite diferentes arquitecturas neuronales que se muestran en la imagen a continuaci√≥n:

![Imagen que muestra patrones comunes de redes neuronales recurrentes.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.es.jpg)

> Imagen del art√≠culo del blog [La Efectividad Irrazonable de las Redes Neuronales Recurrentes](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) de [Andrej Karpaty](http://karpathy.github.io/)

* **Uno a uno** es una red neuronal tradicional con una entrada y una salida.
* **Uno a muchos** es una arquitectura generativa que acepta un valor de entrada y genera una secuencia de valores de salida. Por ejemplo, si queremos entrenar una red de **creaci√≥n de descripciones de im√°genes** que produzca una descripci√≥n textual de una imagen, podemos tomar una imagen como entrada, pasarla a trav√©s de una CNN para obtener su estado oculto y luego tener una cadena recurrente que genere la descripci√≥n palabra por palabra.
* **Muchos a uno** corresponde a las arquitecturas RNN que describimos en la unidad anterior, como la clasificaci√≥n de texto.
* **Muchos a muchos**, o **secuencia a secuencia**, corresponde a tareas como **traducci√≥n autom√°tica**, donde primero una RNN recopila toda la informaci√≥n de la secuencia de entrada en el estado oculto, y otra cadena RNN despliega este estado en la secuencia de salida.

En esta unidad, nos enfocaremos en modelos generativos simples que nos ayuden a generar texto. Para simplificar, utilizaremos la tokenizaci√≥n a nivel de caracteres.

Entrenaremos esta RNN para generar texto paso a paso. En cada paso, tomaremos una secuencia de caracteres de longitud `nchars` y le pediremos a la red que genere el siguiente car√°cter de salida para cada car√°cter de entrada:

![Imagen que muestra un ejemplo de generaci√≥n RNN de la palabra 'HELLO'.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.es.png)

Al generar texto (durante la inferencia), comenzamos con un **mensaje inicial**, que se pasa a trav√©s de las celdas RNN para generar su estado intermedio, y luego desde este estado comienza la generaci√≥n. Generamos un car√°cter a la vez y pasamos el estado y el car√°cter generado a otra celda RNN para generar el siguiente, hasta que generamos suficientes caracteres.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Imagen del autor

## ‚úçÔ∏è Ejercicios: Redes Generativas

Contin√∫a tu aprendizaje en los siguientes cuadernos:

* [Redes Generativas con PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Redes Generativas con TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Generaci√≥n de texto suave y temperatura

La salida de cada celda RNN es una distribuci√≥n de probabilidad de caracteres. Si siempre tomamos el car√°cter con la mayor probabilidad como el siguiente car√°cter en el texto generado, el texto a menudo puede "ciclarse" entre las mismas secuencias de caracteres una y otra vez, como en este ejemplo:

```
today of the second the company and a second the company ...
```

Sin embargo, si observamos la distribuci√≥n de probabilidad para el siguiente car√°cter, podr√≠a ser que la diferencia entre algunas de las probabilidades m√°s altas no sea enorme; por ejemplo, un car√°cter puede tener una probabilidad de 0.2, otro - 0.19, etc. Por ejemplo, al buscar el siguiente car√°cter en la secuencia '*play*', el siguiente car√°cter puede ser igualmente un espacio o **e** (como en la palabra *player*).

Esto nos lleva a la conclusi√≥n de que no siempre es "justo" seleccionar el car√°cter con una mayor probabilidad, porque elegir el segundo m√°s alto a√∫n podr√≠a llevarnos a un texto significativo. Es m√°s sabio **muestrear** caracteres de la distribuci√≥n de probabilidad dada por la salida de la red. Tambi√©n podemos usar un par√°metro, **temperatura**, que aplanar√° la distribuci√≥n de probabilidad en caso de que queramos agregar m√°s aleatoriedad, o la har√° m√°s pronunciada si queremos ce√±irnos m√°s a los caracteres de mayor probabilidad.

Explora c√≥mo se implementa esta generaci√≥n de texto suave en los cuadernos vinculados arriba.

## Conclusi√≥n

Si bien la generaci√≥n de texto puede ser √∫til por derecho propio, los principales beneficios provienen de la capacidad de generar texto utilizando RNNs a partir de alg√∫n vector de caracter√≠sticas inicial. Por ejemplo, la generaci√≥n de texto se utiliza como parte de la traducci√≥n autom√°tica (secuencia a secuencia, en este caso el vector de estado del *codificador* se utiliza para generar o *decodificar* el mensaje traducido), o para generar descripciones textuales de una imagen (en cuyo caso el vector de caracter√≠sticas provendr√≠a de un extractor CNN).

## üöÄ Desaf√≠o

Toma algunas lecciones en Microsoft Learn sobre este tema

* Generaci√≥n de Texto con [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Revisi√≥n y Autoestudio

Aqu√≠ hay algunos art√≠culos para ampliar tu conocimiento

* Diferentes enfoques para la generaci√≥n de texto con Cadenas de Markov, LSTM y GPT-2: [art√≠culo del blog](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Ejemplo de generaci√≥n de texto en [documentaci√≥n de Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Asignaci√≥n](lab/README.md)

Hemos visto c√≥mo generar texto car√°cter por car√°cter. En el laboratorio, explorar√°s la generaci√≥n de texto a nivel de palabras.

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en IA. Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o malas interpretaciones que surjan del uso de esta traducci√≥n.