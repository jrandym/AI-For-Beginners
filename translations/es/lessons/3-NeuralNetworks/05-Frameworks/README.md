# Marcos de Redes Neuronales

Como ya hemos aprendido, para poder entrenar redes neuronales de manera eficiente necesitamos hacer dos cosas:

* Operar con tensores, es decir, multiplicar, sumar y calcular algunas funciones como sigmoid o softmax.
* Calcular gradientes de todas las expresiones, con el fin de realizar la optimizaci√≥n por descenso de gradiente.

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Mientras que la biblioteca `numpy` puede realizar la primera parte, necesitamos alg√∫n mecanismo para calcular gradientes. En [nuestro marco](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) que hemos desarrollado en la secci√≥n anterior, tuvimos que programar manualmente todas las funciones derivadas dentro del m√©todo `backward`, que realiza la retropropagaci√≥n. Idealmente, un marco deber√≠a brindarnos la oportunidad de calcular gradientes de *cualquier expresi√≥n* que podamos definir.

Otra cosa importante es poder realizar c√°lculos en GPU, o en cualquier otra unidad de computaci√≥n especializada, como [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). El entrenamiento de redes neuronales profundas requiere *muchos* c√°lculos, y poder paralelizar esos c√°lculos en GPUs es muy importante.

> ‚úÖ El t√©rmino 'paralelizar' significa distribuir los c√°lculos entre m√∫ltiples dispositivos.

Actualmente, los dos marcos neuronales m√°s populares son: [TensorFlow](http://TensorFlow.org) y [PyTorch](https://pytorch.org/). Ambos proporcionan una API de bajo nivel para operar con tensores tanto en CPU como en GPU. Encima de la API de bajo nivel, tambi√©n hay una API de alto nivel, llamada [Keras](https://keras.io/) y [PyTorch Lightning](https://pytorchlightning.ai/) respectivamente.

API de Bajo Nivel | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
------------------|-------------------------------------|--------------------------------
API de Alto Nivel  | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Las APIs de bajo nivel** en ambos marcos permiten construir lo que se llama **gr√°ficas computacionales**. Esta gr√°fica define c√≥mo calcular la salida (normalmente la funci√≥n de p√©rdida) con los par√°metros de entrada dados, y puede ser enviada para c√°lculo en GPU, si est√° disponible. Existen funciones para diferenciar esta gr√°fica computacional y calcular gradientes, que luego pueden ser utilizados para optimizar los par√°metros del modelo.

**Las APIs de alto nivel** consideran las redes neuronales como una **secuencia de capas**, y facilitan la construcci√≥n de la mayor√≠a de las redes neuronales. Entrenar el modelo generalmente requiere preparar los datos y luego llamar a una funci√≥n `fit` para realizar el trabajo.

La API de alto nivel permite construir redes neuronales t√≠picas muy r√°pidamente sin preocuparse por muchos detalles. Al mismo tiempo, la API de bajo nivel ofrece mucho m√°s control sobre el proceso de entrenamiento, y por lo tanto se utiliza mucho en investigaci√≥n, cuando se trabaja con nuevas arquitecturas de redes neuronales.

Tambi√©n es importante entender que puedes usar ambas APIs juntas, es decir, puedes desarrollar tu propia arquitectura de capa de red utilizando la API de bajo nivel, y luego usarla dentro de la red m√°s grande construida y entrenada con la API de alto nivel. O puedes definir una red usando la API de alto nivel como una secuencia de capas, y luego usar tu propio bucle de entrenamiento de bajo nivel para realizar la optimizaci√≥n. Ambas APIs utilizan los mismos conceptos b√°sicos subyacentes y est√°n dise√±adas para funcionar bien juntas.

## Aprendizaje

En este curso, ofrecemos la mayor parte del contenido tanto para PyTorch como para TensorFlow. Puedes elegir tu marco preferido y solo pasar por los cuadernos correspondientes. Si no est√°s seguro de qu√© marco elegir, lee algunas discusiones en internet sobre **PyTorch vs. TensorFlow**. Tambi√©n puedes echar un vistazo a ambos marcos para obtener una mejor comprensi√≥n.

Donde sea posible, utilizaremos APIs de alto nivel por simplicidad. Sin embargo, creemos que es importante entender c√≥mo funcionan las redes neuronales desde cero, por lo que al principio comenzamos trabajando con la API de bajo nivel y tensores. Sin embargo, si deseas avanzar r√°pidamente y no quieres pasar mucho tiempo aprendiendo estos detalles, puedes saltarte eso e ir directamente a los cuadernos de la API de alto nivel.

## ‚úçÔ∏è Ejercicios: Marcos

Contin√∫a tu aprendizaje en los siguientes cuadernos:

API de Bajo Nivel | [Cuaderno TensorFlow+Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
------------------|-------------------------------------|--------------------------------
API de Alto Nivel  | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Despu√©s de dominar los marcos, recapitularemos la noci√≥n de sobreajuste.

# Sobreajuste

El sobreajuste es un concepto extremadamente importante en el aprendizaje autom√°tico, ¬°y es muy importante entenderlo correctamente!

Considera el siguiente problema de aproximar 5 puntos (representados por `x` en los gr√°ficos a continuaci√≥n):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.es.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.es.jpg)
-------------------------|--------------------------
**Modelo lineal, 2 par√°metros** | **Modelo no lineal, 7 par√°metros**
Error de entrenamiento = 5.3 | Error de entrenamiento = 0
Error de validaci√≥n = 5.1 | Error de validaci√≥n = 20

* A la izquierda, vemos una buena aproximaci√≥n de l√≠nea recta. Debido a que el n√∫mero de par√°metros es adecuado, el modelo capta la idea detr√°s de la distribuci√≥n de puntos correctamente.
* A la derecha, el modelo es demasiado potente. Dado que solo tenemos 5 puntos y el modelo tiene 7 par√°metros, puede ajustarse de tal manera que pase por todos los puntos, haciendo que el error de entrenamiento sea 0. Sin embargo, esto impide que el modelo entienda el patr√≥n correcto detr√°s de los datos, por lo que el error de validaci√≥n es muy alto.

Es muy importante encontrar un equilibrio correcto entre la riqueza del modelo (n√∫mero de par√°metros) y la cantidad de muestras de entrenamiento.

## Por qu√© ocurre el sobreajuste

  * No hay suficientes datos de entrenamiento.
  * Modelo demasiado potente.
  * Demasiado ruido en los datos de entrada.

## C√≥mo detectar el sobreajuste

Como puedes ver en el gr√°fico anterior, el sobreajuste se puede detectar por un error de entrenamiento muy bajo y un error de validaci√≥n alto. Normalmente, durante el entrenamiento, veremos que tanto el error de entrenamiento como el de validaci√≥n comienzan a disminuir, y luego en alg√∫n momento el error de validaci√≥n puede dejar de disminuir y comenzar a aumentar. Esto ser√° una se√±al de sobreajuste y un indicador de que probablemente deber√≠amos detener el entrenamiento en este punto (o al menos hacer una instant√°nea del modelo).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.es.png)

## C√≥mo prevenir el sobreajuste

Si puedes ver que ocurre el sobreajuste, puedes hacer una de las siguientes acciones:

 * Aumentar la cantidad de datos de entrenamiento.
 * Disminuir la complejidad del modelo.
 * Utilizar alguna [t√©cnica de regularizaci√≥n](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), como [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), que consideraremos m√°s adelante.

## Sobreajuste y el Compromiso Bias-Varianza

El sobreajuste es en realidad un caso de un problema m√°s gen√©rico en estad√≠stica llamado [Compromiso Bias-Varianza](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Si consideramos las posibles fuentes de error en nuestro modelo, podemos ver dos tipos de errores:

* **Errores de sesgo** son causados por el hecho de que nuestro algoritmo no puede capturar correctamente la relaci√≥n entre los datos de entrenamiento. Puede resultar del hecho de que nuestro modelo no es lo suficientemente potente (**subajuste**).
* **Errores de varianza**, que son causados por el modelo que aproxima el ruido en los datos de entrada en lugar de una relaci√≥n significativa (**sobreajuste**).

Durante el entrenamiento, el error de sesgo disminuye (a medida que nuestro modelo aprende a aproximar los datos), y el error de varianza aumenta. Es importante detener el entrenamiento - ya sea manualmente (cuando detectamos sobreajuste) o autom√°ticamente (introduciendo regularizaci√≥n) - para prevenir el sobreajuste.

## Conclusi√≥n

En esta lecci√≥n, aprendiste sobre las diferencias entre las diversas APIs para los dos marcos de IA m√°s populares, TensorFlow y PyTorch. Adem√°s, aprendiste sobre un tema muy importante, el sobreajuste.

## üöÄ Desaf√≠o

En los cuadernos acompa√±antes, encontrar√°s 'tareas' al final; trabaja a trav√©s de los cuadernos y completa las tareas.

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Revisi√≥n y Autoestudio

Realiza una investigaci√≥n sobre los siguientes temas:

- TensorFlow
- PyTorch
- Sobreajuste

Preg√∫ntate las siguientes cuestiones:

- ¬øCu√°l es la diferencia entre TensorFlow y PyTorch?
- ¬øCu√°l es la diferencia entre sobreajuste y subajuste?

## [Asignaci√≥n](lab/README.md)

En este laboratorio, se te pide resolver dos problemas de clasificaci√≥n utilizando redes completamente conectadas de una y m√∫ltiples capas usando PyTorch o TensorFlow.

* [Instrucciones](lab/README.md)
* [Cuaderno](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en IA. Aunque nos esforzamos por la precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional humana. No somos responsables de malentendidos o interpretaciones err√≥neas que surjan del uso de esta traducci√≥n.