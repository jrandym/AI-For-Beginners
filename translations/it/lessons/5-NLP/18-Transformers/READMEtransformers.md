# M√©canismes d'Attention et Transformateurs

## [Quiz pr√©-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118)

L'un des probl√®mes les plus importants dans le domaine du traitement du langage naturel (NLP) est **la traduction automatique**, une t√¢che essentielle qui sous-tend des outils tels que Google Translate. Dans cette section, nous nous concentrerons sur la traduction automatique, ou, plus g√©n√©ralement, sur toute t√¢che *s√©quence-√†-s√©quence* (√©galement appel√©e **transduction de phrases**).

Avec les RNN, la s√©quence-√†-s√©quence est mise en ≈ìuvre par deux r√©seaux r√©currents, o√π un r√©seau, l'**encodeur**, r√©duit une s√©quence d'entr√©e en un √©tat cach√©, tandis qu'un autre r√©seau, le **d√©codeur**, d√©ploie cet √©tat cach√© en un r√©sultat traduit. Il y a quelques probl√®mes avec cette approche :

* L'√©tat final du r√©seau encodeur a du mal √† se souvenir du d√©but d'une phrase, ce qui entra√Æne une mauvaise qualit√© du mod√®le pour les longues phrases.
* Tous les mots d'une s√©quence ont le m√™me impact sur le r√©sultat. En r√©alit√©, cependant, certains mots sp√©cifiques dans la s√©quence d'entr√©e ont souvent plus d'impact sur les sorties s√©quentielles que d'autres.

**Les M√©canismes d'Attention** fournissent un moyen de pond√©rer l'impact contextuel de chaque vecteur d'entr√©e sur chaque pr√©diction de sortie du RNN. La fa√ßon dont cela est mis en ≈ìuvre consiste √† cr√©er des raccourcis entre les √©tats interm√©diaires du RNN d'entr√©e et du RNN de sortie. De cette mani√®re, lors de la g√©n√©ration du symbole de sortie y<sub>t</sub>, nous prendrons en compte tous les √©tats cach√©s d'entr√©e h<sub>i</sub>, avec diff√©rents coefficients de poids Œ±<sub>t,i</sub>.

![Image montrant un mod√®le encodeur/d√©codeur avec une couche d'attention additive](../../../../../translated_images/encoder-decoder-attention.7a726296894fb567aa2898c94b17b3289087f6705c11907df8301df9e5eeb3de.it.png)

> Le mod√®le encodeur-d√©codeur avec un m√©canisme d'attention additive dans [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cit√© depuis [ce billet de blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

La matrice d'attention {Œ±<sub>i,j</sub>} repr√©senterait le degr√© d'importance de certains mots d'entr√©e dans la g√©n√©ration d'un mot donn√© dans la s√©quence de sortie. Ci-dessous se trouve un exemple de telle matrice :

![Image montrant un alignement d'exemple trouv√© par RNNsearch-50, tir√©e de Bahdanau - arviz.org](../../../../../translated_images/bahdanau-fig3.09ba2d37f202a6af11de6c82d2d197830ba5f4528d9ea430eb65fd3a75065973.it.png)

> Figure de [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)

Les m√©canismes d'attention sont responsables d'une grande partie de l'√©tat de l'art actuel ou presque dans le NLP. Cependant, l'ajout d'attention augmente consid√©rablement le nombre de param√®tres du mod√®le, ce qui a conduit √† des probl√®mes d'√©chelle avec les RNN. Une contrainte cl√© de l'√©chelle des RNN est que la nature r√©currente des mod√®les rend difficile le traitement par lots et la parall√©lisation de l'entra√Ænement. Dans un RNN, chaque √©l√©ment d'une s√©quence doit √™tre trait√© dans un ordre s√©quentiel, ce qui signifie qu'il ne peut pas √™tre facilement parall√©lis√©.

![Encodeur D√©codeur avec Attention](../../../../../lessons/5-NLP/18-Transformers/images/EncDecAttention.gif)

> Figure provenant du [Blog de Google](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)

L'adoption des m√©canismes d'attention combin√©e √† cette contrainte a conduit √† la cr√©ation des mod√®les de transformateurs d√©sormais √† la pointe de la technologie que nous connaissons et utilisons aujourd'hui, tels que BERT et Open-GPT3.

## Mod√®les de Transformateurs

L'une des id√©es principales derri√®re les transformateurs est d'√©viter la nature s√©quentielle des RNN et de cr√©er un mod√®le qui soit parall√©lisable pendant l'entra√Ænement. Cela est r√©alis√© en mettant en ≈ìuvre deux id√©es :

* encodage positionnel
* utilisation du m√©canisme d'auto-attention pour capturer des motifs au lieu des RNN (ou des CNN) (c'est pourquoi le document qui introduit les transformateurs est intitul√© *[Attention is all you need](https://arxiv.org/abs/1706.03762)*)

### Encodage/Emballage Positionnel

L'id√©e de l'encodage positionnel est la suivante. 
1. Lors de l'utilisation des RNN, la position relative des tokens est repr√©sent√©e par le nombre d'√©tapes, et n'a donc pas besoin d'√™tre explicitement repr√©sent√©e. 
2. Cependant, une fois que nous passons √† l'attention, nous devons conna√Ætre les positions relatives des tokens au sein d'une s√©quence. 
3. Pour obtenir l'encodage positionnel, nous augmentons notre s√©quence de tokens avec une s√©quence de positions de tokens dans la s√©quence (c'est-√†-dire, une s√©quence de nombres 0,1, ...).
4. Nous m√©langeons ensuite la position du token avec un vecteur d'embedding de token. Pour transformer la position (entier) en un vecteur, nous pouvons utiliser diff√©rentes approches :

* Embedding entra√Ænable, similaire √† l'embedding de token. C'est l'approche que nous consid√©rons ici. Nous appliquons des couches d'embedding sur les tokens et leurs positions, r√©sultant en des vecteurs d'embedding de m√™mes dimensions, que nous ajoutons ensuite ensemble.
* Fonction d'encodage de position fixe, comme propos√© dans l'article original.

<img src="images/pos-embedding.png" width="50%"/>

> Image par l'auteur

Le r√©sultat que nous obtenons avec l'embedding positionnel incorpore √† la fois le token original et sa position au sein d'une s√©quence.

### Auto-Attention Multi-T√™te

Ensuite, nous devons capturer certains motifs au sein de notre s√©quence. Pour ce faire, les transformateurs utilisent un m√©canisme d'**auto-attention**, qui est essentiellement une attention appliqu√©e √† la m√™me s√©quence en tant qu'entr√©e et sortie. L'application de l'auto-attention nous permet de prendre en compte le **contexte** au sein de la phrase et de voir quels mots sont inter-reli√©s. Par exemple, cela nous permet de voir quels mots sont r√©f√©r√©s par des co-r√©f√©rences, telles que *il*, et √©galement de prendre en compte le contexte :

![](../../../../../translated_images/CoreferenceResolution.861924d6d384a7d68d8d0039d06a71a151f18a796b8b1330239d3590bd4947eb.it.png)

> Image provenant du [Blog de Google](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)

Dans les transformateurs, nous utilisons **l'Attention Multi-T√™te** afin de donner au r√©seau la capacit√© de capturer plusieurs types de d√©pendances diff√©rents, par exemple les relations entre mots √† long terme et √† court terme, co-r√©f√©rence par rapport √† autre chose, etc.

[Notebook TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb) contient plus de d√©tails sur la mise en ≈ìuvre des couches de transformateurs.

### Attention Encodeur-D√©codeur

Dans les transformateurs, l'attention est utilis√©e √† deux endroits :

* Pour capturer des motifs au sein du texte d'entr√©e √† l'aide de l'auto-attention
* Pour effectuer la traduction de s√©quence - c'est la couche d'attention entre l'encodeur et le d√©codeur.

L'attention encodeur-d√©codeur est tr√®s similaire au m√©canisme d'attention utilis√© dans les RNN, comme d√©crit au d√©but de cette section. Ce diagramme anim√© explique le r√¥le de l'attention encodeur-d√©codeur.

![GIF anim√© montrant comment les √©valuations sont effectu√©es dans les mod√®les de transformateurs.](../../../../../lessons/5-NLP/18-Transformers/images/transformer-animated-explanation.gif)

Puisque chaque position d'entr√©e est mapp√©e ind√©pendamment √† chaque position de sortie, les transformateurs peuvent mieux se parall√©liser que les RNN, ce qui permet des mod√®les de langage beaucoup plus grands et plus expressifs. Chaque t√™te d'attention peut √™tre utilis√©e pour apprendre diff√©rentes relations entre les mots, ce qui am√©liore les t√¢ches de traitement du langage naturel en aval.

## BERT

**BERT** (Repr√©sentations d'Encodeur Bidirectionnelles √† partir de Transformateurs) est un tr√®s grand r√©seau de transformateurs √† plusieurs couches avec 12 couches pour *BERT-base*, et 24 pour *BERT-large*. Le mod√®le est d'abord pr√©-entra√Æn√© sur un grand corpus de donn√©es textuelles (WikiPedia + livres) en utilisant un entra√Ænement non supervis√© (pr√©diction de mots masqu√©s dans une phrase). Au cours de la pr√©-formation, le mod√®le absorbe des niveaux significatifs de compr√©hension linguistique qui peuvent ensuite √™tre exploit√©s avec d'autres ensembles de donn√©es via un ajustement fin. Ce processus est appel√© **apprentissage par transfert**.

![image de http://jalammar.github.io/illustrated-bert/](../../../../../translated_images/jalammarBERT-language-modeling-masked-lm.34f113ea5fec4362e39ee4381aab7cad06b5465a0b5f053a0f2aa05fbe14e746.it.png)

> Image [source](http://jalammar.github.io/illustrated-bert/)

## ‚úçÔ∏è Exercices : Transformateurs

Poursuivez votre apprentissage dans les notebooks suivants :

* [Transformateurs en PyTorch](../../../../../lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb)
* [Transformateurs en TensorFlow](../../../../../lessons/5-NLP/18-Transformers/TransformersTF.ipynb)

## Conclusion

Dans cette le√ßon, vous avez appris sur les Transformateurs et les M√©canismes d'Attention, tous des outils essentiels dans la bo√Æte √† outils NLP. Il existe de nombreuses variations des architectures de Transformateurs, y compris BERT, DistilBERT, BigBird, OpenGPT3 et plus encore qui peuvent √™tre ajust√©es. Le [package HuggingFace](https://github.com/huggingface/) fournit un r√©f√©rentiel pour entra√Æner bon nombre de ces architectures avec PyTorch et TensorFlow.

## üöÄ D√©fi

## [Quiz post-conf√©rence](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218)

## R√©vision et Auto-√©tude

* [Billet de blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/), expliquant l'article classique [Attention is all you need](https://arxiv.org/abs/1706.03762) sur les transformateurs.
* [Une s√©rie de billets de blog](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) sur les transformateurs, expliquant l'architecture en d√©tail.

## [Devoir](assignment.md)

**Disclaimer**: 
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.