# Redes Generativas

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Las Redes Neuronales Recurrentes (RNN) y sus variantes con celdas de compuerta, como las Celdas de Memoria a Largo Plazo (LSTM) y las Unidades Recurrentes con Compuerta (GRU), proporcionan un mecanismo para el modelado del lenguaje, ya que pueden aprender el orden de las palabras y ofrecer predicciones para la siguiente palabra en una secuencia. Esto nos permite utilizar RNN para **tareas generativas**, como la generaci√≥n de texto ordinario, la traducci√≥n autom√°tica e incluso la creaci√≥n de subt√≠tulos para im√°genes.

> ‚úÖ Piensa en todas las veces que te has beneficiado de tareas generativas, como la finalizaci√≥n de texto mientras escribes. Investiga en tus aplicaciones favoritas para ver si han aprovechado las RNN.

En la arquitectura de RNN que discutimos en la unidad anterior, cada unidad RNN produc√≠a el siguiente estado oculto como salida. Sin embargo, tambi√©n podemos agregar otra salida a cada unidad recurrente, lo que nos permitir√≠a generar una **secuencia** (que tiene la misma longitud que la secuencia original). Adem√°s, podemos utilizar unidades RNN que no aceptan una entrada en cada paso, y solo toman un vector de estado inicial, para luego producir una secuencia de salidas.

Esto permite diferentes arquitecturas neuronales que se muestran en la imagen a continuaci√≥n:

![Imagen que muestra patrones comunes de redes neuronales recurrentes.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.it.jpg)

> Imagen del art√≠culo [Efectividad Irrazonable de las Redes Neuronales Recurrentes](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) de [Andrej Karpaty](http://karpathy.github.io/)

* **Uno a uno** es una red neuronal tradicional con una entrada y una salida.
* **Uno a muchos** es una arquitectura generativa que acepta un valor de entrada y genera una secuencia de valores de salida. Por ejemplo, si queremos entrenar una red de **subtitulaci√≥n de im√°genes** que produzca una descripci√≥n textual de una imagen, podemos usar una imagen como entrada, pasarla a trav√©s de una CNN para obtener su estado oculto, y luego tener una cadena recurrente que genere el subt√≠tulo palabra por palabra.
* **Muchos a uno** corresponde a las arquitecturas RNN que describimos en la unidad anterior, como la clasificaci√≥n de texto.
* **Muchos a muchos**, o **secuencia a secuencia**, corresponde a tareas como la **traducci√≥n autom√°tica**, donde una primera RNN recopila toda la informaci√≥n de la secuencia de entrada en el estado oculto, y otra cadena RNN despliega este estado en la secuencia de salida.

En esta unidad, nos enfocaremos en modelos generativos simples que nos ayuden a generar texto. Para simplificar, utilizaremos la tokenizaci√≥n a nivel de caracteres.

Entrenaremos esta RNN para generar texto paso a paso. En cada paso, tomaremos una secuencia de caracteres de longitud `nchars` y pediremos a la red que genere el siguiente car√°cter de salida para cada car√°cter de entrada:

![Imagen que muestra un ejemplo de generaci√≥n de la palabra 'HELLO' por una RNN.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.it.png)

Al generar texto (durante la inferencia), comenzamos con alg√∫n **prompter**, que se pasa a trav√©s de las celdas RNN para generar su estado intermedio, y luego desde este estado comienza la generaci√≥n. Generamos un car√°cter a la vez y pasamos el estado y el car√°cter generado a otra celda RNN para generar el siguiente, hasta que generemos suficientes caracteres.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Imagen del autor

## ‚úçÔ∏è Ejercicios: Redes Generativas

Contin√∫a tu aprendizaje en los siguientes cuadernos:

* [Redes Generativas con PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Redes Generativas con TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Generaci√≥n de texto suave y temperatura

La salida de cada celda RNN es una distribuci√≥n de probabilidad de caracteres. Si siempre tomamos el car√°cter con la probabilidad m√°s alta como el siguiente car√°cter en el texto generado, el texto a menudo puede volverse "c√≠clico" entre las mismas secuencias de caracteres una y otra vez, como en este ejemplo:

```
today of the second the company and a second the company ...
```

Sin embargo, si miramos la distribuci√≥n de probabilidad para el siguiente car√°cter, podr√≠a ser que la diferencia entre algunas de las probabilidades m√°s altas no sea enorme, por ejemplo, un car√°cter puede tener una probabilidad de 0.2, otro - 0.19, etc. Por ejemplo, al buscar el siguiente car√°cter en la secuencia '*play*', el siguiente car√°cter podr√≠a ser igualmente un espacio o **e** (como en la palabra *player*).

Esto nos lleva a la conclusi√≥n de que no siempre es "justo" seleccionar el car√°cter con una probabilidad m√°s alta, porque elegir el segundo m√°s alto podr√≠a a√∫n llevarnos a un texto significativo. Es m√°s sabio **muestrear** caracteres de la distribuci√≥n de probabilidad dada por la salida de la red. Tambi√©n podemos utilizar un par√°metro, **temperatura**, que aplanar√° la distribuci√≥n de probabilidad, en caso de que queramos a√±adir m√°s aleatoriedad, o hacerla m√°s pronunciada, si queremos ce√±irnos m√°s a los caracteres de mayor probabilidad.

Explora c√≥mo se implementa esta generaci√≥n de texto suave en los cuadernos enlazados anteriormente.

## Conclusi√≥n

Aunque la generaci√≥n de texto puede ser √∫til por s√≠ misma, los principales beneficios provienen de la capacidad de generar texto utilizando RNN a partir de alg√∫n vector de caracter√≠sticas inicial. Por ejemplo, la generaci√≥n de texto se utiliza como parte de la traducci√≥n autom√°tica (secuencia a secuencia, en este caso el vector de estado del *encoder* se utiliza para generar o *decodificar* el mensaje traducido), o para generar una descripci√≥n textual de una imagen (en cuyo caso el vector de caracter√≠sticas provendr√≠a de un extractor CNN).

## üöÄ Desaf√≠o

Toma algunas lecciones en Microsoft Learn sobre este tema.

* Generaci√≥n de Texto con [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## Revisi√≥n y Autoestudio

Aqu√≠ hay algunos art√≠culos para ampliar tus conocimientos:

* Diferentes enfoques para la generaci√≥n de texto con Cadenas de Markov, LSTM y GPT-2: [art√≠culo del blog](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Ejemplo de generaci√≥n de texto en la [documentaci√≥n de Keras](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Asignaci√≥n](lab/README.md)

Hemos visto c√≥mo generar texto car√°cter por car√°cter. En el laboratorio, explorar√°s la generaci√≥n de texto a nivel de palabras.

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.