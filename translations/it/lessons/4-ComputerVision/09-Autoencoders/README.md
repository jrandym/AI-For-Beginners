# Autoencoders

Cuando se entrena CNNs, uno de los problemas es que necesitamos una gran cantidad de datos etiquetados. En el caso de la clasificaci√≥n de im√°genes, debemos separar las im√°genes en diferentes clases, lo que implica un esfuerzo manual.

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/109)

Sin embargo, podr√≠amos querer utilizar datos en bruto (sin etiquetar) para entrenar extractores de caracter√≠sticas de CNN, lo que se denomina **aprendizaje auto-supervisado**. En lugar de etiquetas, utilizaremos im√°genes de entrenamiento como entrada y salida de la red. La idea principal de un **autoencoder** es que tendremos una **red de codificaci√≥n** que convierte la imagen de entrada en un **espacio latente** (normalmente es solo un vector de menor tama√±o), y luego la **red de decodificaci√≥n**, cuyo objetivo es reconstruir la imagen original.

> ‚úÖ Un [autoencoder](https://wikipedia.org/wiki/Autoencoder) es "un tipo de red neuronal artificial utilizada para aprender codificaciones eficientes de datos no etiquetados."

Dado que estamos entrenando un autoencoder para capturar la mayor cantidad de informaci√≥n posible de la imagen original para una reconstrucci√≥n precisa, la red intenta encontrar la mejor **inmersi√≥n** de las im√°genes de entrada para capturar su significado.

![Diagrama de Autoencoder](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.it.jpg)

> Imagen del [blog de Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

## Escenarios para usar Autoencoders

Aunque reconstruir im√°genes originales no parece √∫til por s√≠ mismo, hay algunos escenarios donde los autoencoders son especialmente √∫tiles:

* **Reducir la dimensi√≥n de las im√°genes para visualizaci√≥n** o **entrenar embeddings de im√°genes**. Generalmente, los autoencoders ofrecen mejores resultados que PCA, porque tienen en cuenta la naturaleza espacial de las im√°genes y las caracter√≠sticas jer√°rquicas.
* **Eliminaci√≥n de ruido**, es decir, quitar el ruido de la imagen. Debido a que el ruido contiene mucha informaci√≥n innecesaria, el autoencoder no puede ajustarlo todo en un espacio latente relativamente peque√±o, y por lo tanto solo captura la parte importante de la imagen. Al entrenar eliminadores de ruido, comenzamos con im√°genes originales y utilizamos im√°genes con ruido a√±adido artificialmente como entrada para el autoencoder.
* **Super-resoluci√≥n**, aumentando la resoluci√≥n de la imagen. Comenzamos con im√°genes de alta resoluci√≥n y utilizamos im√°genes de menor resoluci√≥n como entrada para el autoencoder.
* **Modelos generativos**. Una vez que entrenamos el autoencoder, la parte del decodificador puede usarse para crear nuevos objetos a partir de vectores latentes aleatorios.

## Autoencoders Variacionales (VAE)

Los autoencoders tradicionales reducen la dimensi√≥n de los datos de entrada de alguna manera, identificando las caracter√≠sticas importantes de las im√°genes de entrada. Sin embargo, los vectores latentes a menudo no tienen mucho sentido. En otras palabras, tomando como ejemplo el conjunto de datos MNIST, identificar qu√© d√≠gitos corresponden a diferentes vectores latentes no es una tarea f√°cil, ya que vectores latentes cercanos no necesariamente corresponden a los mismos d√≠gitos.

Por otro lado, para entrenar modelos *generativos*, es mejor tener cierta comprensi√≥n del espacio latente. Esta idea nos lleva a los **autoencoders variacionales** (VAE).

El VAE es el autoencoder que aprende a predecir la *distribuci√≥n estad√≠stica* de los par√°metros latentes, denominada **distribuci√≥n latente**. Por ejemplo, podemos querer que los vectores latentes se distribuyan normalmente con una media z<sub>mean</sub> y una desviaci√≥n est√°ndar z<sub>sigma</sub> (tanto la media como la desviaci√≥n est√°ndar son vectores de alguna dimensionalidad d). El codificador en el VAE aprende a predecir esos par√°metros, y luego el decodificador toma un vector aleatorio de esta distribuci√≥n para reconstruir el objeto.

Para resumir:

 * Desde el vector de entrada, predecimos `z_mean` y `z_log_sigma` (en lugar de predecir la desviaci√≥n est√°ndar en s√≠, predecimos su logaritmo)
 * Muestreamos un vector `sample` de la distribuci√≥n N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * El decodificador intenta decodificar la imagen original utilizando `sample` como vector de entrada

 <img src="images/vae.png" width="50%">

> Imagen de [este post del blog](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) de Isaak Dykeman

Los autoencoders variacionales utilizan una funci√≥n de p√©rdida compleja que consiste en dos partes:

* **P√©rdida de reconstrucci√≥n** es la funci√≥n de p√©rdida que muestra cu√°n cerca est√° una imagen reconstruida del objetivo (puede ser el Error Cuadr√°tico Medio, o MSE). Es la misma funci√≥n de p√©rdida que en los autoencoders normales.
* **P√©rdida KL**, que asegura que las distribuciones de variables latentes se mantengan cerca de la distribuci√≥n normal. Se basa en la noci√≥n de [divergencia de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - una m√©trica para estimar cu√°n similares son dos distribuciones estad√≠sticas.

Una ventaja importante de los VAE es que nos permiten generar nuevas im√°genes de manera relativamente f√°cil, porque sabemos de qu√© distribuci√≥n muestrear vectores latentes. Por ejemplo, si entrenamos un VAE con un vector latente 2D en MNIST, podemos variar los componentes del vector latente para obtener diferentes d√≠gitos:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Imagen de [Dmitry Soshnikov](http://soshnikov.com)

Observa c√≥mo las im√°genes se fusionan entre s√≠, a medida que comenzamos a obtener vectores latentes de diferentes porciones del espacio de par√°metros latentes. Tambi√©n podemos visualizar este espacio en 2D:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Imagen de [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è Ejercicios: Autoencoders

Aprende m√°s sobre autoencoders en estos cuadernos correspondientes:

* [Autoencoders en TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Autoencoders en PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Propiedades de los Autoencoders

* **Espec√≠ficos para datos** - solo funcionan bien con el tipo de im√°genes en las que han sido entrenados. Por ejemplo, si entrenamos una red de super-resoluci√≥n en flores, no funcionar√° bien en retratos. Esto se debe a que la red puede producir una imagen de mayor resoluci√≥n tomando detalles finos de las caracter√≠sticas aprendidas del conjunto de datos de entrenamiento.
* **Con p√©rdida** - la imagen reconstruida no es la misma que la imagen original. La naturaleza de la p√©rdida est√° definida por la *funci√≥n de p√©rdida* utilizada durante el entrenamiento.
* Funciona con **datos no etiquetados**.

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/209)

## Conclusi√≥n

En esta lecci√≥n, aprendiste sobre los diversos tipos de autoencoders disponibles para el cient√≠fico de IA. Aprendiste c√≥mo construirlos y c√≥mo usarlos para reconstruir im√°genes. Tambi√©n aprendiste sobre el VAE y c√≥mo utilizarlo para generar nuevas im√°genes.

## üöÄ Desaf√≠o

En esta lecci√≥n, aprendiste sobre el uso de autoencoders para im√°genes. ¬°Pero tambi√©n pueden ser utilizados para m√∫sica! Echa un vistazo al proyecto [MusicVAE](https://magenta.tensorflow.org/music-vae) del proyecto Magenta, que utiliza autoencoders para aprender a reconstruir m√∫sica. Realiza algunos [experimentos](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) con esta biblioteca para ver qu√© puedes crear.

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## Revisi√≥n y Autoestudio

Para referencia, lee m√°s sobre autoencoders en estos recursos:

* [Construyendo Autoencoders en Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Publicaci√≥n en el blog sobre NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Autoencoders Variacionales Explicados](https://kvfrans.com/variational-autoencoders-explained/)
* [Autoencoders Variacionales Condicionales](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Asignaci√≥n

Al final de [este cuaderno usando TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb), encontrar√°s una 'tarea' - utiliza esto como tu asignaci√≥n.

**Disclaimer**:  
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en IA. Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No somos responsables de malentendidos o malas interpretaciones que surjan del uso de esta traducci√≥n.