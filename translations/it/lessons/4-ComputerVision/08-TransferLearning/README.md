# Redes Neurais Pr√©-treinadas e Aprendizado por Transfer√™ncia

Treinar CNNs pode levar muito tempo, e uma grande quantidade de dados √© necess√°ria para essa tarefa. No entanto, muito do tempo √© gasto aprendendo os melhores filtros de baixo n√≠vel que uma rede pode usar para extrair padr√µes de imagens. Surge uma pergunta natural: podemos usar uma rede neural treinada em um conjunto de dados e adapt√°-la para classificar diferentes imagens sem precisar de um processo completo de treinamento?

## [Quiz pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/108)

Essa abordagem √© chamada de **aprendizado por transfer√™ncia**, porque transferimos algum conhecimento de um modelo de rede neural para outro. No aprendizado por transfer√™ncia, normalmente come√ßamos com um modelo pr√©-treinado, que foi treinado em um grande conjunto de dados de imagens, como o **ImageNet**. Esses modelos j√° conseguem extrair diferentes caracter√≠sticas de imagens gen√©ricas, e em muitos casos, simplesmente construir um classificador em cima dessas caracter√≠sticas extra√≠das pode resultar em um bom desempenho.

> ‚úÖ Aprendizado por Transfer√™ncia √© um termo que voc√™ encontra em outros campos acad√™micos, como Educa√ß√£o. Refere-se ao processo de levar conhecimento de um dom√≠nio e aplic√°-lo a outro.

## Modelos Pr√©-treinados como Extratores de Recursos

As redes convolucionais que discutimos na se√ß√£o anterior continham v√°rias camadas, cada uma delas destinada a extrair algumas caracter√≠sticas da imagem, come√ßando por combina√ß√µes de pixels de baixo n√≠vel (como linhas ou tra√ßos horizontais/verticais), at√© combina√ß√µes de caracter√≠sticas de n√≠vel mais alto, correspondendo a coisas como o olho de uma chama. Se treinarmos uma CNN em um conjunto de dados suficientemente grande de imagens gen√©ricas e diversas, a rede deve aprender a extrair essas caracter√≠sticas comuns.

Tanto o Keras quanto o PyTorch cont√™m fun√ß√µes para carregar facilmente os pesos de redes neurais pr√©-treinadas para algumas arquiteturas comuns, a maioria das quais foi treinada em imagens do ImageNet. Os mais utilizados est√£o descritos na p√°gina de [Arquiteturas de CNN](../07-ConvNets/CNN_Architectures.md) da aula anterior. Em particular, voc√™ pode considerar usar uma das seguintes:

* **VGG-16/VGG-19**, que s√£o modelos relativamente simples, mas ainda oferecem boa precis√£o. Muitas vezes, usar o VGG como uma primeira tentativa √© uma boa escolha para ver como o aprendizado por transfer√™ncia funciona.
* **ResNet** √© uma fam√≠lia de modelos proposta pela Microsoft Research em 2015. Eles t√™m mais camadas e, portanto, consomem mais recursos.
* **MobileNet** √© uma fam√≠lia de modelos com tamanho reduzido, adequada para dispositivos m√≥veis. Use-os se voc√™ tiver poucos recursos e puder sacrificar um pouco de precis√£o.

Aqui est√£o exemplos de caracter√≠sticas extra√≠das de uma imagem de um gato pela rede VGG-16:

![Caracter√≠sticas extra√≠das pela VGG-16](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.it.png)

## Conjunto de Dados de Gatos vs. Cachorros

Neste exemplo, usaremos um conjunto de dados de [Gatos e Cachorros](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste), que √© muito pr√≥ximo de um cen√°rio real de classifica√ß√£o de imagens.

## ‚úçÔ∏è Exerc√≠cio: Aprendizado por Transfer√™ncia

Vamos ver o aprendizado por transfer√™ncia em a√ß√£o nos cadernos correspondentes:

* [Aprendizado por Transfer√™ncia - PyTorch](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb)
* [Aprendizado por Transfer√™ncia - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb)

## Visualizando o Gato Adversarial

Uma rede neural pr√©-treinada cont√©m diferentes padr√µes dentro de seu *c√©rebro*, incluindo no√ß√µes de **gato ideal** (assim como cachorro ideal, zebra ideal, etc.). Seria interessante de alguma forma **visualizar essa imagem**. No entanto, isso n√£o √© simples, pois os padr√µes est√£o espalhados por todo o peso da rede e tamb√©m organizados em uma estrutura hier√°rquica.

Uma abordagem que podemos adotar √© come√ßar com uma imagem aleat√≥ria e, em seguida, tentar usar a t√©cnica de **otimiza√ß√£o por gradiente** para ajustar essa imagem de tal forma que a rede comece a pensar que √© um gato.

![Loop de Otimiza√ß√£o de Imagem](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.it.png)

No entanto, se fizermos isso, receberemos algo muito semelhante a um ru√≠do aleat√≥rio. Isso ocorre porque *existem muitas maneiras de fazer a rede pensar que a imagem de entrada √© um gato*, incluindo algumas que n√£o fazem sentido visualmente. Embora essas imagens contenham muitos padr√µes t√≠picos de um gato, n√£o h√° nada que as restrinja a serem visualmente distintas.

Para melhorar o resultado, podemos adicionar outro termo √† fun√ß√£o de perda, que √© chamado de **perda de varia√ß√£o**. √â uma m√©trica que mostra qu√£o semelhantes s√£o os pixels vizinhos da imagem. Minimizar a perda de varia√ß√£o torna a imagem mais suave e elimina o ru√≠do, revelando assim padr√µes visualmente mais atraentes. Aqui est√° um exemplo de tais imagens "ideais", que s√£o classificadas como gato e como zebra com alta probabilidade:

![Gato Ideal](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.it.png) | ![Zebra Ideal](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.it.png)
-----|-----
 *Gato Ideal* | *Zebra Ideal*

Uma abordagem semelhante pode ser usada para realizar os chamados **ataques adversariais** em uma rede neural. Suponha que queremos enganar uma rede neural e fazer um cachorro parecer um gato. Se pegarmos a imagem de um cachorro, que √© reconhecida por uma rede como um cachorro, podemos ent√£o ajust√°-la um pouco usando a otimiza√ß√£o por gradiente, at√© que a rede comece a classific√°-la como um gato:

![Imagem de um Cachorro](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.it.png) | ![Imagem de um cachorro classificado como gato](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.it.png)
-----|-----
*Imagem original de um cachorro* | *Imagem de um cachorro classificado como gato*

Veja o c√≥digo para reproduzir os resultados acima no seguinte caderno:

* [Gato Ideal e Adversarial - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/AdversarialCat_TF.ipynb)
## Conclus√£o

Usando o aprendizado por transfer√™ncia, voc√™ consegue montar rapidamente um classificador para uma tarefa de classifica√ß√£o de objetos personalizados e alcan√ßar alta precis√£o. Voc√™ pode ver que tarefas mais complexas que estamos resolvendo agora exigem maior poder computacional e n√£o podem ser facilmente resolvidas na CPU. Na pr√≥xima unidade, tentaremos usar uma implementa√ß√£o mais leve para treinar o mesmo modelo usando recursos computacionais menores, o que resulta em uma precis√£o apenas ligeiramente inferior.

## üöÄ Desafio

Nos cadernos acompanhados, h√° notas no final sobre como o conhecimento transferido funciona melhor com dados de treinamento um tanto semelhantes (um novo tipo de animal, talvez). Fa√ßa algumas experimenta√ß√µes com tipos de imagens completamente novos para ver qu√£o bem ou mal seus modelos de conhecimento transferido se comportam.

## [Quiz p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## Revis√£o e Autoestudo

Leia [TrainingTricks.md](TrainingTricks.md) para aprofundar seu conhecimento sobre algumas outras maneiras de treinar seus modelos.

## [Tarefa](lab/README.md)

Neste laborat√≥rio, usaremos um conjunto de dados de pets da [Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/) com 35 ra√ßas de gatos e cachorros, e construiremos um classificador de aprendizado por transfer√™ncia.

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.