# Apprentissage par Renforcement Profond

L'apprentissage par renforcement (RL) est consid√©r√© comme l'un des paradigmes fondamentaux de l'apprentissage machine, aux c√¥t√©s de l'apprentissage supervis√© et non supervis√©. Alors que dans l'apprentissage supervis√©, nous nous appuyons sur un ensemble de donn√©es avec des r√©sultats connus, le RL repose sur **l'apprentissage par l'action**. Par exemple, lorsque nous d√©couvrons un nouveau jeu vid√©o, nous commen√ßons √† jouer, m√™me sans conna√Ætre les r√®gles, et rapidement, nous parvenons √† am√©liorer nos comp√©tences simplement en jouant et en ajustant notre comportement.

## [Quiz pr√©-cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Pour effectuer du RL, nous avons besoin de :

* Un **environnement** ou **simulateur** qui d√©finit les r√®gles du jeu. Nous devrions √™tre capables de r√©aliser des exp√©riences dans le simulateur et d'observer les r√©sultats.
* Une **fonction de r√©compense**, qui indique √† quel point notre exp√©rience a √©t√© r√©ussie. Dans le cas de l'apprentissage d'un jeu vid√©o, la r√©compense serait notre score final.

Sur la base de la fonction de r√©compense, nous devrions √™tre en mesure d'ajuster notre comportement et d'am√©liorer nos comp√©tences, de sorte qu'√† la prochaine partie, nous jouions mieux. La principale diff√©rence entre les autres types d'apprentissage machine et le RL est qu'en RL, nous ne savons g√©n√©ralement pas si nous gagnons ou perdons avant d'avoir termin√© le jeu. Ainsi, nous ne pouvons pas dire si un mouvement particulier est bon ou non - nous ne recevons une r√©compense qu'√† la fin du jeu.

Lors de l'apprentissage par renforcement, nous r√©alisons g√©n√©ralement de nombreuses exp√©riences. Au cours de chaque exp√©rience, nous devons √©quilibrer entre le suivi de la strat√©gie optimale que nous avons apprise jusqu'√† pr√©sent (**exploitation**) et l'exploration de nouveaux √©tats possibles (**exploration**).

## OpenAI Gym

Un excellent outil pour le RL est l'[OpenAI Gym](https://gym.openai.com/) - un **environnement de simulation**, capable de simuler de nombreux environnements diff√©rents, allant des jeux Atari √† la physique derri√®re l'√©quilibre d'un poteau. C'est l'un des environnements de simulation les plus populaires pour former des algorithmes d'apprentissage par renforcement, et il est maintenu par [OpenAI](https://openai.com/).

> **Note** : Vous pouvez voir tous les environnements disponibles dans OpenAI Gym [ici](https://gym.openai.com/envs/#classic_control).

## √âquilibre CartPole

Vous avez probablement tous vu des dispositifs d'√©quilibre modernes tels que le *Segway* ou les *Gyroscooters*. Ils parviennent √† s'√©quilibrer automatiquement en ajustant leurs roues en r√©ponse √† un signal provenant d'un acc√©l√©rom√®tre ou d'un gyroscope. Dans cette section, nous allons apprendre √† r√©soudre un probl√®me similaire - l'√©quilibre d'un poteau. C'est comparable √† une situation o√π un artiste de cirque doit √©quilibrer un poteau sur sa main - mais cet √©quilibre ne se produit que dans une dimension.

Une version simplifi√©e de l'√©quilibre est connue sous le nom de probl√®me **CartPole**. Dans le monde du cartpole, nous avons un curseur horizontal qui peut se d√©placer √† gauche ou √† droite, et l'objectif est de maintenir un poteau vertical au sommet du curseur pendant qu'il se d√©place.

<img alt="un cartpole" src="images/cartpole.png" width="200"/>

Pour cr√©er et utiliser cet environnement, nous avons besoin de quelques lignes de code Python :

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Chaque environnement peut √™tre accessible de la m√™me mani√®re :
* `env.reset` starts a new experiment
* `env.step` effectue une √©tape de simulation. Il re√ßoit une **action** de l'**espace d'actions** et renvoie une **observation** (de l'espace d'observation), ainsi qu'une r√©compense et un indicateur de terminaison.

Dans l'exemple ci-dessus, nous effectuons une action al√©atoire √† chaque √©tape, ce qui explique pourquoi la dur√©e de vie de l'exp√©rience est tr√®s courte :

![cartpole non √©quilibr√©](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

L'objectif d'un algorithme de RL est de former un mod√®le - la soi-disant **politique** œÄ - qui renverra l'action en r√©ponse √† un √©tat donn√©. Nous pouvons √©galement consid√©rer la politique comme √©tant probabiliste, par exemple, pour tout √©tat *s* et action *a*, elle renverra la probabilit√© œÄ(*a*|*s*) que nous devrions prendre *a* dans l'√©tat *s*.

## Algorithme des Gradients de Politique

La mani√®re la plus √©vidente de mod√©liser une politique est de cr√©er un r√©seau de neurones qui prendra des √©tats en entr√©e et renverra les actions correspondantes (ou plut√¥t les probabilit√©s de toutes les actions). En un sens, cela serait similaire √† une t√¢che de classification normale, avec une diff√©rence majeure - nous ne savons pas √† l'avance quelles actions nous devrions prendre √† chacune des √©tapes.

L'id√©e ici est d'estimer ces probabilit√©s. Nous construisons un vecteur de **r√©compenses cumul√©es** qui montre notre r√©compense totale √† chaque √©tape de l'exp√©rience. Nous appliquons √©galement un **escompte de r√©compense** en multipliant les r√©compenses ant√©rieures par un coefficient Œ≥=0.99, afin de diminuer le r√¥le des r√©compenses ant√©rieures. Ensuite, nous renfor√ßons ces √©tapes le long du chemin de l'exp√©rience qui produisent des r√©compenses plus importantes.

> En savoir plus sur l'algorithme de Gradient de Politique et le voir en action dans le [carnet d'exemples](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Algorithme Acteur-Critique

Une version am√©lior√©e de l'approche des Gradients de Politique est appel√©e **Acteur-Critique**. L'id√©e principale est que le r√©seau de neurones serait entra√Æn√© pour renvoyer deux choses :

* La politique, qui d√©termine quelle action prendre. Cette partie est appel√©e **acteur**.
* L'estimation de la r√©compense totale que nous pouvons esp√©rer obtenir dans cet √©tat - cette partie est appel√©e **critique**.

En un sens, cette architecture ressemble √† un [GAN](../../4-ComputerVision/10-GANs/README.md), o√π nous avons deux r√©seaux qui sont entra√Æn√©s l'un contre l'autre. Dans le mod√®le acteur-critique, l'acteur propose l'action que nous devons prendre, et le critique essaie d'√™tre critique et d'estimer le r√©sultat. Cependant, notre objectif est d'entra√Æner ces r√©seaux de mani√®re conjointe.

Comme nous connaissons √† la fois les vraies r√©compenses cumul√©es et les r√©sultats renvoy√©s par le critique pendant l'exp√©rience, il est relativement facile de construire une fonction de perte qui minimisera la diff√©rence entre elles. Cela nous donnerait la **perte du critique**. Nous pouvons calculer la **perte de l'acteur** en utilisant la m√™me approche que dans l'algorithme de gradient de politique.

Apr√®s avoir ex√©cut√© l'un de ces algorithmes, nous pouvons nous attendre √† ce que notre CartPole se comporte comme ceci :

![un cartpole √©quilibr√©](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è Exercices : Gradients de Politique et RL Acteur-Critique

Poursuivez votre apprentissage dans les carnets suivants :

* [RL dans TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL dans PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Autres T√¢ches de RL

L'apprentissage par renforcement est aujourd'hui un domaine de recherche en pleine expansion. Voici quelques exemples int√©ressants d'apprentissage par renforcement :

* Apprendre √† un ordinateur √† jouer √† des **jeux Atari**. La difficult√© dans ce probl√®me est que nous n'avons pas d'√©tat simple repr√©sent√© sous forme de vecteur, mais plut√¥t une capture d'√©cran - et nous devons utiliser le CNN pour convertir cette image d'√©cran en un vecteur de caract√©ristiques, ou pour extraire des informations de r√©compense. Les jeux Atari sont disponibles dans le Gym.
* Apprendre √† un ordinateur √† jouer √† des jeux de soci√©t√©, tels que les √©checs et le Go. R√©cemment, des programmes √† la pointe de la technologie comme **Alpha Zero** ont √©t√© entra√Æn√©s √† partir de z√©ro par deux agents jouant l'un contre l'autre, s'am√©liorant √† chaque √©tape.
* Dans l'industrie, le RL est utilis√© pour cr√©er des syst√®mes de contr√¥le √† partir de simulations. Un service appel√© [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) est sp√©cifiquement con√ßu pour cela.

## Conclusion

Nous avons maintenant appris comment former des agents pour obtenir de bons r√©sultats simplement en leur fournissant une fonction de r√©compense qui d√©finit l'√©tat souhait√© du jeu, et en leur offrant la possibilit√© d'explorer intelligemment l'espace de recherche. Nous avons essay√© avec succ√®s deux algorithmes et obtenu un bon r√©sultat dans un d√©lai relativement court. Cependant, ce n'est que le d√©but de votre voyage dans le RL, et vous devriez certainement envisager de suivre un cours s√©par√© si vous souhaitez approfondir vos connaissances.

## üöÄ D√©fi

Explorez les applications √©num√©r√©es dans la section 'Autres T√¢ches de RL' et essayez d'en impl√©menter une !

## [Quiz post-cours](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## Revue et Auto-√©tude

En savoir plus sur l'apprentissage par renforcement classique dans notre [Curriculum d'Apprentissage Machine pour D√©butants](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Regardez [cette excellente vid√©o](https://www.youtube.com/watch?v=qv6UVOQ0F44) qui parle de la fa√ßon dont un ordinateur peut apprendre √† jouer √† Super Mario.

## Devoir : [Entra√Æner une Voiture de Montagne](lab/README.md)

Votre objectif lors de ce devoir serait d'entra√Æner un environnement Gym diff√©rent - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.