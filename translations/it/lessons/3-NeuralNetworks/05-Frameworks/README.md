# Frameworks de Redes Neurais

Como j√° aprendemos, para treinar redes neurais de forma eficiente, precisamos fazer duas coisas:

* Operar em tensores, por exemplo, multiplicar, adicionar e calcular algumas fun√ß√µes como sigmoid ou softmax
* Calcular gradientes de todas as express√µes, a fim de realizar a otimiza√ß√£o por descida de gradiente

## [Question√°rio pr√©-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

Enquanto a biblioteca `numpy` pode realizar a primeira parte, precisamos de algum mecanismo para calcular gradientes. No [nosso framework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) que desenvolvemos na se√ß√£o anterior, tivemos que programar manualmente todas as fun√ß√µes derivadas dentro do m√©todo `backward`, que realiza a retropropaga√ß√£o. Idealmente, um framework deve nos oferecer a oportunidade de calcular gradientes de *qualquer express√£o* que possamos definir.

Outra coisa importante √© ser capaz de realizar c√°lculos em GPU ou em outras unidades de computa√ß√£o especializadas, como o [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit). O treinamento de redes neurais profundas exige *muitos* c√°lculos, e a capacidade de paralelizar esses c√°lculos em GPUs √© muito importante.

> ‚úÖ O termo 'paralelizar' significa distribuir os c√°lculos entre v√°rios dispositivos.

Atualmente, os dois frameworks de redes neurais mais populares s√£o: [TensorFlow](http://TensorFlow.org) e [PyTorch](https://pytorch.org/). Ambos fornecem uma API de baixo n√≠vel para operar com tensores tanto em CPU quanto em GPU. Al√©m da API de baixo n√≠vel, tamb√©m existe uma API de n√≠vel superior, chamada [Keras](https://keras.io/) e [PyTorch Lightning](https://pytorchlightning.ai/) respectivamente.

API de Baixo N√≠vel | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
-------------------|-------------------------------------|--------------------------------
API de Alto N√≠vel  | [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**APIs de baixo n√≠vel** em ambos os frameworks permitem que voc√™ construa os chamados **gr√°ficos computacionais**. Este gr√°fico define como calcular a sa√≠da (geralmente a fun√ß√£o de perda) com par√¢metros de entrada dados, e pode ser enviado para computa√ß√£o na GPU, se dispon√≠vel. Existem fun√ß√µes para diferenciar esse gr√°fico computacional e calcular gradientes, que podem ent√£o ser usados para otimizar os par√¢metros do modelo.

**APIs de alto n√≠vel** consideram essencialmente redes neurais como uma **sequ√™ncia de camadas**, tornando a constru√ß√£o da maioria das redes neurais muito mais f√°cil. O treinamento do modelo geralmente requer a prepara√ß√£o dos dados e, em seguida, a chamada de uma fun√ß√£o `fit` para realizar a tarefa.

A API de alto n√≠vel permite que voc√™ construa redes neurais t√≠picas rapidamente, sem se preocupar com muitos detalhes. Ao mesmo tempo, a API de baixo n√≠vel oferece muito mais controle sobre o processo de treinamento, e, portanto, √© muito utilizada em pesquisas, quando voc√™ est√° lidando com novas arquiteturas de redes neurais.

√â tamb√©m importante entender que voc√™ pode usar ambas as APIs juntas, por exemplo, voc√™ pode desenvolver sua pr√≥pria arquitetura de camada de rede usando a API de baixo n√≠vel e, em seguida, us√°-la dentro da rede maior constru√≠da e treinada com a API de alto n√≠vel. Ou voc√™ pode definir uma rede usando a API de alto n√≠vel como uma sequ√™ncia de camadas e, em seguida, usar seu pr√≥prio loop de treinamento de baixo n√≠vel para realizar a otimiza√ß√£o. Ambas as APIs utilizam os mesmos conceitos b√°sicos subjacentes e s√£o projetadas para funcionar bem juntas.

## Aprendizado

Neste curso, oferecemos a maior parte do conte√∫do tanto para PyTorch quanto para TensorFlow. Voc√™ pode escolher seu framework preferido e apenas percorrer os notebooks correspondentes. Se voc√™ n√£o tiver certeza de qual framework escolher, leia algumas discuss√µes na internet sobre **PyTorch vs. TensorFlow**. Voc√™ tamb√©m pode dar uma olhada em ambos os frameworks para obter uma melhor compreens√£o.

Sempre que poss√≠vel, usaremos APIs de Alto N√≠vel por uma quest√£o de simplicidade. No entanto, acreditamos que √© importante entender como as redes neurais funcionam desde o in√≠cio, portanto, no come√ßo, come√ßamos trabalhando com a API de baixo n√≠vel e tensores. Contudo, se voc√™ quiser avan√ßar rapidamente e n√£o quiser passar muito tempo aprendendo esses detalhes, pode pular essas partes e ir direto para os notebooks da API de alto n√≠vel.

## ‚úçÔ∏è Exerc√≠cios: Frameworks

Continue seu aprendizado nos seguintes notebooks:

API de Baixo N√≠vel | [Notebook TensorFlow+Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
-------------------|-------------------------------------|--------------------------------
API de Alto N√≠vel  | [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Ap√≥s dominar os frameworks, vamos recapitular a no√ß√£o de sobreajuste.

# Sobreajuste

O sobreajuste √© um conceito extremamente importante em aprendizado de m√°quina, e √© crucial compreend√™-lo corretamente!

Considere o seguinte problema de aproximar 5 pontos (representados por `x` nos gr√°ficos abaixo):

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.it.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.it.jpg)
-------------------------|--------------------------
**Modelo linear, 2 par√¢metros** | **Modelo n√£o-linear, 7 par√¢metros**
Erro de treinamento = 5.3 | Erro de valida√ß√£o = 0
Erro de valida√ß√£o = 5.1 | Erro de valida√ß√£o = 20

* √Ä esquerda, vemos uma boa aproxima√ß√£o de linha reta. Como o n√∫mero de par√¢metros √© adequado, o modelo compreende bem a distribui√ß√£o dos pontos.
* √Ä direita, o modelo √© muito poderoso. Como temos apenas 5 pontos e o modelo possui 7 par√¢metros, ele pode se ajustar de tal forma a passar por todos os pontos, fazendo com que o erro de treinamento seja 0. No entanto, isso impede que o modelo entenda o padr√£o correto por tr√°s dos dados, resultando em um erro de valida√ß√£o muito alto.

√â muito importante encontrar um equil√≠brio adequado entre a complexidade do modelo (n√∫mero de par√¢metros) e o n√∫mero de amostras de treinamento.

## Por que o sobreajuste ocorre

  * Dados de treinamento insuficientes
  * Modelo excessivamente poderoso
  * Muito ru√≠do nos dados de entrada

## Como detectar o sobreajuste

Como voc√™ pode ver no gr√°fico acima, o sobreajuste pode ser detectado por um erro de treinamento muito baixo e um erro de valida√ß√£o alto. Normalmente, durante o treinamento, observamos ambos os erros de treinamento e valida√ß√£o come√ßando a diminuir, e ent√£o, em algum momento, o erro de valida√ß√£o pode parar de diminuir e come√ßar a aumentar. Isso ser√° um sinal de sobreajuste e um indicativo de que devemos provavelmente parar o treinamento neste ponto (ou pelo menos fazer uma captura do modelo).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.it.png)

## Como prevenir o sobreajuste

Se voc√™ perceber que o sobreajuste est√° ocorrendo, pode fazer uma das seguintes a√ß√µes:

 * Aumentar a quantidade de dados de treinamento
 * Diminuir a complexidade do modelo
 * Utilizar alguma [t√©cnica de regulariza√ß√£o](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), como [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), que consideraremos mais adiante.

## Sobreajuste e a Troca de Vi√©s-Vari√¢ncia

O sobreajuste √©, na verdade, um caso de um problema mais gen√©rico em estat√≠sticas chamado [Troca de Vi√©s-Vari√¢ncia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Se considerarmos as poss√≠veis fontes de erro em nosso modelo, podemos observar dois tipos de erros:

* **Erros de vi√©s** s√£o causados pelo fato de nosso algoritmo n√£o conseguir capturar corretamente a rela√ß√£o entre os dados de treinamento. Isso pode resultar do fato de nosso modelo n√£o ser poderoso o suficiente (**subajuste**).
* **Erros de vari√¢ncia**, que s√£o causados pelo modelo aproximar o ru√≠do nos dados de entrada em vez de uma rela√ß√£o significativa (**sobreajuste**).

Durante o treinamento, o erro de vi√©s diminui (√† medida que nosso modelo aprende a aproximar os dados), e o erro de vari√¢ncia aumenta. √â importante parar o treinamento - seja manualmente (quando detectamos sobreajuste) ou automaticamente (introduzindo regulariza√ß√£o) - para evitar o sobreajuste.

## Conclus√£o

Nesta li√ß√£o, voc√™ aprendeu sobre as diferen√ßas entre as v√°rias APIs dos dois frameworks de IA mais populares, TensorFlow e PyTorch. Al√©m disso, voc√™ aprendeu sobre um t√≥pico muito importante, o sobreajuste.

## üöÄ Desafio

Nos notebooks acompanhantes, voc√™ encontrar√° 'tarefas' na parte inferior; trabalhe pelos notebooks e complete as tarefas.

## [Question√°rio p√≥s-aula](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## Revis√£o e Autoestudo

Fa√ßa uma pesquisa sobre os seguintes t√≥picos:

- TensorFlow
- PyTorch
- Sobreajuste

Pergunte a si mesmo as seguintes quest√µes:

- Qual √© a diferen√ßa entre TensorFlow e PyTorch?
- Qual √© a diferen√ßa entre sobreajuste e subajuste?

## [Tarefa](lab/README.md)

Neste laborat√≥rio, voc√™ √© solicitado a resolver dois problemas de classifica√ß√£o usando redes totalmente conectadas de uma e v√°rias camadas, utilizando PyTorch ou TensorFlow.

* [Instru√ß√µes](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.