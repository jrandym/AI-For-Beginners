# Introducci√≥n a las Redes Neuronales. Perceptr√≥n Multicapa

En la secci√≥n anterior, aprendiste sobre el modelo de red neuronal m√°s simple: el perceptr√≥n de una sola capa, un modelo lineal de clasificaci√≥n binaria.

En esta secci√≥n, ampliaremos este modelo a un marco m√°s flexible, que nos permitir√°:

* realizar **clasificaci√≥n multiclase** adem√°s de la clasificaci√≥n binaria
* resolver **problemas de regresi√≥n** adem√°s de la clasificaci√≥n
* separar clases que no son linealmente separables

Tambi√©n desarrollaremos nuestro propio marco modular en Python que nos permitir√° construir diferentes arquitecturas de redes neuronales.

## [Cuestionario previo a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalizaci√≥n del Aprendizaje Autom√°tico

Comencemos formalizando el problema del Aprendizaje Autom√°tico. Supongamos que tenemos un conjunto de datos de entrenamiento **X** con etiquetas **Y**, y necesitamos construir un modelo *f* que realice las predicciones m√°s precisas. La calidad de las predicciones se mide mediante la **funci√≥n de p√©rdida** ‚Ñí. Las siguientes funciones de p√©rdida son las m√°s utilizadas:

* Para problemas de regresi√≥n, cuando necesitamos predecir un n√∫mero, podemos usar el **error absoluto** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, o el **error cuadr√°tico** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Para la clasificaci√≥n, usamos la **p√©rdida 0-1** (que es esencialmente lo mismo que la **precisi√≥n** del modelo), o la **p√©rdida log√≠stica**.

Para el perceptr√≥n de una sola capa, la funci√≥n *f* se defini√≥ como una funci√≥n lineal *f(x)=wx+b* (donde *w* es la matriz de pesos, *x* es el vector de caracter√≠sticas de entrada y *b* es el vector de sesgo). Para diferentes arquitecturas de redes neuronales, esta funci√≥n puede adoptar una forma m√°s compleja.

> En el caso de la clasificaci√≥n, a menudo es deseable obtener probabilidades de las clases correspondientes como salida de la red. Para convertir n√∫meros arbitrarios en probabilidades (por ejemplo, para normalizar la salida), a menudo usamos la funci√≥n **softmax** œÉ, y la funci√≥n *f* se convierte en *f(x)=œÉ(wx+b)*

En la definici√≥n de *f* anterior, *w* y *b* se denominan **par√°metros** Œ∏=‚ü®*w,b*‚ü©. Dado el conjunto de datos ‚ü®**X**,**Y**‚ü©, podemos calcular un error general en todo el conjunto de datos como una funci√≥n de los par√°metros Œ∏.

> ‚úÖ **El objetivo del entrenamiento de la red neuronal es minimizar el error variando los par√°metros Œ∏**

## Optimizaci√≥n por Descenso de Gradiente

Hay un m√©todo bien conocido de optimizaci√≥n de funciones llamado **descenso de gradiente**. La idea es que podemos calcular una derivada (en el caso multidimensional, llamada **gradiente**) de la funci√≥n de p√©rdida con respecto a los par√°metros, y variar los par√°metros de tal manera que el error disminuya. Esto se puede formalizar de la siguiente manera:

* Inicializa los par√°metros con algunos valores aleatorios w<sup>(0)</sup>, b<sup>(0)</sup>
* Repite el siguiente paso muchas veces:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

Durante el entrenamiento, se supone que los pasos de optimizaci√≥n se calculan considerando todo el conjunto de datos (recuerda que la p√©rdida se calcula como una suma a trav√©s de todas las muestras de entrenamiento). Sin embargo, en la pr√°ctica, tomamos peque√±as porciones del conjunto de datos llamadas **minibatches**, y calculamos gradientes basados en un subconjunto de datos. Dado que el subconjunto se toma aleatoriamente cada vez, este m√©todo se llama **descenso de gradiente estoc√°stico** (SGD).

## Perceptrones Multicapa y Retropropagaci√≥n

La red de una sola capa, como hemos visto anteriormente, es capaz de clasificar clases que son linealmente separables. Para construir un modelo m√°s rico, podemos combinar varias capas de la red. Matem√°ticamente, esto significar√≠a que la funci√≥n *f* tendr√≠a una forma m√°s compleja y se calcular√≠a en varios pasos:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

Aqu√≠, Œ± es una **funci√≥n de activaci√≥n no lineal**, œÉ es una funci√≥n softmax y los par√°metros Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*.

El algoritmo de descenso de gradiente seguir√≠a siendo el mismo, pero ser√≠a m√°s dif√≠cil calcular los gradientes. Dada la regla de diferenciaci√≥n en cadena, podemos calcular derivadas como:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ La regla de diferenciaci√≥n en cadena se utiliza para calcular las derivadas de la funci√≥n de p√©rdida con respecto a los par√°metros.

Ten en cuenta que la parte m√°s a la izquierda de todas esas expresiones es la misma, y as√≠ podemos calcular efectivamente las derivadas comenzando desde la funci√≥n de p√©rdida y yendo "hacia atr√°s" a trav√©s del gr√°fico computacional. As√≠, el m√©todo de entrenamiento de un perceptr√≥n multicapa se llama **retropropagaci√≥n**, o 'backprop'.

<img alt="gr√°fico computacional" src="images/ComputeGraphGrad.png"/>

> TODO: cita de imagen

> ‚úÖ Cubriremos la retropropagaci√≥n con mucho m√°s detalle en nuestro ejemplo de cuaderno.  

## Conclusi√≥n

En esta lecci√≥n, hemos construido nuestra propia biblioteca de redes neuronales y la hemos utilizado para una tarea simple de clasificaci√≥n bidimensional.

## üöÄ Desaf√≠o

En el cuaderno adjunto, implementar√°s tu propio marco para construir y entrenar perceptrones multicapa. Podr√°s ver en detalle c√≥mo operan las redes neuronales modernas.

Procede al cuaderno [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) y trabaja en √©l.

## [Cuestionario posterior a la clase](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## Revisi√≥n y Autoestudio

La retropropagaci√≥n es un algoritmo com√∫nmente utilizado en IA y ML, vale la pena estudiarlo [con m√°s detalle](https://wikipedia.org/wiki/Backpropagation)

## [Tarea](lab/README.md)

En este laboratorio, se te pide que utilices el marco que construiste en esta lecci√≥n para resolver la clasificaci√≥n de d√≠gitos manuscritos de MNIST.

* [Instrucciones](lab/README.md)
* [Cuaderno](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.