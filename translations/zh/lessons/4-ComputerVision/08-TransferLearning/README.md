# 预训练网络与迁移学习

训练卷积神经网络（CNN）可能需要大量时间，并且需要大量数据。然而，很多时间都花在学习网络可以用来从图像中提取模式的最佳低级滤波器上。一个自然的问题随之而来——我们能否使用在一个数据集上训练的神经网络，并将其调整为分类不同的图像，而无需完整的训练过程？

## [课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/108)

这种方法被称为**迁移学习**，因为我们将一些知识从一个神经网络模型转移到另一个模型。在迁移学习中，我们通常从一个预训练模型开始，该模型已经在一些大型图像数据集上进行训练，例如**ImageNet**。这些模型已经能够很好地提取通用图像中的不同特征，在许多情况下，仅在这些提取的特征上构建一个分类器就能获得良好的结果。

> ✅ 迁移学习是一个在其他学术领域（例如教育）中也会遇到的术语。它指的是将一个领域的知识应用到另一个领域的过程。

## 作为特征提取器的预训练模型

我们在前一部分中讨论的卷积网络包含多个层，每一层都旨在从图像中提取一些特征，从低级的像素组合（例如水平/垂直线或笔画）开始，到更高级的特征组合，对应于火焰的眼睛等。如果我们在足够大的通用和多样化图像数据集上训练CNN，网络应该能够学习提取这些常见特征。

Keras和PyTorch都包含函数，可以轻松加载一些常见架构的预训练神经网络权重，其中大多数是在ImageNet图像上训练的。最常用的模型在前一课的[卷积神经网络架构](../07-ConvNets/CNN_Architectures.md)页面中进行了描述。特别是，您可能想考虑使用以下模型之一：

* **VGG-16/VGG-19**，这些是相对简单的模型，但仍能提供良好的准确性。通常，使用VGG作为第一次尝试是一个不错的选择，以查看迁移学习的效果。
* **ResNet** 是微软研究在2015年提出的一系列模型。它们具有更多层，因此需要更多资源。
* **MobileNet** 是一系列尺寸较小的模型，适合移动设备。如果您资源有限，并且可以牺牲一点准确性，请使用它们。

以下是VGG-16网络从一张猫的图片中提取的样本特征：

![VGG-16提取的特征](../../../../../translated_images/features.6291f9c7ba3a0b951af88fc9864632b9115365410765680680d30c927dd67354.zh.png)

## 猫与狗数据集

在这个例子中，我们将使用一个[猫与狗](https://www.microsoft.com/download/details.aspx?id=54765&WT.mc_id=academic-77998-cacaste)的数据集，这与真实的图像分类场景非常接近。

## ✍️ 练习：迁移学习

让我们在相应的笔记本中看到迁移学习的实际应用：

* [迁移学习 - PyTorch](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb)
* [迁移学习 - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb)

## 可视化对抗性猫

预训练的神经网络在其*大脑*中包含不同的模式，包括**理想猫**（以及理想狗、理想斑马等）的概念。将这种图像以某种方式**可视化**将会很有趣。然而，这并不简单，因为模式分布在整个网络权重中，并且以层次结构组织。

我们可以采取的一种方法是从一张随机图像开始，然后尝试使用**梯度下降优化**技术调整该图像，使得网络开始认为它是一只猫。

![图像优化循环](../../../../../translated_images/ideal-cat-loop.999fbb8ff306e044f997032f4eef9152b453e6a990e449bbfb107de2493cc37e.zh.png)

然而，如果我们这样做，我们将得到与随机噪声非常相似的东西。这是因为*有很多方法可以让网络认为输入图像是一只猫*，其中一些在视觉上并不合理。虽然这些图像包含许多典型于猫的模式，但没有任何东西限制它们在视觉上具有独特性。

为了改善结果，我们可以在损失函数中添加另一个项，称为**变异损失**。它是一种度量，显示图像中相邻像素的相似程度。最小化变异损失使图像更平滑，并去除噪声——从而揭示出更具视觉吸引力的模式。以下是被高概率分类为猫和斑马的“理想”图像示例：

![理想猫](../../../../../translated_images/ideal-cat.203dd4597643d6b0bd73038b87f9c0464322725e3a06ab145d25d4a861c70592.zh.png) | ![理想斑马](../../../../../translated_images/ideal-zebra.7f70e8b54ee15a7a314000bb5df38a6cfe086ea04d60df4d3ef313d046b98a2b.zh.png)
-----|-----
 *理想猫* | *理想斑马*

类似的方法可以用于对神经网络进行所谓的**对抗攻击**。假设我们想要欺骗一个神经网络，让一只狗看起来像一只猫。如果我们拿到一张被网络识别为狗的图像，我们可以稍微调整它，使用梯度下降优化，直到网络开始将其分类为猫：

![一只狗的图片](../../../../../translated_images/original-dog.8f68a67d2fe0911f33041c0f7fce8aa4ea919f9d3917ec4b468298522aeb6356.zh.png) | ![被分类为猫的狗的图片](../../../../../translated_images/adversarial-dog.d9fc7773b0142b89752539bfbf884118de845b3851c5162146ea0b8809fc820f.zh.png)
-----|-----
*狗的原始图片* | *被分类为猫的狗的图片*

请参阅以下笔记本中的代码以重现上述结果：

* [理想与对抗性猫 - TensorFlow](../../../../../lessons/4-ComputerVision/08-TransferLearning/AdversarialCat_TF.ipynb)

## 结论

通过迁移学习，您能够快速为自定义对象分类任务构建分类器并获得高准确率。您可以看到，我们现在解决的更复杂的任务需要更高的计算能力，无法轻松在CPU上解决。在下一个单元中，我们将尝试使用更轻量的实现来训练相同的模型，使用较低的计算资源，这将导致准确率略微降低。

## 🚀 挑战

在随附的笔记本中，底部有关于如何在相对相似的训练数据（也许是新类型的动物）上迁移知识效果最佳的说明。请尝试一些完全新类型的图像，看看您的迁移知识模型表现如何。

## [课后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## 复习与自学

阅读[TrainingTricks.md](TrainingTricks.md)，以加深您对其他训练模型方法的了解。

## [作业](lab/README.md)

在这个实验中，我们将使用现实生活中的[Oxford-IIIT](https://www.robots.ox.ac.uk/~vgg/data/pets/)宠物数据集，包含35种猫和狗的品种，我们将构建一个迁移学习分类器。

**免责声明**：  
本文件使用基于机器的人工智能翻译服务进行翻译。虽然我们努力追求准确性，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应被视为权威来源。对于关键信息，建议进行专业人工翻译。我们对因使用此翻译而产生的任何误解或误释不承担责任。