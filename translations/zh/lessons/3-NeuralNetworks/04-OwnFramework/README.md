# 神经网络简介：多层感知器

在上一节中，您学习了最简单的神经网络模型——单层感知器，这是一种线性二分类模型。

在本节中，我们将扩展此模型，构建一个更灵活的框架，使我们能够：

* 除了二分类之外，执行**多类分类**
* 除了分类之外，解决**回归问题**
* 分离那些无法线性分离的类

我们还将开发自己的模块化框架，使用 Python 构建不同的神经网络架构。

## [课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## 机器学习的形式化

让我们开始形式化机器学习问题。假设我们有一个带标签的训练数据集 **X** 和 **Y**，我们需要构建一个模型 *f*，使其能够做出最准确的预测。预测的质量通过**损失函数** ℒ 来衡量。以下是常用的损失函数：

* 对于回归问题，当我们需要预测一个数字时，可以使用**绝对误差** ∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|，或**平方误差** ∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* 对于分类，我们使用**0-1损失**（这实际上与模型的**准确率**相同），或**逻辑损失**。

对于单层感知器，函数 *f* 被定义为线性函数 *f(x)=wx+b*（这里 *w* 是权重矩阵，*x* 是输入特征向量，*b* 是偏置向量）。对于不同的神经网络架构，这个函数可以呈现出更复杂的形式。

> 在分类的情况下，通常希望将相应类的概率作为网络输出。为了将任意数字转换为概率（例如，对输出进行归一化），我们通常使用**softmax**函数 σ，函数 *f* 变为 *f(x)=σ(wx+b)*。

在上面的 *f* 定义中，*w* 和 *b* 被称为**参数** θ=⟨*w,b*⟩。给定数据集 ⟨**X**,**Y**⟩，我们可以计算整个数据集的总体误差，作为参数 θ 的函数。

> ✅ **神经网络训练的目标是通过调整参数 θ 来最小化误差**

## 梯度下降优化

有一种众所周知的函数优化方法称为**梯度下降**。其思想是，我们可以计算损失函数对参数的导数（在多维情况下称为**梯度**），并以某种方式调整参数，使得误差减少。这可以形式化为：

* 以一些随机值初始化参数 w<sup>(0)</sup>，b<sup>(0)</sup>
* 重复以下步骤多次：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

在训练过程中，优化步骤应考虑整个数据集（请记住，损失是通过所有训练样本的总和计算的）。然而，在实际应用中，我们采用称为**小批量**的数据集，并基于数据的子集计算梯度。由于每次随机选择子集，这种方法称为**随机梯度下降**（SGD）。

## 多层感知器与反向传播

如上所述，单层网络能够对线性可分的类进行分类。为了构建更丰富的模型，我们可以组合多个网络层。在数学上，这意味着函数 *f* 将具有更复杂的形式，并将在多个步骤中计算：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

这里，α 是**非线性激活函数**，σ 是 softmax 函数，参数 θ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>* >。

梯度下降算法将保持不变，但计算梯度会更加困难。根据链式求导法则，我们可以计算导数如下：

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ 链式求导法则用于计算损失函数对参数的导数。

请注意，所有这些表达式的最左侧部分是相同的，因此我们可以有效地从损失函数开始，向“后面”通过计算图计算导数。因此，多层感知器的训练方法称为**反向传播**，或简称为“反向传播”。

<img alt="计算图" src="images/ComputeGraphGrad.png"/>

> TODO: 图片引用

> ✅ 我们将在我们的笔记本示例中更详细地介绍反向传播。

## 结论

在本课中，我们构建了自己的神经网络库，并将其用于简单的二维分类任务。

## 🚀 挑战

在随附的笔记本中，您将实现自己的框架，以构建和训练多层感知器。您将能够详细了解现代神经网络的运作。

请继续查看 [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) 笔记本并完成其中的内容。

## [课后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## 复习与自学

反向传播是一种常用的算法，在人工智能和机器学习中值得深入研究 [更详细](https://wikipedia.org/wiki/Backpropagation)。

## [作业](lab/README.md)

在这个实验中，您需要使用本课中构建的框架来解决 MNIST 手写数字分类问题。

* [说明](lab/README.md)
* [笔记本](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**免责声明**：  
本文件使用机器翻译的人工智能翻译服务进行翻译。虽然我们努力确保准确性，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们对因使用本翻译而产生的任何误解或误释不承担责任。