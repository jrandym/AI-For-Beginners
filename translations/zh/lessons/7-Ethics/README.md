# 伦理与负责任的人工智能

您几乎完成了本课程，我希望到现在为止，您已经清楚地认识到人工智能是基于一系列正式的数学方法，这些方法使我们能够在数据中找到关系，并训练模型以复制人类行为的某些方面。在历史的这一时刻，我们认为人工智能是一个非常强大的工具，可以从数据中提取模式，并将这些模式应用于解决新问题。

## [课前小测验](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/)

然而，在科幻作品中，我们经常看到人工智能对人类构成威胁的故事。通常这些故事围绕某种人工智能反叛展开，当人工智能决定与人类对抗时。这暗示了人工智能具有某种情感或能够做出开发者未曾预见的决策。

我们在本课程中学习到的人工智能不过是大型矩阵运算。它是一个非常强大的工具，帮助我们解决问题，正如其他任何强大的工具一样——它可以用于善，也可以用于恶。重要的是，它可能会被*误用*。

## 负责任的人工智能原则

为了避免这种意外或故意的人工智能误用，微软提出了重要的[负责任的人工智能原则](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-77998-cacaste)。以下概念支撑着这些原则：

* **公平性**与*模型偏见*的重要问题相关，这种偏见可能是由于使用了有偏见的数据进行训练。例如，当我们尝试预测一个人获得软件开发者职位的概率时，模型可能会对男性给予更高的偏好——仅仅因为训练数据集可能偏向于男性受众。我们需要仔细平衡训练数据，调查模型以避免偏见，并确保模型考虑到更相关的特征。
* **可靠性和安全性**。由于其本质，人工智能模型可能会犯错误。神经网络返回概率，我们在做决策时需要考虑这一点。每个模型都有一定的精确度和召回率，我们需要理解这一点，以防止错误建议可能造成的伤害。
* **隐私和安全**在某种程度上具有人工智能特有的含义。例如，当我们使用某些数据来训练模型时，这些数据在某种程度上会“融入”模型中。一方面，这增加了安全性和隐私性，另一方面，我们需要记住模型是基于哪些数据进行训练的。
* **包容性**意味着我们并不是在构建替代人类的人工智能，而是为了增强人类，使我们的工作更具创造性。这也与公平性有关，因为在处理代表性不足的社区时，我们收集的大多数数据集可能会有偏见，我们需要确保这些社区被包含并得到人工智能的正确处理。
* **透明性**。这包括确保我们始终明确使用的人工智能。此外，在可能的情况下，我们希望使用*可解释*的人工智能系统。
* **问责制**。当人工智能模型做出某些决策时，谁对这些决策负责并不总是明确的。我们需要确保了解人工智能决策的责任所在。在大多数情况下，我们希望将人类纳入重要决策的循环中，以便实际的人被追究责任。

## 负责任的人工智能工具

微软开发了[负责任的人工智能工具箱](https://github.com/microsoft/responsible-ai-toolbox)，其中包含一套工具：

* 可解释性仪表板 (InterpretML)
* 公平性仪表板 (FairLearn)
* 错误分析仪表板
* 负责任的人工智能仪表板，包括

   - EconML - 关注假设问题的因果分析工具
   - DiCE - 反事实分析工具，让您看到哪些特征需要更改以影响模型的决策

有关人工智能伦理的更多信息，请访问[本课程](https://github.com/microsoft/ML-For-Beginners/tree/main/1-Introduction/3-fairness?WT.mc_id=academic-77998-cacaste)，该课程包括作业。

## 复习与自学

参加这个[学习路径](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77998-cacaste)，以了解更多关于负责任的人工智能的信息。

## [课后小测验](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/)

**免责声明**：  
本文件使用机器翻译的人工智能翻译服务进行翻译。尽管我们努力确保准确性，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们对因使用此翻译而导致的任何误解或误释不承担责任。