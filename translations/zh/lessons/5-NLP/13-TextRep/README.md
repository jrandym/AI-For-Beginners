# 将文本表示为张量

## [课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## 文本分类

在本节的第一部分，我们将重点关注**文本分类**任务。我们将使用[AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset)数据集，该数据集包含以下新闻文章：

* 类别：科技
* 标题：肯塔基公司获得资助研究肽（美联社）
* 正文：美联社 - 一家由路易斯维尔大学的化学研究人员创办的公司获得了一项资助，用于开发...

我们的目标是根据文本将新闻项分类到一个类别中。

## 文本表示

如果我们想用神经网络解决自然语言处理（NLP）任务，我们需要某种方式将文本表示为张量。计算机已经将文本字符表示为映射到屏幕上字体的数字，使用如ASCII或UTF-8等编码。

<img alt="显示字符与ASCII和二进制表示之间映射的图像" src="images/ascii-character-map.png" width="50%"/>

> [图片来源](https://www.seobility.net/en/wiki/ASCII)

作为人类，我们理解每个字母**代表**的含义，以及所有字符如何结合形成句子的单词。然而，计算机本身并没有这样的理解，神经网络必须在训练过程中学习其含义。

因此，在表示文本时，我们可以使用不同的方法：

* **字符级表示**，即通过将每个字符视为一个数字来表示文本。假设我们的文本语料库中有*C*个不同的字符，单词*Hello*将由5x*C*的张量表示。每个字母将对应于独热编码中的一个张量列。
* **词级表示**，在这种表示中，我们创建一个文本中所有单词的**词汇表**，然后使用独热编码表示单词。这种方法在某种程度上更好，因为每个字母本身并没有太多含义，因此通过使用更高层次的语义概念——单词——我们简化了神经网络的任务。然而，考虑到字典的庞大，我们需要处理高维稀疏张量。

无论采用何种表示，我们首先需要将文本转换为一系列**标记**，一个标记可以是一个字符、一个单词，或者有时甚至是一个单词的一部分。然后，我们将标记转换为数字，通常使用**词汇表**，这个数字可以通过独热编码输入到神经网络中。

## N-grams

在自然语言中，单词的确切含义只能在上下文中确定。例如，*neural network*和*fishing network*的含义完全不同。考虑这一点的一个方法是基于单词对构建我们的模型，将单词对视为独立的词汇标记。这样，句子*I like to go fishing*将被表示为以下标记序列：*I like*, *like to*, *to go*, *go fishing*。这种方法的问题在于字典大小显著增长，像*go fishing*和*go shopping*这样的组合被表示为不同的标记，尽管它们共享相同的动词，但并没有任何语义相似性。

在某些情况下，我们还可以考虑使用三元组——三个单词的组合。因此，这种方法通常被称为**n-grams**。此外，使用字符级表示的n-grams也很有意义，在这种情况下，n-grams大致对应于不同的音节。

## 词袋模型和TF/IDF

在解决文本分类等任务时，我们需要能够用一个固定大小的向量表示文本，这个向量将用作最终密集分类器的输入。实现这一点的最简单方法之一是将所有单个单词表示结合起来，例如通过相加。如果我们将每个单词的独热编码相加，我们将得到一个频率向量，显示每个单词在文本中出现的次数。这种文本表示被称为**词袋模型**（BoW）。

<img src="images/bow.png" width="90%"/>

> 图片由作者提供

BoW本质上表示文本中出现的单词及其数量，这确实可以很好地指示文本的主题。例如，关于政治的新闻文章可能包含*president*和*country*等单词，而科学出版物则可能包含*collider*、*discovered*等。因此，单词频率在许多情况下可以很好地指示文本内容。

BoW的问题在于某些常见单词，如*and*、*is*等，出现在大多数文本中，并且它们的频率最高，掩盖了真正重要的单词。我们可以通过考虑单词在整个文档集合中出现的频率来降低这些单词的重要性。这就是TF/IDF方法背后的主要思想，详细内容将在本课程附带的笔记本中介绍。

然而，这些方法都无法完全考虑文本的**语义**。我们需要更强大的神经网络模型来做到这一点，稍后我们将在本节中讨论。

## ✍️ 练习：文本表示

在以下笔记本中继续学习：

* [使用PyTorch进行文本表示](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [使用TensorFlow进行文本表示](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## 结论

到目前为止，我们研究了可以为不同单词添加频率权重的技术。然而，它们无法表示含义或顺序。正如著名语言学家J. R. Firth在1935年所说：“一个词的完整含义总是与上下文相关，任何与上下文无关的意义研究都无法被认真对待。”我们将在课程的后面学习如何通过语言建模从文本中捕捉上下文信息。

## 🚀 挑战

尝试使用词袋模型和不同数据模型进行其他练习。您可能会受到这个[在Kaggle上的比赛](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)的启发。

## [课后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## 复习与自学

在[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)上练习您的文本嵌入和词袋技术。

## [作业：笔记本](assignment.md)

**免责声明**：
本文件使用基于机器的人工智能翻译服务进行翻译。虽然我们努力追求准确性，但请注意，自动翻译可能包含错误或不准确之处。原文件的母语版本应被视为权威来源。对于关键信息，建议进行专业人工翻译。我们对因使用此翻译而导致的任何误解或误读不承担责任。