# 循环神经网络

## [课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

在之前的章节中，我们使用了丰富的文本语义表示，并在嵌入的基础上使用了简单的线性分类器。这种架构的作用是捕捉句子中单词的聚合意义，但它没有考虑单词的**顺序**，因为在嵌入之上的聚合操作移除了原始文本中的这一信息。由于这些模型无法建模单词的顺序，因此它们无法解决更复杂或模糊的任务，如文本生成或问答。

为了捕捉文本序列的意义，我们需要使用另一种神经网络架构，称为**循环神经网络**（RNN）。在RNN中，我们一次将句子中的一个符号传递给网络，网络生成一些**状态**，然后我们将其与下一个符号一起再次传递给网络。

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.zh.png)

> 图片由作者提供

给定输入序列的标记 X<sub>0</sub>,...,X<sub>n</sub>，RNN创建一系列神经网络块，并使用反向传播对该序列进行端到端训练。每个网络块接受一对 (X<sub>i</sub>,S<sub>i</sub>) 作为输入，并生成 S<sub>i+1</sub> 作为结果。最终状态 S<sub>n</sub> 或（输出 Y<sub>n</sub>）进入线性分类器以生成结果。所有网络块共享相同的权重，并通过一次反向传播传递进行端到端训练。

由于状态向量 S<sub>0</sub>,...,S<sub>n</sub> 被传递通过网络，因此它能够学习单词之间的顺序依赖关系。例如，当单词 *not* 在序列中出现时，它可以学习在状态向量中否定某些元素，从而实现否定。

> ✅ 由于上面图片中所有 RNN 块的权重是共享的，因此可以将同一图片表示为一个块（右侧），并带有一个递归反馈环，将网络的输出状态传回输入。

## RNN 单元的结构

让我们看看一个简单的 RNN 单元是如何组织的。它接受前一个状态 S<sub>i-1</sub> 和当前符号 X<sub>i</sub> 作为输入，并必须生成输出状态 S<sub>i</sub>（有时我们也对一些其他输出 Y<sub>i</sub> 感兴趣，例如在生成网络的情况下）。

一个简单的 RNN 单元内部有两个权重矩阵：一个用于转换输入符号（我们称之为 W），另一个用于转换输入状态（H）。在这种情况下，网络的输出计算为 σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b)，其中 σ 是激活函数，b 是附加偏置。

<img alt="RNN 单元结构" src="images/rnn-anatomy.png" width="50%"/>

> 图片由作者提供

在许多情况下，输入标记在进入 RNN 之前会通过嵌入层以降低维度。在这种情况下，如果输入向量的维度为 *emb_size*，状态向量为 *hid_size*，则 W 的大小为 *emb_size*×*hid_size*，H 的大小为 *hid_size*×*hid_size*。

## 长短期记忆（LSTM）

经典 RNN 的主要问题之一是所谓的**消失梯度**问题。由于 RNN 是在一次反向传播中端到端训练的，因此它在将误差传播到网络的第一层时遇到困难，因此网络无法学习远距离标记之间的关系。避免这个问题的一种方法是通过使用所谓的**门**引入**显式状态管理**。这种类型有两个著名的架构：**长短期记忆**（LSTM）和**门控循环单元**（GRU）。

![展示长短期记忆单元的图片](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> 图片来源待定

LSTM 网络的组织方式类似于 RNN，但有两个状态在层与层之间传递：实际状态 C 和隐藏向量 H。在每个单元中，隐藏向量 H<sub>i</sub> 与输入 X<sub>i</sub> 连接在一起，并通过**门**控制状态 C 的变化。每个门都是一个具有 sigmoid 激活的神经网络（输出范围为 [0,1]），在与状态向量相乘时可以被视为按位掩码。上面图片中的门如下（从左到右）：

* **遗忘门**接收隐藏向量并确定我们需要忘记向量 C 的哪些分量，以及哪些分量需要保留。
* **输入门**从输入和隐藏向量中提取一些信息并将其插入状态中。
* **输出门**通过带有 *tanh* 激活的线性层转换状态，然后使用隐藏向量 H<sub>i</sub> 选择其某些分量以生成新状态 C<sub>i+1</sub>。

状态 C 的分量可以视为一些可以打开和关闭的标志。例如，当我们在序列中遇到名字 *Alice* 时，我们可能希望假设它指的是一个女性角色，并在状态中提高一个标志，表示我们在句子中有一个女性名词。当我们进一步遇到短语 *and Tom* 时，我们将提高一个标志，表示我们有一个复数名词。因此，通过操控状态，我们可以追踪句子部分的语法属性。

> ✅ 理解 LSTM 内部机制的一个优秀资源是 Christopher Olah 的这篇精彩文章 [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。

## 双向和多层 RNN

我们讨论了单向操作的循环网络，从序列的开始到结束。这看起来很自然，因为它类似于我们阅读和听取语言的方式。然而，由于在许多实际情况下我们可以随机访问输入序列，因此在两个方向上运行循环计算可能是有意义的。这种网络称为**双向** RNN。在处理双向网络时，我们需要两个隐藏状态向量，一个用于每个方向。

循环网络，无论是单向还是双向，都捕捉序列中的某些模式，并可以将其存储到状态向量中或传递到输出中。与卷积网络一样，我们可以在第一个网络之上构建另一个循环层，以捕捉更高层次的模式，并从第一个层提取的低层模式中构建。这引导我们到**多层 RNN**的概念，它由两个或更多的循环网络组成，其中前一层的输出作为输入传递给下一层。

![展示多层长短期记忆 RNN 的图片](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.zh.jpg)

*图片来源于 [这篇精彩的文章](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) by Fernando López*

## ✍️ 练习：嵌入

继续在以下笔记本中学习：

* [使用 PyTorch 的 RNN](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [使用 TensorFlow 的 RNN](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## 结论

在本单元中，我们看到 RNN 可用于序列分类，但实际上它们可以处理更多任务，如文本生成、机器翻译等。我们将在下一个单元中考虑这些任务。

## 🚀 挑战

阅读一些关于 LSTM 的文献，并考虑它们的应用：

- [网格长短期记忆](https://arxiv.org/pdf/1507.01526v1.pdf)
- [展示、关注与讲述：具有视觉注意的神经图像字幕生成](https://arxiv.org/pdf/1502.03044v2.pdf)

## [课后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## 复习与自学

- [理解 LSTM 网络](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.

## [作业：笔记本](assignment.md)

**免责声明**：  
本文件是使用机器翻译的人工智能翻译服务进行翻译的。虽然我们努力追求准确性，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应视为权威来源。对于关键信息，建议进行专业的人类翻译。我们对因使用本翻译而导致的任何误解或误释不承担责任。