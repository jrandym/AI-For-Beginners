# Generative Netzwerke

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

Recurrent Neural Networks (RNNs) und ihre gated Cell-Varianten wie Long Short Term Memory Cells (LSTMs) und Gated Recurrent Units (GRUs) bieten einen Mechanismus f√ºr das Sprachmodellieren, da sie die Wortreihenfolge lernen und Vorhersagen f√ºr das n√§chste Wort in einer Sequenz machen k√∂nnen. Dies erm√∂glicht es uns, RNNs f√ºr **generative Aufgaben** zu verwenden, wie zum Beispiel die gew√∂hnliche Textgenerierung, maschinelle √úbersetzung und sogar Bildbeschriftung.

> ‚úÖ Denk dar√ºber nach, wie oft du von generativen Aufgaben wie der Textvervollst√§ndigung beim Tippen profitiert hast. Recherchiere deine Lieblingsanwendungen, um zu sehen, ob sie RNNs genutzt haben.

In der RNN-Architektur, die wir in der vorherigen Einheit besprochen haben, erzeugte jede RNN-Einheit den n√§chsten verborgenen Zustand als Ausgabe. Wir k√∂nnen jedoch auch eine weitere Ausgabe zu jeder rekurrenten Einheit hinzuf√ºgen, die es uns erm√∂glichen w√ºrde, eine **Sequenz** auszugeben (die in der L√§nge der urspr√ºnglichen Sequenz entspricht). Dar√ºber hinaus k√∂nnen wir RNN-Einheiten verwenden, die nicht bei jedem Schritt eine Eingabe akzeptieren, sondern einfach einen anf√§nglichen Zustandsvektor nehmen und dann eine Sequenz von Ausgaben erzeugen.

Dies erm√∂glicht verschiedene neuronale Architekturen, die im Bild unten gezeigt sind:

![Bild, das g√§ngige Muster rekurrenter neuronaler Netzwerke zeigt.](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.de.jpg)

> Bild aus dem Blogbeitrag [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) von [Andrej Karpaty](http://karpathy.github.io/)

* **Eins-zu-eins** ist ein traditionelles neuronales Netzwerk mit einer Eingabe und einer Ausgabe
* **Eins-zu-viele** ist eine generative Architektur, die einen Eingabewert akzeptiert und eine Sequenz von Ausgabewerten erzeugt. Wenn wir beispielsweise ein **Bildbeschriftungs**-Netzwerk trainieren m√∂chten, das eine textuelle Beschreibung eines Bildes erzeugt, k√∂nnen wir ein Bild als Eingabe nehmen, es durch ein CNN leiten, um seinen verborgenen Zustand zu erhalten, und dann eine rekursive Kette verwenden, um die Beschriftung Wort f√ºr Wort zu generieren
* **Viele-zu-eins** entspricht den RNN-Architekturen, die wir in der vorherigen Einheit beschrieben haben, wie z.B. der Textklassifikation
* **Viele-zu-viele**, oder **Sequenz-zu-Sequenz**, entspricht Aufgaben wie **maschineller √úbersetzung**, bei denen wir zuerst ein RNN haben, das alle Informationen aus der Eingabesequenz in den verborgenen Zustand aufnimmt, und eine andere RNN-Kette diesen Zustand in die Ausgabesequenz entrollt.

In dieser Einheit werden wir uns auf einfache generative Modelle konzentrieren, die uns helfen, Text zu generieren. Zur Vereinfachung verwenden wir die Zeichenebene Tokenisierung.

Wir werden dieses RNN trainieren, um Text Schritt f√ºr Schritt zu generieren. Bei jedem Schritt nehmen wir eine Sequenz von Zeichen der L√§nge `nchars` und bitten das Netzwerk, das n√§chste Ausgabesymbol f√ºr jedes Eingabesymbol zu generieren:

![Bild, das ein Beispiel f√ºr die RNN-Generierung des Wortes 'HELLO' zeigt.](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.de.png)

Beim Generieren von Text (w√§hrend der Inferenz) beginnen wir mit einem **Prompt**, der durch RNN-Zellen geleitet wird, um seinen Zwischenzustand zu erzeugen, und dann beginnt die Generierung von diesem Zustand. Wir generieren ein Zeichen nach dem anderen und √ºbergeben den Zustand und das generierte Zeichen an eine andere RNN-Zelle, um das n√§chste zu generieren, bis wir gen√ºgend Zeichen generiert haben.

<img src="images/rnn-generate-inf.png" width="60%"/>

> Bild vom Autor

## ‚úçÔ∏è √úbungen: Generative Netzwerke

Setze dein Lernen in den folgenden Notebooks fort:

* [Generative Netzwerke mit PyTorch](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [Generative Netzwerke mit TensorFlow](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## Sanfte Textgenerierung und Temperatur

Die Ausgabe jeder RNN-Zelle ist eine Wahrscheinlichkeitsverteilung von Zeichen. Wenn wir immer das Zeichen mit der h√∂chsten Wahrscheinlichkeit als n√§chstes Zeichen im generierten Text nehmen, kann der Text oft zwischen denselben Zeichenfolgen "zyklisch" werden, wie in diesem Beispiel:

```
today of the second the company and a second the company ...
```

Wenn wir jedoch die Wahrscheinlichkeitsverteilung f√ºr das n√§chste Zeichen betrachten, k√∂nnte es sein, dass der Unterschied zwischen den h√∂chsten Wahrscheinlichkeiten nicht gro√ü ist, z.B. k√∂nnte ein Zeichen eine Wahrscheinlichkeit von 0.2 haben, ein anderes - 0.19, usw. Wenn wir nach dem n√§chsten Zeichen in der Sequenz '*play*' suchen, k√∂nnte das n√§chste Zeichen ebenso gut ein Leerzeichen oder **e** (wie im Wort *player*) sein.

Dies f√ºhrt uns zu dem Schluss, dass es nicht immer "fair" ist, das Zeichen mit der h√∂heren Wahrscheinlichkeit auszuw√§hlen, da die Wahl des zweith√∂chsten uns immer noch zu bedeutungsvollem Text f√ºhren k√∂nnte. Es ist kl√ºger, Zeichen aus der Wahrscheinlichkeitsverteilung zu **sample**n, die durch die Netzwerkausgabe gegeben wird. Wir k√∂nnen auch einen Parameter, **Temperatur**, verwenden, der die Wahrscheinlichkeitsverteilung abflacht, falls wir mehr Zuf√§lligkeit hinzuf√ºgen m√∂chten, oder sie steiler machen, wenn wir uns mehr an den Zeichen mit der h√∂chsten Wahrscheinlichkeit orientieren m√∂chten.

Erforsche, wie diese sanfte Textgenerierung in den oben verlinkten Notebooks implementiert ist.

## Fazit

Obwohl die Textgenerierung an sich n√ºtzlich sein kann, kommen die gr√∂√üten Vorteile von der F√§higkeit, Text mithilfe von RNNs aus einem anf√§nglichen Merkmalsvektor zu generieren. Zum Beispiel wird die Textgenerierung als Teil der maschinellen √úbersetzung verwendet (Sequenz-zu-Sequenz, in diesem Fall wird der Zustandsvektor aus dem *Encoder* verwendet, um die √ºbersetzte Nachricht zu generieren oder *zu dekodieren*), oder um eine textuelle Beschreibung eines Bildes zu erzeugen (in diesem Fall w√ºrde der Merkmalsvektor aus dem CNN-Extractor stammen).

## üöÄ Herausforderung

Nehmt an einigen Lektionen auf Microsoft Learn zu diesem Thema teil

* Textgenerierung mit [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [Nachlesequiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## √úberpr√ºfung & Selbststudium

Hier sind einige Artikel, um dein Wissen zu erweitern

* Verschiedene Ans√§tze zur Textgenerierung mit Markov-Kette, LSTM und GPT-2: [Blogbeitrag](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* Textgenerierungsbeispiel in der [Keras-Dokumentation](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [Aufgabe](lab/README.md)

Wir haben gesehen, wie man Text Zeichen f√ºr Zeichen generiert. Im Labor wirst du die Textgenerierung auf Wortebene erkunden.

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als die ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.