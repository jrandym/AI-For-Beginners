# Rekurrente Neuronale Netze

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

In den vorherigen Abschnitten haben wir reichhaltige semantische Darstellungen von Texten und einen einfachen linearen Klassifikator auf den Embeddings verwendet. Was diese Architektur tut, ist, die aggregierte Bedeutung von W√∂rtern in einem Satz zu erfassen, jedoch ber√ºcksichtigt sie nicht die **Reihenfolge** der W√∂rter, da die Aggregationsoperation √ºber den Embeddings diese Information aus dem urspr√ºnglichen Text entfernt hat. Da diese Modelle nicht in der Lage sind, die Wortreihenfolge zu modellieren, k√∂nnen sie komplexere oder mehrdeutige Aufgaben wie Textgenerierung oder Fragenbeantwortung nicht l√∂sen.

Um die Bedeutung einer Textsequenz zu erfassen, m√ºssen wir eine andere Architektur neuronaler Netze verwenden, die als **rekurrentes neuronales Netz** oder RNN bezeichnet wird. Im RNN leiten wir unseren Satz symbolweise durch das Netzwerk, und das Netzwerk erzeugt einen **Zustand**, den wir dann erneut mit dem n√§chsten Symbol an das Netzwerk √ºbergeben.

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.de.png)

> Bild vom Autor

Gegeben die Eingabesequenz von Token X<sub>0</sub>,...,X<sub>n</sub> erstellt das RNN eine Sequenz von neuronalen Netzwerkbl√∂cken und trainiert diese Sequenz End-to-End mithilfe von R√ºckpropagation. Jeder Netzwerkblock nimmt ein Paar (X<sub>i</sub>,S<sub>i</sub>) als Eingabe und produziert S<sub>i+1</sub> als Ergebnis. Der endg√ºltige Zustand S<sub>n</sub> oder (Ausgabe Y<sub>n</sub>) geht in einen linearen Klassifikator, um das Ergebnis zu produzieren. Alle Netzwerkbl√∂cke teilen sich die gleichen Gewichte und werden End-to-End mit einem R√ºckpropagationsdurchlauf trainiert.

Da Zustandsvektoren S<sub>0</sub>,...,S<sub>n</sub> durch das Netzwerk geleitet werden, ist es in der Lage, die sequentiellen Abh√§ngigkeiten zwischen W√∂rtern zu lernen. Zum Beispiel, wenn das Wort *not* irgendwo in der Sequenz erscheint, kann es lernen, bestimmte Elemente innerhalb des Zustandsvektors zu negieren, was zu einer Negation f√ºhrt.

> ‚úÖ Da die Gewichte aller RNN-Bl√∂cke im obigen Bild geteilt werden, kann dasselbe Bild als ein Block (rechts) mit einer rekurrenten R√ºckkopplungsschleife dargestellt werden, die den Ausgangszustand des Netzwerks zur√ºck an den Eingang √ºbergibt.

## Anatomie einer RNN-Zelle

Schauen wir uns an, wie eine einfache RNN-Zelle organisiert ist. Sie akzeptiert den vorherigen Zustand S<sub>i-1</sub> und das aktuelle Symbol X<sub>i</sub> als Eingaben und muss den Ausgangszustand S<sub>i</sub> produzieren (und manchmal interessieren wir uns auch f√ºr eine andere Ausgabe Y<sub>i</sub>, wie im Fall von generativen Netzwerken).

Eine einfache RNN-Zelle hat zwei Gewichtsmatrizen: eine transformiert ein Eingabesymbol (nennen wir sie W) und eine andere transformiert einen Eingabestatus (H). In diesem Fall wird der Ausgang des Netzwerks als œÉ(W√óX<sub>i</sub>+H√óS<sub>i-1</sub>+b) berechnet, wobei œÉ die Aktivierungsfunktion ist und b der zus√§tzliche Bias.

<img alt="Anatomie einer RNN-Zelle" src="images/rnn-anatomy.png" width="50%"/>

> Bild vom Autor

In vielen F√§llen werden Eingabetoken vor dem Eintritt in das RNN durch die Embedding-Schicht geleitet, um die Dimensionalit√§t zu reduzieren. In diesem Fall, wenn die Dimension der Eingangsvektoren *emb_size* betr√§gt und der Zustandsvektor *hid_size* - die Gr√∂√üe von W ist *emb_size*√ó*hid_size* und die Gr√∂√üe von H ist *hid_size*√ó*hid_size*.

## Langzeit-Kurzzeitged√§chtnis (LSTM)

Eines der Hauptprobleme klassischer RNNs ist das sogenannte **verschwinden der Gradienten**-Problem. Da RNNs End-to-End in einem R√ºckpropagationsdurchlauf trainiert werden, hat es Schwierigkeiten, den Fehler auf die ersten Schichten des Netzwerks zu propagieren, und daher kann das Netzwerk keine Beziehungen zwischen weit entfernten Token lernen. Eine M√∂glichkeit, dieses Problem zu vermeiden, besteht darin, eine **explizite Zustandsverwaltung** durch die Verwendung sogenannter **Tore** einzuf√ºhren. Es gibt zwei bekannte Architekturen dieser Art: **Langzeit-Kurzzeitged√§chtnis** (LSTM) und **Gated Relay Unit** (GRU).

![Bild, das ein Beispiel f√ºr eine Langzeit-Kurzzeitged√§chtniszelle zeigt](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> Bildquelle TBD

Das LSTM-Netzwerk ist √§hnlich wie das RNN organisiert, aber es gibt zwei Zust√§nde, die von Schicht zu Schicht √ºbergeben werden: den aktuellen Zustand C und den versteckten Vektor H. An jeder Einheit wird der versteckte Vektor H<sub>i</sub> mit dem Eingabevektor X<sub>i</sub> verkn√ºpft, und sie steuern, was mit dem Zustand C √ºber die **Tore** geschieht. Jedes Tor ist ein neuronales Netzwerk mit sigmoidaler Aktivierung (Ausgabe im Bereich [0,1]), das als bitweiser Maske betrachtet werden kann, wenn es mit dem Zustandsvektor multipliziert wird. Es gibt die folgenden Tore (von links nach rechts im obigen Bild):

* Das **Vergessenstor** nimmt einen versteckten Vektor und bestimmt, welche Komponenten des Vektors C wir vergessen m√ºssen und welche durchgelassen werden sollen.
* Das **Eingangstor** nimmt einige Informationen aus den Eingabe- und versteckten Vektoren und f√ºgt sie in den Zustand ein.
* Das **Ausgangstor** transformiert den Zustand √ºber eine lineare Schicht mit *tanh*-Aktivierung und w√§hlt dann einige seiner Komponenten unter Verwendung eines versteckten Vektors H<sub>i</sub> aus, um einen neuen Zustand C<sub>i+1</sub> zu erzeugen.

Die Komponenten des Zustands C k√∂nnen als einige Flags betrachtet werden, die ein- und ausgeschaltet werden k√∂nnen. Wenn wir zum Beispiel im Verlauf der Sequenz auf einen Namen wie *Alice* sto√üen, m√∂chten wir vielleicht annehmen, dass es sich um eine weibliche Figur handelt, und das Flag im Zustand erh√∂hen, dass wir ein weibliches Substantiv im Satz haben. Wenn wir dann auf die Phrasen *und Tom* sto√üen, heben wir das Flag, dass wir ein Pluralnoun haben. So k√∂nnen wir durch die Manipulation des Zustands angeblich die grammatikalischen Eigenschaften von Satzteilen im Auge behalten.

> ‚úÖ Eine hervorragende Ressource zum Verst√§ndnis der Interna von LSTM ist dieser gro√üartige Artikel [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) von Christopher Olah.

## Bidirektionale und mehrschichtige RNNs

Wir haben rekurrente Netze besprochen, die in eine Richtung arbeiten, vom Anfang einer Sequenz bis zum Ende. Das wirkt nat√ºrlich, da es der Art und Weise √§hnelt, wie wir lesen und Sprache h√∂ren. Da wir jedoch in vielen praktischen F√§llen zuf√§lligen Zugriff auf die Eingabesequenz haben, kann es sinnvoll sein, die rekursive Berechnung in beide Richtungen auszuf√ºhren. Solche Netzwerke werden **bidirektionale** RNNs genannt. Bei der Arbeit mit einem bidirektionalen Netzwerk ben√∂tigen wir zwei versteckte Zustandsvektoren, einen f√ºr jede Richtung.

Ein rekurrentes Netzwerk, ob einseitig oder bidirektional, erfasst bestimmte Muster innerhalb einer Sequenz und kann sie in einem Zustandsvektor speichern oder in die Ausgabe √ºbergeben. Wie bei konvolutionalen Netzwerken k√∂nnen wir eine weitere rekursive Schicht √ºber der ersten aufbauen, um h√∂here Muster zu erfassen und von den von der ersten Schicht extrahierten niederwertigen Mustern aufzubauen. Dies f√ºhrt uns zum Begriff eines **mehrschichtigen RNN**, das aus zwei oder mehr rekurrenten Netzwerken besteht, wobei die Ausgabe der vorherigen Schicht als Eingabe an die n√§chste Schicht √ºbergeben wird.

![Bild, das ein mehrschichtiges Langzeit-Kurzzeitged√§chtnis-RNN zeigt](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.de.jpg)

*Bild von [diesem wunderbaren Beitrag](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) von Fernando L√≥pez*

## ‚úçÔ∏è √úbungen: Embeddings

Setze dein Lernen in den folgenden Notebooks fort:

* [RNNs mit PyTorch](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [RNNs mit TensorFlow](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## Fazit

In dieser Einheit haben wir gesehen, dass RNNs f√ºr die Sequenzklassifizierung verwendet werden k√∂nnen, aber tats√§chlich k√∂nnen sie viele weitere Aufgaben bew√§ltigen, wie Textgenerierung, maschinelle √úbersetzung und mehr. Diese Aufgaben werden wir in der n√§chsten Einheit betrachten.

## üöÄ Herausforderung

Lies einige Literatur √ºber LSTMs und ziehe deren Anwendungen in Betracht:

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Nachlesequiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## √úberpr√ºfung & Selbststudium

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) von Christopher Olah.

## [Aufgabe: Notebooks](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, sollten Sie sich bewusst sein, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner Originalsprache sollte als die ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.