# Text als Tensoren darstellen

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## Textklassifikation

Im ersten Teil dieses Abschnitts konzentrieren wir uns auf die Aufgabe der **Textklassifikation**. Wir verwenden den [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) Datensatz, der Nachrichtenartikel wie die folgenden enth√§lt:

* Kategorie: Sci/Tech
* Titel: Ky. Unternehmen gewinnt Zuschuss zur Untersuchung von Peptiden (AP)
* Text: AP - Ein Unternehmen, das von einem Chemieforscher an der Universit√§t von Louisville gegr√ºndet wurde, hat einen Zuschuss erhalten, um...

Unser Ziel wird es sein, den Nachrichtenartikel basierend auf dem Text in eine der Kategorien einzuordnen.

## Text darstellen

Wenn wir Aufgaben der Verarbeitung nat√ºrlicher Sprache (NLP) mit neuronalen Netzwerken l√∂sen wollen, ben√∂tigen wir eine M√∂glichkeit, Text als Tensoren darzustellen. Computer stellen bereits textuelle Zeichen als Zahlen dar, die auf Schriftarten auf Ihrem Bildschirm mithilfe von Kodierungen wie ASCII oder UTF-8 abgebildet werden.

<img alt="Bild, das ein Diagramm zeigt, das ein Zeichen einer ASCII- und bin√§ren Darstellung zuordnet" src="images/ascii-character-map.png" width="50%"/>

> [Bildquelle](https://www.seobility.net/en/wiki/ASCII)

Als Menschen verstehen wir, was jeder Buchstabe **darstellt**, und wie alle Zeichen zusammenkommen, um die W√∂rter eines Satzes zu bilden. Computer hingegen haben ein solches Verst√§ndnis nicht, und neuronale Netzwerke m√ºssen die Bedeutung w√§hrend des Trainings lernen.

Daher k√∂nnen wir verschiedene Ans√§tze zur Darstellung von Text verwenden:

* **Zeichenebene Darstellung**, bei der wir Text darstellen, indem wir jedes Zeichen als eine Zahl behandeln. Angenommen, wir haben *C* verschiedene Zeichen in unserem Textkorpus, w√ºrde das Wort *Hallo* durch einen 5x*C* Tensor dargestellt. Jeder Buchstabe w√ºrde einer Tensor-Spalte in der One-Hot-Kodierung entsprechen.
* **Wortebene Darstellung**, bei der wir ein **W√∂rterbuch** aller W√∂rter in unserem Text erstellen und dann W√∂rter mithilfe von One-Hot-Kodierung darstellen. Dieser Ansatz ist in gewisser Weise besser, da jeder Buchstabe f√ºr sich genommen nicht viel Bedeutung hat, und indem wir somit h√∂herwertige semantische Konzepte - W√∂rter - verwenden, vereinfachen wir die Aufgabe f√ºr das neuronale Netzwerk. Aufgrund der gro√üen W√∂rterbuchgr√∂√üe m√ºssen wir jedoch mit hochdimensionalen sp√§rlichen Tensoren umgehen.

Unabh√§ngig von der Darstellung m√ºssen wir den Text zun√§chst in eine Sequenz von **Tokens** umwandeln, wobei ein Token entweder ein Zeichen, ein Wort oder manchmal sogar ein Teil eines Wortes ist. Dann wandeln wir das Token in eine Zahl um, typischerweise unter Verwendung eines **W√∂rterbuchs**, und diese Zahl kann √ºber One-Hot-Kodierung in ein neuronales Netzwerk eingespeist werden.

## N-Gramme

In der nat√ºrlichen Sprache kann die genaue Bedeutung von W√∂rtern nur im Kontext bestimmt werden. Zum Beispiel sind die Bedeutungen von *neuronales Netzwerk* und *Fischernetz* v√∂llig unterschiedlich. Eine M√∂glichkeit, dies zu ber√ºcksichtigen, besteht darin, unser Modell auf Wortpaaren aufzubauen und Wortpaare als separate W√∂rterbuch-Token zu betrachten. Auf diese Weise wird der Satz *Ich gehe gerne angeln* durch die folgende Token-Sequenz dargestellt: *Ich gehe*, *gehe gerne*, *gerne angeln*. Das Problem mit diesem Ansatz ist, dass die W√∂rterbuchgr√∂√üe erheblich w√§chst und Kombinationen wie *gehe angeln* und *gehe einkaufen* durch unterschiedliche Tokens dargestellt werden, die trotz des gleichen Verbs keine semantische √Ñhnlichkeit aufweisen.

In einigen F√§llen k√∂nnen wir auch Tri-Gramme - Kombinationen von drei W√∂rtern - in Betracht ziehen. Daher wird dieser Ansatz oft als **n-Gramme** bezeichnet. Es macht auch Sinn, n-Gramme mit der Zeichenebene Darstellung zu verwenden, wobei n-Gramme grob unterschiedlichen Silben entsprechen.

## Bag-of-Words und TF/IDF

Bei der L√∂sung von Aufgaben wie der Textklassifikation m√ºssen wir in der Lage sein, Text durch einen festgelegten Vektor fester Gr√∂√üe darzustellen, den wir als Eingabe f√ºr den abschlie√üenden dichten Klassifizierer verwenden. Eine der einfachsten M√∂glichkeiten, dies zu tun, besteht darin, alle individuellen Wortdarstellungen zu kombinieren, z.B. indem wir sie addieren. Wenn wir die One-Hot-Kodierungen jedes Wortes addieren, erhalten wir einen Vektor von Frequenzen, der zeigt, wie oft jedes Wort im Text erscheint. Eine solche Textdarstellung wird als **Bag of Words** (BoW) bezeichnet.

<img src="images/bow.png" width="90%"/>

> Bild vom Autor

Ein BoW stellt im Wesentlichen dar, welche W√∂rter im Text erscheinen und in welchen Mengen, was tats√§chlich ein guter Hinweis darauf sein kann, worum es im Text geht. Zum Beispiel enth√§lt ein Nachrichtenartikel √ºber Politik wahrscheinlich W√∂rter wie *Pr√§sident* und *Land*, w√§hrend eine wissenschaftliche Publikation etwas wie *Collider*, *entdeckt* usw. enthalten w√ºrde. Daher k√∂nnen Wortfrequenzen in vielen F√§llen ein guter Indikator f√ºr den Textinhalt sein.

Das Problem mit BoW ist, dass bestimmte h√§ufige W√∂rter, wie *und*, *ist* usw., in den meisten Texten erscheinen und die h√∂chsten Frequenzen aufweisen, wodurch die W√∂rter, die wirklich wichtig sind, in den Hintergrund gedr√§ngt werden. Wir k√∂nnen die Bedeutung dieser W√∂rter verringern, indem wir die H√§ufigkeit ber√ºcksichtigen, mit der W√∂rter in der gesamten Dokumentensammlung auftreten. Dies ist die Hauptidee hinter dem TF/IDF-Ansatz, der in den Notebooks, die zu dieser Lektion geh√∂ren, detaillierter behandelt wird.

Allerdings k√∂nnen keine dieser Ans√§tze die **Semantik** des Textes vollst√§ndig ber√ºcksichtigen. Wir ben√∂tigen leistungsf√§higere Modelle neuronaler Netzwerke, um dies zu tun, was wir sp√§ter in diesem Abschnitt besprechen werden.

## ‚úçÔ∏è √úbungen: Textdarstellung

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [Textdarstellung mit PyTorch](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [Textdarstellung mit TensorFlow](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## Fazit

Bis jetzt haben wir Techniken untersucht, die Frequenzgewichtungen f√ºr verschiedene W√∂rter hinzuf√ºgen k√∂nnen. Sie sind jedoch nicht in der Lage, Bedeutung oder Reihenfolge darzustellen. Wie der ber√ºhmte Linguist J. R. Firth 1935 sagte: "Die vollst√§ndige Bedeutung eines Wortes ist immer kontextabh√§ngig, und kein Studium der Bedeutung, das vom Kontext getrennt ist, kann ernst genommen werden." Wir werden sp√§ter im Kurs lernen, wie man kontextuelle Informationen aus Text mithilfe von Sprachmodellen erfasst.

## üöÄ Herausforderung

Versuchen Sie einige andere √úbungen mit Bag-of-Words und verschiedenen Datenmodellen. Sie k√∂nnten inspiriert werden von diesem [Wettbewerb auf Kaggle](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)

## [Nach dem Vortrag Quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## √úberpr√ºfung & Selbststudium

√úben Sie Ihre F√§higkeiten mit Text-Embeddings und Bag-of-Words-Techniken auf [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste)

## [Aufgabe: Notebooks](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als die ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.