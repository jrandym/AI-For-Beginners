# Sprachmodellierung

Semantische Einbettungen, wie Word2Vec und GloVe, sind in der Tat ein erster Schritt in Richtung **Sprachmodellierung** - Modelle zu erstellen, die irgendwie die Natur der Sprache *verstehen* (oder *darstellen*).

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

Die Hauptidee hinter der Sprachmodellierung besteht darin, sie auf unlabeled Datens√§tzen auf un√ºberwachtem Weg zu trainieren. Dies ist wichtig, da wir gro√üe Mengen an unlabeled Text zur Verf√ºgung haben, w√§hrend die Menge an labeled Text immer durch den Aufwand, den wir f√ºr das Labeling aufwenden k√∂nnen, begrenzt ist. Oft k√∂nnen wir Sprachmodelle erstellen, die **fehlende W√∂rter** im Text vorhersagen, da es einfach ist, ein zuf√§lliges Wort im Text zu maskieren und es als Trainingsbeispiel zu verwenden.

## Einbettungen trainieren

In unseren vorherigen Beispielen haben wir vortrainierte semantische Einbettungen verwendet, aber es ist interessant zu sehen, wie diese Einbettungen trainiert werden k√∂nnen. Es gibt mehrere m√∂gliche Ans√§tze, die verwendet werden k√∂nnen:

* **N-Gram** Sprachmodellierung, wenn wir ein Token vorhersagen, indem wir auf N vorherige Tokens (N-gram) schauen.
* **Continuous Bag-of-Words** (CBoW), wenn wir das mittlere Token $W_0$ in einer Token-Sequenz $W_{-N}$, ..., $W_N$ vorhersagen.
* **Skip-gram**, wo wir eine Menge benachbarter Tokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} aus dem mittleren Token $W_0$ vorhersagen.

![Bild aus dem Papier zur Umwandlung von W√∂rtern in Vektoren](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.de.png)

> Bild aus [diesem Papier](https://arxiv.org/pdf/1301.3781.pdf)

## ‚úçÔ∏è Beispiel-Notebooks: CBoW-Modell trainieren

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [Training CBoW Word2Vec mit TensorFlow](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)
* [Training CBoW Word2Vec mit PyTorch](../../../../../lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb)

## Fazit

In der vorherigen Lektion haben wir gesehen, dass Wort-Einbettungen wie Magie funktionieren! Jetzt wissen wir, dass das Trainieren von Wort-Einbettungen keine sehr komplexe Aufgabe ist und wir in der Lage sein sollten, unsere eigenen Wort-Einbettungen f√ºr dom√§nenspezifischen Text zu trainieren, falls erforderlich.

## [Nachlese-Quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## √úberpr√ºfung & Selbststudium

* [Offizielles PyTorch-Tutorial zur Sprachmodellierung](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Offizielles TensorFlow-Tutorial zum Training des Word2Vec-Modells](https://www.TensorFlow.org/tutorials/text/word2vec).
* Die Verwendung des **gensim**-Frameworks, um die am h√§ufigsten verwendeten Einbettungen in wenigen Codezeilen zu trainieren, wird [in dieser Dokumentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) beschrieben.

## üöÄ [Aufgabe: Trainiere Skip-Gram-Modell](lab/README.md)

Im Labor fordern wir Sie heraus, den Code aus dieser Lektion zu √§ndern, um ein Skip-Gram-Modell anstelle von CBoW zu trainieren. [Lesen Sie die Details](lab/README.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als die ma√ügebliche Quelle angesehen werden. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.