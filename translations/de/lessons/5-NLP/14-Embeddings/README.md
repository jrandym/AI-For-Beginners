# Einbettungen

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

Beim Training von Klassifikatoren, die auf BoW oder TF/IDF basieren, haben wir mit hochdimensionalen Bag-of-Words-Vektoren mit einer L√§nge von `vocab_size` gearbeitet und explizit von niedrigdimensionalen positionsbasierten Repr√§sentationsvektoren in sp√§rliche One-Hot-Repr√§sentationen umgewandelt. Diese One-Hot-Repr√§sentation ist jedoch nicht speichereffizient. Dar√ºber hinaus wird jedes Wort unabh√§ngig voneinander behandelt, d.h. One-Hot-kodierte Vektoren dr√ºcken keine semantische √Ñhnlichkeit zwischen W√∂rtern aus.

Die Idee der **Einbettung** besteht darin, W√∂rter durch niederdimensionale dichte Vektoren darzustellen, die in gewisser Weise die semantische Bedeutung eines Wortes widerspiegeln. Wir werden sp√§ter besprechen, wie man sinnvolle Wort-Einbettungen erstellt, aber f√ºr den Moment denken wir einfach an Einbettungen als eine M√∂glichkeit, die Dimensionalit√§t eines Wortvektors zu verringern.

Die Einbettungsschicht w√ºrde ein Wort als Eingabe nehmen und einen Ausgabvektor der angegebenen `embedding_size` erzeugen. In gewissem Sinne √§hnelt es einer `Linear`-Schicht, aber anstatt einen One-Hot-kodierten Vektor zu verwenden, kann es eine Wortnummer als Eingabe akzeptieren, was es uns erm√∂glicht, gro√üe One-Hot-kodierte Vektoren zu vermeiden.

Durch die Verwendung einer Einbettungsschicht als erste Schicht in unserem Klassifikator-Netzwerk k√∂nnen wir von einem Bag-of-Words- zu einem **Einbettungsbeutel**-Modell wechseln, bei dem wir zun√§chst jedes Wort in unserem Text in die entsprechende Einbettung umwandeln und dann eine aggregierte Funktion √ºber all diese Einbettungen berechnen, wie z.B. `sum`, `average` oder `max`.  

![Bild, das einen Einbettungsklassifikator f√ºr f√ºnf aufeinanderfolgende W√∂rter zeigt.](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.de.png)

> Bild vom Autor

## ‚úçÔ∏è √úbungen: Einbettungen

Setze dein Lernen in den folgenden Notebooks fort:
* [Einbettungen mit PyTorch](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [Einbettungen TensorFlow](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## Semantische Einbettungen: Word2Vec

W√§hrend die Einbettungsschicht gelernt hat, W√∂rter in Vektorrepr√§sentationen abzubilden, hatte diese Repr√§sentation nicht unbedingt viel semantische Bedeutung. Es w√§re sch√∂n, eine Vektorrepr√§sentation zu lernen, sodass √§hnliche W√∂rter oder Synonyme Vektoren entsprechen, die in Bezug auf eine bestimmte Vektordistanz (z.B. euklidische Distanz) nah beieinander liegen.

Um dies zu erreichen, m√ºssen wir unser Einbettungsmodell auf eine bestimmte Weise auf einer gro√üen Textsammlung vortrainieren. Eine M√∂glichkeit, semantische Einbettungen zu trainieren, nennt sich [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). Es basiert auf zwei Hauptarchitekturen, die verwendet werden, um eine verteilte Repr√§sentation von W√∂rtern zu erzeugen:

 - **Kontinuierlicher Bag-of-Words** (CBoW) ‚Äî In dieser Architektur trainieren wir das Modell, um ein Wort aus dem umgebenden Kontext vorherzusagen. Gegeben das ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$ besteht das Ziel des Modells darin, $W_0$ aus $(W_{-2},W_{-1},W_1,W_2)$ vorherzusagen.
 - **Kontinuierliches Skip-Gram** ist das Gegenteil von CBoW. Das Modell verwendet das umgebende Fenster von Kontextw√∂rtern, um das aktuelle Wort vorherzusagen.

CBoW ist schneller, w√§hrend Skip-Gram langsamer ist, aber eine bessere Darstellung seltener W√∂rter liefert.

![Bild, das sowohl CBoW- als auch Skip-Gram-Algorithmen zur Umwandlung von W√∂rtern in Vektoren zeigt.](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.de.png)

> Bild aus [diesem Papier](https://arxiv.org/pdf/1301.3781.pdf)

Vortrainierte Word2Vec-Einbettungen (sowie andere √§hnliche Modelle wie GloVe) k√∂nnen auch anstelle der Einbettungsschicht in neuronalen Netzwerken verwendet werden. Allerdings m√ºssen wir uns mit Vokabularen auseinandersetzen, da das Vokabular, das zum Vortrainieren von Word2Vec/GloVe verwendet wird, wahrscheinlich von dem Vokabular in unserem Textkorpus abweicht. Sieh dir die oben genannten Notebooks an, um zu sehen, wie dieses Problem gel√∂st werden kann.

## Kontextuelle Einbettungen

Eine wesentliche Einschr√§nkung traditioneller vortrainierter Einbettungsrepr√§sentationen wie Word2Vec ist das Problem der Mehrdeutigkeit von W√∂rtern. W√§hrend vortrainierte Einbettungen einige Bedeutungen von W√∂rtern im Kontext erfassen k√∂nnen, wird jede m√∂gliche Bedeutung eines Wortes in der gleichen Einbettung kodiert. Dies kann Probleme in nachgelagerten Modellen verursachen, da viele W√∂rter, wie das Wort 'play', je nach verwendetem Kontext unterschiedliche Bedeutungen haben.

Zum Beispiel haben die W√∂rter 'play' in diesen beiden verschiedenen S√§tzen eine ganz andere Bedeutung:

- Ich ging zu einem **Theaterst√ºck**.
- John m√∂chte mit seinen Freunden **spielen**.

Die vortrainierten Einbettungen oben repr√§sentieren beide Bedeutungen des Wortes 'play' in der gleichen Einbettung. Um diese Einschr√§nkung zu √ºberwinden, m√ºssen wir Einbettungen auf der Grundlage des **Sprachmodells** erstellen, das auf einem gro√üen Textkorpus trainiert wurde und *wei√ü*, wie W√∂rter in unterschiedlichen Kontexten zusammengef√ºgt werden k√∂nnen. Die Diskussion √ºber kontextuelle Einbettungen f√§llt au√üerhalb des Rahmens dieses Tutorials, aber wir werden sp√§ter im Kurs darauf zur√ºckkommen, wenn wir √ºber Sprachmodelle sprechen.

## Fazit

In dieser Lektion hast du entdeckt, wie man Einbettungsschichten in TensorFlow und Pytorch erstellt und verwendet, um die semantischen Bedeutungen von W√∂rtern besser widerzuspiegeln.

## üöÄ Herausforderung

Word2Vec wurde f√ºr einige interessante Anwendungen verwendet, einschlie√ülich der Generierung von Songtexten und Poesie. Schau dir [diesen Artikel](https://www.politetype.com/blog/word2vec-color-poems) an, der erkl√§rt, wie der Autor Word2Vec zur Erstellung von Poesie verwendet hat. Schau dir auch [dieses Video von Dan Shiffmann](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) an, um eine andere Erkl√§rung dieser Technik zu entdecken. Versuche dann, diese Techniken auf deinen eigenen Textkorpus anzuwenden, vielleicht aus Kaggle.

## [Nachlese-Quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## √úberpr√ºfung & Selbststudium

Lies dir dieses Papier √ºber Word2Vec durch: [Effiziente Sch√§tzung von Wortdarstellungen im Vektorraum](https://arxiv.org/pdf/1301.3781.pdf)

## [Aufgabe: Notebooks](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als die ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.