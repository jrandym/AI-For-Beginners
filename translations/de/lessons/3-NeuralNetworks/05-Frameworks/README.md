# Frameworks f√ºr Neuronale Netzwerke

Wie wir bereits gelernt haben, um neuronale Netzwerke effizient zu trainieren, m√ºssen wir zwei Dinge tun:

* Mit Tensors arbeiten, z. B. multiplizieren, addieren und Funktionen wie Sigmoid oder Softmax berechnen
* Die Gradienten aller Ausdr√ºcke berechnen, um die Optimierung durch Gradientenabstieg durchzuf√ºhren

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/105)

W√§hrend die `numpy` Bibliothek den ersten Teil erledigen kann, ben√∂tigen wir einen Mechanismus zur Berechnung von Gradienten. In [unserem Framework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb), das wir im vorherigen Abschnitt entwickelt haben, mussten wir alle Ableitungsfunktionen manuell innerhalb der `backward` Methode programmieren, die Backpropagation durchf√ºhrt. Idealerweise sollte ein Framework uns die M√∂glichkeit geben, die Gradienten *jeden Ausdrucks* zu berechnen, den wir definieren k√∂nnen.

Ein weiterer wichtiger Aspekt ist die F√§higkeit, Berechnungen auf der GPU oder anderen spezialisierten Recheneinheiten wie [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) durchzuf√ºhren. Das Training tiefer neuronaler Netzwerke erfordert *eine Menge* Berechnungen, und die M√∂glichkeit, diese Berechnungen auf GPUs zu parallelisieren, ist von gro√üer Bedeutung.

> ‚úÖ Der Begriff 'parallelisieren' bedeutet, die Berechnungen auf mehrere Ger√§te zu verteilen.

Derzeit sind die beiden beliebtesten neuronalen Frameworks: [TensorFlow](http://TensorFlow.org) und [PyTorch](https://pytorch.org/). Beide bieten eine Low-Level-API, um mit Tensors sowohl auf CPU als auch auf GPU zu arbeiten. Dar√ºber hinaus gibt es eine h√∂herstufige API, die entsprechend [Keras](https://keras.io/) und [PyTorch Lightning](https://pytorchlightning.ai/) genannt wird.

Low-Level API | [TensorFlow](http://TensorFlow.org) | [PyTorch](https://pytorch.org/)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](https://keras.io/) | [PyTorch Lightning](https://pytorchlightning.ai/)

**Low-Level APIs** in beiden Frameworks erm√∂glichen es Ihnen, sogenannte **Berechnungsgraphen** zu erstellen. Dieser Graph definiert, wie man die Ausgabe (normalerweise die Verlustfunktion) mit gegebenen Eingabeparametern berechnet und kann zur Berechnung auf der GPU verschoben werden, wenn diese verf√ºgbar ist. Es gibt Funktionen, um diesen Berechnungsgraphen zu differenzieren und Gradienten zu berechnen, die dann zur Optimierung der Modellparameter verwendet werden k√∂nnen.

**High-Level APIs** betrachten neuronale Netzwerke im Wesentlichen als eine **Folge von Schichten** und erleichtern den Aufbau der meisten neuronalen Netzwerke erheblich. Das Training des Modells erfordert normalerweise die Vorbereitung der Daten und dann das Aufrufen einer `fit` Funktion, um die Arbeit zu erledigen.

Die hochgradige API erm√∂glicht es Ihnen, typische neuronale Netzwerke sehr schnell zu konstruieren, ohne sich um viele Details k√ºmmern zu m√ºssen. Gleichzeitig bieten Low-Level-APIs viel mehr Kontrolle √ºber den Trainingsprozess, und daher werden sie h√§ufig in der Forschung verwendet, wenn es darum geht, neue Architekturen neuronaler Netzwerke zu entwickeln.

Es ist auch wichtig zu verstehen, dass Sie beide APIs zusammen verwenden k√∂nnen, z. B. k√∂nnen Sie Ihre eigene Netzwerkarchitektur mit der Low-Level-API entwickeln und sie dann in das gr√∂√üere Netzwerk einf√ºgen, das mit der High-Level-API konstruiert und trainiert wurde. Oder Sie k√∂nnen ein Netzwerk mit der High-Level-API als Folge von Schichten definieren und dann Ihre eigene Low-Level-Trainingsschleife verwenden, um die Optimierung durchzuf√ºhren. Beide APIs verwenden die gleichen grundlegenden zugrunde liegenden Konzepte und sind so gestaltet, dass sie gut zusammenarbeiten.

## Lernen

In diesem Kurs bieten wir den Gro√üteil des Inhalts sowohl f√ºr PyTorch als auch f√ºr TensorFlow an. Sie k√∂nnen Ihr bevorzugtes Framework ausw√§hlen und nur die entsprechenden Notebooks durchgehen. Wenn Sie sich nicht sicher sind, welches Framework Sie w√§hlen sollen, lesen Sie einige Diskussionen im Internet √ºber **PyTorch vs. TensorFlow**. Sie k√∂nnen auch beide Frameworks betrachten, um ein besseres Verst√§ndnis zu bekommen.

Wo immer m√∂glich, werden wir High-Level-APIs zur Vereinfachung verwenden. Wir glauben jedoch, dass es wichtig ist, zu verstehen, wie neuronale Netzwerke von Grund auf funktionieren, weshalb wir zu Beginn mit der Low-Level-API und Tensors arbeiten. Wenn Sie jedoch schnell vorankommen und nicht viel Zeit mit dem Lernen dieser Details verbringen m√∂chten, k√∂nnen Sie diese √ºberspringen und direkt zu den High-Level-API-Notebooks gehen.

## ‚úçÔ∏è √úbungen: Frameworks

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

Low-Level API | [TensorFlow+Keras Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb) | [PyTorch](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb)
--------------|-------------------------------------|--------------------------------
High-level API| [Keras](../../../../../lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) | *PyTorch Lightning*

Nachdem Sie die Frameworks gemeistert haben, lassen Sie uns das Konzept des Overfittings zusammenfassen.

# Overfitting

Overfitting ist ein √§u√üerst wichtiges Konzept im maschinellen Lernen, und es ist sehr wichtig, es richtig zu verstehen!

Betrachten Sie das folgende Problem, 5 Punkte (dargestellt durch `x` in den Grafiken unten) zu approximieren:

![linear](../../../../../translated_images/overfit1.f24b71c6f652e59e6bed7245ffbeaecc3ba320e16e2221f6832b432052c4da43.de.jpg) | ![overfit](../../../../../translated_images/overfit2.131f5800ae10ca5e41d12a411f5f705d9ee38b1b10916f284b787028dd55cc1c.de.jpg)
-------------------------|--------------------------
**Lineares Modell, 2 Parameter** | **Nicht-lineares Modell, 7 Parameter**
Training Fehler = 5.3 | Training Fehler = 0
Validierungsfehler = 5.1 | Validierungsfehler = 20

* Links sehen wir eine gute gerade Linienapproximation. Da die Anzahl der Parameter angemessen ist, erfasst das Modell die Idee hinter der Punktverteilung gut.
* Rechts ist das Modell zu m√§chtig. Da wir nur 5 Punkte haben und das Modell 7 Parameter hat, kann es sich so anpassen, dass es durch alle Punkte verl√§uft, was den Trainingsfehler auf 0 reduziert. Dies hindert das Modell jedoch daran, das korrekte Muster hinter den Daten zu verstehen, sodass der Validierungsfehler sehr hoch ist.

Es ist sehr wichtig, ein korrektes Gleichgewicht zwischen der Komplexit√§t des Modells (Anzahl der Parameter) und der Anzahl der Trainingsproben zu finden.

## Warum Overfitting auftritt

  * Nicht gen√ºgend Trainingsdaten
  * Zu m√§chtiges Modell
  * Zu viel Rauschen in den Eingabedaten

## Wie man Overfitting erkennt

Wie Sie aus dem obigen Diagramm sehen k√∂nnen, kann Overfitting durch einen sehr niedrigen Trainingsfehler und einen hohen Validierungsfehler erkannt werden. Normalerweise sehen wir w√§hrend des Trainings, dass sowohl der Trainings- als auch der Validierungsfehler zu sinken beginnen, und dann kann der Validierungsfehler an einem bestimmten Punkt aufh√∂ren zu sinken und anfangen zu steigen. Dies wird ein Zeichen f√ºr Overfitting sein und der Hinweis, dass wir wahrscheinlich an dieser Stelle das Training stoppen sollten (oder zumindest einen Snapshot des Modells erstellen sollten).

![overfitting](../../../../../translated_images/Overfitting.408ad91cd90b4371d0a81f4287e1409c359751adeb1ae450332af50e84f08c3e.de.png)

## Wie man Overfitting verhindert

Wenn Sie sehen, dass Overfitting auftritt, k√∂nnen Sie Folgendes tun:

 * Erh√∂hen Sie die Menge an Trainingsdaten
 * Verringern Sie die Komplexit√§t des Modells
 * Verwenden Sie eine [Regularisierungstechnik](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md), wie z. B. [Dropout](../../4-ComputerVision/08-TransferLearning/TrainingTricks.md#Dropout), die wir sp√§ter betrachten werden.

## Overfitting und Bias-Varianz-Handel

Overfitting ist tats√§chlich ein Fall eines allgemeineren Problems in der Statistik, das als [Bias-Varianz-Handel](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) bekannt ist. Wenn wir die m√∂glichen Fehlerquellen in unserem Modell betrachten, k√∂nnen wir zwei Arten von Fehlern sehen:

* **Bias-Fehler** entstehen, weil unser Algorithmus nicht in der Lage ist, die Beziehung zwischen den Trainingsdaten korrekt zu erfassen. Dies kann darauf zur√ºckzuf√ºhren sein, dass unser Modell nicht m√§chtig genug ist (**Underfitting**).
* **Varianz-Fehler**, die entstehen, weil das Modell Rauschen in den Eingabedaten anstelle von bedeutungsvollen Beziehungen approximiert (**Overfitting**).

W√§hrend des Trainings nimmt der Bias-Fehler ab (da unser Modell lernt, die Daten zu approximieren), und der Varianz-Fehler nimmt zu. Es ist wichtig, das Training zu stoppen - entweder manuell (wenn wir Overfitting erkennen) oder automatisch (durch Einf√ºhrung von Regularisierung) - um Overfitting zu verhindern.

## Fazit

In dieser Lektion haben Sie die Unterschiede zwischen den verschiedenen APIs der beiden beliebtesten KI-Frameworks, TensorFlow und PyTorch, kennengelernt. Dar√ºber hinaus haben Sie ein sehr wichtiges Thema, Overfitting, behandelt.

## üöÄ Herausforderung

In den begleitenden Notebooks finden Sie am Ende 'Aufgaben'; arbeiten Sie die Notebooks durch und erf√ºllen Sie die Aufgaben.

## [Nachlesequiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/205)

## √úberpr√ºfung & Selbststudium

Recherchieren Sie zu den folgenden Themen:

- TensorFlow
- PyTorch
- Overfitting

Stellen Sie sich die folgenden Fragen:

- Was ist der Unterschied zwischen TensorFlow und PyTorch?
- Was ist der Unterschied zwischen Overfitting und Underfitting?

## [Aufgabe](lab/README.md)

In diesem Labor werden Sie gebeten, zwei Klassifikationsprobleme mit ein- und mehrschichtigen voll verbundenen Netzwerken mit PyTorch oder TensorFlow zu l√∂sen.

* [Anweisungen](lab/README.md)
* [Notebook](../../../../../lessons/3-NeuralNetworks/05-Frameworks/lab/LabFrameworks.ipynb)

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als die ma√ügebliche Quelle betrachtet werden. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.