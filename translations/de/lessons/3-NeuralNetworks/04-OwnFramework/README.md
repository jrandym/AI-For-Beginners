# EinfÃ¼hrung in Neuronale Netzwerke. Mehrschichtiger Perzeptron

Im vorherigen Abschnitt haben Sie das einfachste Modell eines neuronalen Netzwerks kennengelernt - den einlagigen Perzeptron, ein lineares Klassifikationsmodell fÃ¼r zwei Klassen.

In diesem Abschnitt werden wir dieses Modell zu einem flexibleren Rahmen erweitern, der es uns ermÃ¶glicht:

* **Mehrklassenklassifikation** zusÃ¤tzlich zur Zwei-Klassen-Klassifikation durchzufÃ¼hren
* **Regressionsprobleme** zusÃ¤tzlich zur Klassifikation zu lÃ¶sen
* Klassen zu trennen, die nicht linear separierbar sind

Wir werden auch unser eigenes modulares Framework in Python entwickeln, das es uns ermÃ¶glicht, verschiedene Architekturen neuronaler Netzwerke zu konstruieren.

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Formalisierung des Maschinellen Lernens

Lassen Sie uns mit der Formalisierung des Problems des Maschinellen Lernens beginnen. Angenommen, wir haben einen Trainingsdatensatz **X** mit Labels **Y**, und wir mÃ¼ssen ein Modell *f* erstellen, das die genauesten Vorhersagen trifft. Die QualitÃ¤t der Vorhersagen wird durch die **Verlustfunktion** â„’ gemessen. Folgende Verlustfunktionen werden hÃ¤ufig verwendet:

* FÃ¼r das Regressionsproblem, wenn wir eine Zahl vorhersagen mÃ¼ssen, kÃ¶nnen wir den **absoluten Fehler** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| oder den **quadratischen Fehler** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> verwenden
* FÃ¼r die Klassifikation verwenden wir den **0-1 Fehler** (der im Wesentlichen dasselbe wie die **Genauigkeit** des Modells ist) oder den **logistischen Fehler**.

FÃ¼r den einlagigen Perzeptron wurde die Funktion *f* als lineare Funktion *f(x)=wx+b* definiert (hier ist *w* die Gewichtsmatrix, *x* der Vektor der Eingangsmerkmale und *b* der Bias-Vektor). FÃ¼r verschiedene Architekturen neuronaler Netzwerke kann diese Funktion komplexere Formen annehmen.

> Im Falle der Klassifikation ist es oft wÃ¼nschenswert, Wahrscheinlichkeiten der entsprechenden Klassen als Netzwerk-Ausgabe zu erhalten. Um willkÃ¼rliche Zahlen in Wahrscheinlichkeiten umzuwandeln (z.B. um die Ausgabe zu normalisieren), verwenden wir oft die **Softmax**-Funktion Ïƒ, und die Funktion *f* wird zu *f(x)=Ïƒ(wx+b)*

In der obigen Definition von *f* werden *w* und *b* als **Parameter** Î¸=âŸ¨*w,b*âŸ© bezeichnet. Gegebenen den Datensatz âŸ¨**X**,**Y**âŸ© kÃ¶nnen wir einen Gesamter Fehler Ã¼ber den gesamten Datensatz als Funktion der Parameter Î¸ berechnen.

> âœ… **Das Ziel des Trainings eines neuronalen Netzwerks besteht darin, den Fehler durch Variieren der Parameter Î¸ zu minimieren.**

## Gradientabstieg-Optimierung

Es gibt eine bekannte Methode zur Optimierung von Funktionen, die als **Gradientenabstieg** bezeichnet wird. Die Idee ist, dass wir die Ableitung (im mehrdimensionalen Fall als **Gradient** bezeichnet) der Verlustfunktion bezÃ¼glich der Parameter berechnen und die Parameter so variieren, dass der Fehler sinkt. Dies kann wie folgt formalisiert werden:

* Initialisieren Sie die Parameter mit zufÃ¤lligen Werten w<sup>(0)</sup>, b<sup>(0)</sup>
* Wiederholen Sie den folgenden Schritt viele Male:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

WÃ¤hrend des Trainings sollen die Optimierungsschritte unter BerÃ¼cksichtigung des gesamten Datensatzes berechnet werden (denken Sie daran, dass der Verlust als Summe Ã¼ber alle Trainingsproben berechnet wird). In der Praxis nehmen wir jedoch kleine Portionen des Datensatzes, die als **Minibatches** bezeichnet werden, und berechnen die Gradienten basierend auf einer Teilmenge von Daten. Da die Teilmenge jedes Mal zufÃ¤llig ausgewÃ¤hlt wird, wird diese Methode als **stochastischer Gradientabstieg** (SGD) bezeichnet.

## Mehrschichtige Perzeptrons und RÃ¼ckpropagation

Das einlagige Netzwerk, wie wir oben gesehen haben, ist in der Lage, linear separierbare Klassen zu klassifizieren. Um ein reichhaltigeres Modell zu erstellen, kÃ¶nnen wir mehrere Schichten des Netzwerks kombinieren. Mathematisch wÃ¼rde das bedeuten, dass die Funktion *f* eine komplexere Form annehmen wÃ¼rde und in mehreren Schritten berechnet wird:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Hier ist Î± eine **nicht-lineare Aktivierungsfunktion**, Ïƒ ist eine Softmax-Funktion, und die Parameter sind Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Der Gradientabstiegsalgorithmus bleibt derselbe, aber es wird schwieriger, die Gradienten zu berechnen. Gegebenen der Kettenregel fÃ¼r Ableitungen kÃ¶nnen wir die Ableitungen wie folgt berechnen:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Die Kettenregel wird verwendet, um die Ableitungen der Verlustfunktion bezÃ¼glich der Parameter zu berechnen.

Beachten Sie, dass der linkeste Teil all dieser AusdrÃ¼cke gleich ist, und somit kÃ¶nnen wir die Ableitungen effektiv berechnen, indem wir von der Verlustfunktion ausgehen und "rÃ¼ckwÃ¤rts" durch den Berechnungsgraphen gehen. Daher wird die Methode zum Trainieren eines mehrschichtigen Perzeptrons als **RÃ¼ckpropagation** oder 'Backprop' bezeichnet.

<img alt="Berechnungsgraph" src="images/ComputeGraphGrad.png"/>

> TODO: Bildnachweis

> âœ… Wir werden die RÃ¼ckpropagation in unserem Notizbuchbeispiel viel detaillierter behandeln.  

## Fazit

In dieser Lektion haben wir unsere eigene Bibliothek fÃ¼r neuronale Netzwerke erstellt und sie fÃ¼r eine einfache zweidimensionale Klassifikationsaufgabe verwendet.

## ğŸš€ Herausforderung

Im begleitenden Notizbuch werden Sie Ihr eigenes Framework zur Erstellung und zum Training von mehrschichtigen Perzeptrons implementieren. Sie werden im Detail sehen, wie moderne neuronale Netzwerke funktionieren.

Gehen Sie zum Notizbuch [OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) und arbeiten Sie es durch.

## [Nachvorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## ÃœberprÃ¼fung & Selbststudium

RÃ¼ckpropagation ist ein gÃ¤ngiger Algorithmus, der in KI und ML verwendet wird und es wert ist, [im Detail](https://wikipedia.org/wiki/Backpropagation) studiert zu werden.

## [Aufgabe](lab/README.md)

In diesem Labor werden Sie gebeten, das Framework, das Sie in dieser Lektion erstellt haben, zu verwenden, um die Klassifikation handgeschriebener Ziffern im MNIST-Datensatz zu lÃ¶sen.

* [Anleitungen](lab/README.md)
* [Notizbuch](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**Haftungsausschluss**:  
Dieses Dokument wurde mit Hilfe von KI-Ãœbersetzungsdiensten Ã¼bersetzt. Obwohl wir uns um Genauigkeit bemÃ¼hen, beachten Sie bitte, dass automatisierte Ãœbersetzungen Fehler oder Ungenauigkeiten enthalten kÃ¶nnen. Das Originaldokument in seiner ursprÃ¼nglichen Sprache sollte als die maÃŸgebliche Quelle betrachtet werden. FÃ¼r kritische Informationen wird eine professionelle menschliche Ãœbersetzung empfohlen. Wir Ã¼bernehmen keine Haftung fÃ¼r MissverstÃ¤ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser Ãœbersetzung entstehen.