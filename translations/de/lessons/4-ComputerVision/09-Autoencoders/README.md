# Autoencoder

Beim Training von CNNs besteht eines der Probleme darin, dass wir eine gro√üe Menge an beschrifteten Daten ben√∂tigen. Im Falle der Bildklassifikation m√ºssen wir Bilder in verschiedene Klassen unterteilen, was einen manuellen Aufwand erfordert.

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/109)

Wir m√∂chten jedoch m√∂glicherweise rohe (unbeschriftete) Daten f√ºr das Training von CNN-Feature-Extraktoren verwenden, was als **selbst√ºberwachtes Lernen** bezeichnet wird. Anstelle von Beschriftungen verwenden wir Trainingsbilder sowohl als Eingabe als auch als Ausgabe des Netzwerks. Die Hauptidee des **Autoencoders** ist, dass wir ein **Encoder-Netzwerk** haben, das das Eingangsbild in einen **latent space** (normalerweise ein Vektor einer kleineren Gr√∂√üe) umwandelt, und dann das **Decoder-Netzwerk**, dessen Ziel es ist, das urspr√ºngliche Bild wiederherzustellen.

> ‚úÖ Ein [Autoencoder](https://wikipedia.org/wiki/Autoencoder) ist "eine Art k√ºnstliches neuronales Netzwerk, das verwendet wird, um effiziente Kodierungen von unbeschrifteten Daten zu lernen."

Da wir einen Autoencoder trainieren, um so viele Informationen wie m√∂glich aus dem urspr√ºnglichen Bild f√ºr eine genaue Rekonstruktion zu erfassen, versucht das Netzwerk, die beste **Einbettung** der Eingabebilder zu finden, um die Bedeutung zu erfassen.

![AutoEncoder Diagramm](../../../../../translated_images/autoencoder_schema.5e6fc9ad98a5eb6197f3513cf3baf4dfbe1389a6ae74daebda64de9f1c99f142.de.jpg)

> Bild von [Keras Blog](https://blog.keras.io/building-autoencoders-in-keras.html)

## Szenarien f√ºr die Verwendung von Autoencodern

Obwohl die Rekonstruktion urspr√ºnglicher Bilder an sich nicht n√ºtzlich erscheint, gibt es einige Szenarien, in denen Autoencoder besonders n√ºtzlich sind:

* **Reduzierung der Dimension von Bildern zur Visualisierung** oder **Training von Bild-Einbettungen**. In der Regel liefern Autoencoder bessere Ergebnisse als PCA, da sie die r√§umliche Natur von Bildern und hierarchische Merkmale ber√ºcksichtigen.
* **Rauschunterdr√ºckung**, d.h. das Entfernen von Rauschen aus dem Bild. Da Rauschen viele nutzlose Informationen enth√§lt, kann der Autoencoder diese nicht alle in den relativ kleinen latenten Raum einf√ºgen und erfasst daher nur den wichtigen Teil des Bildes. Beim Training von Rauschunterdr√ºckern beginnen wir mit den Originalbildern und verwenden Bilder mit k√ºnstlich hinzugef√ºgtem Rauschen als Eingabe f√ºr den Autoencoder.
* **Superaufl√∂sung**, d.h. Erh√∂hung der Bildaufl√∂sung. Wir beginnen mit hochaufl√∂senden Bildern und verwenden das Bild mit niedrigerer Aufl√∂sung als Eingabe f√ºr den Autoencoder.
* **Generative Modelle**. Sobald wir den Autoencoder trainiert haben, kann der Decoder-Teil verwendet werden, um neue Objekte aus zuf√§lligen latenten Vektoren zu erstellen.

## Variationale Autoencoder (VAE)

Traditionelle Autoencoder reduzieren die Dimension der Eingabedaten auf irgendeine Weise, indem sie die wichtigen Merkmale der Eingabebilder herausfinden. Allerdings machen latente Vektoren oft nicht viel Sinn. Mit anderen Worten, wenn wir das MNIST-Dataset als Beispiel nehmen, ist es keine einfache Aufgabe herauszufinden, welche Ziffern verschiedenen latenten Vektoren entsprechen, da nahe latente Vektoren nicht unbedingt den gleichen Ziffern entsprechen.

Andererseits ist es besser, um *generative* Modelle zu trainieren, ein gewisses Verst√§ndnis des latenten Raums zu haben. Diese Idee f√ºhrt uns zu **variational auto-encoder** (VAE).

VAE ist der Autoencoder, der lernt, die *statistische Verteilung* der latenten Parameter, die sogenannte **latente Verteilung**, vorherzusagen. Zum Beispiel m√∂chten wir, dass latente Vektoren normal verteilt sind mit einem Mittelwert z<sub>mean</sub> und einer Standardabweichung z<sub>sigma</sub> (sowohl Mittelwert als auch Standardabweichung sind Vektoren einer bestimmten Dimension d). Der Encoder in VAE lernt, diese Parameter vorherzusagen, und dann nimmt der Decoder einen zuf√§lligen Vektor aus dieser Verteilung, um das Objekt zu rekonstruieren.

Zusammenfassend:

 * Vom Eingangsvektor sagen wir `z_mean` und `z_log_sigma` voraus (anstatt die Standardabweichung selbst vorherzusagen, sagen wir ihren Logarithmus voraus)
 * Wir ziehen einen Vektor `sample` aus der Verteilung N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * Der Decoder versucht, das urspr√ºngliche Bild unter Verwendung von `sample` als Eingangsvektor zu decodieren

 <img src="images/vae.png" width="50%">

> Bild von [diesem Blogbeitrag](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) von Isaak Dykeman

Variationale Autoencoder verwenden eine komplexe Verlustfunktion, die aus zwei Teilen besteht:

* **Rekonstruktionsverlust** ist die Verlustfunktion, die zeigt, wie nah ein rekonstruiertes Bild am Zielbild ist (es kann der Mittlere Quadratische Fehler, oder MSE, sein). Es ist dieselbe Verlustfunktion wie bei normalen Autoencodern.
* **KL-Verlust**, der sicherstellt, dass die Verteilungen der latenten Variablen nahe an der Normalverteilung bleiben. Er basiert auf dem Konzept der [Kullback-Leibler-Divergenz](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - eine Metrik zur Sch√§tzung, wie √§hnlich zwei statistische Verteilungen sind.

Ein wichtiger Vorteil von VAEs ist, dass sie es uns erm√∂glichen, relativ einfach neue Bilder zu generieren, da wir wissen, aus welcher Verteilung wir latente Vektoren ziehen k√∂nnen. Wenn wir beispielsweise VAE mit einem 2D-latenten Vektor auf MNIST trainieren, k√∂nnen wir dann die Komponenten des latenten Vektors variieren, um verschiedene Ziffern zu erhalten:

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Bild von [Dmitry Soshnikov](http://soshnikov.com)

Beobachten Sie, wie die Bilder ineinander √ºbergehen, w√§hrend wir latente Vektoren aus verschiedenen Bereichen des latenten Parameterraums erhalten. Wir k√∂nnen diesen Raum auch in 2D visualisieren:

<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Bild von [Dmitry Soshnikov](http://soshnikov.com)

## ‚úçÔ∏è √úbungen: Autoencoder

Erfahren Sie mehr √ºber Autoencoder in diesen entsprechenden Notizb√ºchern:

* [Autoencoders in TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)
* [Autoencoders in PyTorch](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb)

## Eigenschaften von Autoencodern

* **Daten-spezifisch** - sie funktionieren nur gut mit der Art von Bildern, auf denen sie trainiert wurden. Wenn wir beispielsweise ein Superaufl√∂sungsnetzwerk auf Blumen trainieren, wird es bei Portr√§ts nicht gut abschneiden. Das liegt daran, dass das Netzwerk ein hochaufl√∂sendes Bild erzeugen kann, indem es feine Details aus den im Trainingsdatensatz gelernten Merkmalen entnimmt.
* **Verlustbehaftet** - das rekonstruierte Bild ist nicht dasselbe wie das urspr√ºngliche Bild. Die Art des Verlusts wird durch die *Verlustfunktion* definiert, die w√§hrend des Trainings verwendet wird.
* Funktioniert mit **unbeschrifteten Daten**.

## [Nachlese-Quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/209)

## Fazit

In dieser Lektion haben Sie die verschiedenen Arten von Autoencodern kennengelernt, die f√ºr den KI-Wissenschaftler verf√ºgbar sind. Sie haben gelernt, wie man sie baut und wie man sie zur Rekonstruktion von Bildern verwendet. Sie haben auch √ºber den VAE gelernt und wie man ihn verwendet, um neue Bilder zu generieren.

## üöÄ Herausforderung

In dieser Lektion haben Sie gelernt, wie man Autoencoder f√ºr Bilder verwendet. Aber sie k√∂nnen auch f√ºr Musik verwendet werden! Schauen Sie sich das Projekt [MusicVAE](https://magenta.tensorflow.org/music-vae) des Magenta-Projekts an, das Autoencoder verwendet, um zu lernen, Musik zu rekonstruieren. Machen Sie einige [Experimente](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) mit dieser Bibliothek, um zu sehen, was Sie erstellen k√∂nnen.

## [Nachlese-Quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## √úberpr√ºfung & Selbststudium

Zur Referenz lesen Sie mehr √ºber Autoencoder in diesen Ressourcen:

* [Autoencoder in Keras erstellen](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blogbeitrag auf NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variationale Autoencoder erkl√§rt](https://kvfrans.com/variational-autoencoders-explained/)
* [Bedingte Variationale Autoencoder](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Aufgabe

Am Ende von [diesem Notizbuch mit TensorFlow](../../../../../lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb) finden Sie eine 'Aufgabe' - verwenden Sie dies als Ihre Aufgabe.

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als autoritative Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Verantwortung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.