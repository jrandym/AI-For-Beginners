# Deep Reinforcement Learning

Reinforcement Learning (RL) wird als eines der grundlegenden Paradigmen des maschinellen Lernens angesehen, neben dem √ºberwachten und un√ºberwachten Lernen. W√§hrend wir im √ºberwachten Lernen auf Datens√§tze mit bekannten Ergebnissen angewiesen sind, basiert RL auf **Lernen durch Handeln**. Zum Beispiel, wenn wir ein Computer-Spiel zum ersten Mal sehen, fangen wir an zu spielen, selbst ohne die Regeln zu kennen, und bald sind wir in der Lage, unsere F√§higkeiten allein durch das Spielen und Anpassen unseres Verhaltens zu verbessern.

## [Vorlesungsquiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

Um RL durchzuf√ºhren, ben√∂tigen wir:

* Eine **Umgebung** oder **Simulation**, die die Regeln des Spiels festlegt. Wir sollten in der Lage sein, die Experimente in der Simulation durchzuf√ºhren und die Ergebnisse zu beobachten.
* Eine **Belohnungsfunktion**, die angibt, wie erfolgreich unser Experiment war. Im Fall des Lernens, ein Computerspiel zu spielen, w√§re die Belohnung unser Endpunktestand.

Basierend auf der Belohnungsfunktion sollten wir in der Lage sein, unser Verhalten anzupassen und unsere F√§higkeiten zu verbessern, damit wir beim n√§chsten Mal besser spielen. Der Hauptunterschied zwischen anderen Arten des maschinellen Lernens und RL besteht darin, dass wir im RL typischerweise nicht wissen, ob wir gewinnen oder verlieren, bis wir das Spiel beenden. Daher k√∂nnen wir nicht sagen, ob ein bestimmter Zug allein gut oder schlecht ist - wir erhalten die Belohnung erst am Ende des Spiels.

W√§hrend des RL f√ºhren wir typischerweise viele Experimente durch. W√§hrend jedes Experiments m√ºssen wir ein Gleichgewicht finden zwischen der Verfolgung der optimalen Strategie, die wir bisher gelernt haben (**Exploitation**), und der Erkundung neuer m√∂glicher Zust√§nde (**Exploration**).

## OpenAI Gym

Ein gro√üartiges Werkzeug f√ºr RL ist das [OpenAI Gym](https://gym.openai.com/) - eine **Simulationsumgebung**, die viele verschiedene Umgebungen simulieren kann, von Atari-Spielen bis hin zur Physik hinter dem Balancieren eines Mastes. Es ist eine der beliebtesten Simulationsumgebungen zur Schulung von Reinforcement-Learning-Algorithmen und wird von [OpenAI](https://openai.com/) gepflegt.

> **Hinweis**: Sie k√∂nnen alle verf√ºgbaren Umgebungen von OpenAI Gym [hier](https://gym.openai.com/envs/#classic_control) einsehen.

## CartPole Balancing

Sie haben wahrscheinlich alle moderne Balancierger√§te wie den *Segway* oder *Gyroscooter* gesehen. Diese sind in der Lage, automatisch zu balancieren, indem sie ihre R√§der als Reaktion auf ein Signal von einem Beschleunigungsmesser oder Gyroskop anpassen. In diesem Abschnitt werden wir lernen, wie man ein √§hnliches Problem l√∂st - das Balancieren eines Mastes. Es ist vergleichbar mit einer Situation, in der ein Zirkusk√ºnstler einen Mast auf seiner Hand balancieren muss - jedoch findet dieses Balancieren nur in 1D statt.

Eine vereinfachte Version des Balancierens ist als **CartPole**-Problem bekannt. In der CartPole-Welt haben wir einen horizontalen Schieber, der sich nach links oder rechts bewegen kann, und das Ziel ist es, einen vertikalen Mast oben auf dem Schieber auszubalancieren, w√§hrend er sich bewegt.

<img alt="ein cartpole" src="images/cartpole.png" width="200"/>

Um diese Umgebung zu erstellen und zu nutzen, ben√∂tigen wir ein paar Zeilen Python-Code:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

Jede Umgebung kann auf genau die gleiche Weise aufgerufen werden:
* `env.reset` starts a new experiment
* `env.step` f√ºhrt einen Simulationsschritt durch. Es erh√§lt eine **Aktion** aus dem **Aktionsraum** und gibt eine **Beobachtung** (aus dem Beobachtungsraum) sowie eine Belohnung und ein Beendigungsflag zur√ºck.

Im obigen Beispiel f√ºhren wir bei jedem Schritt eine zuf√§llige Aktion aus, weshalb die Lebensdauer des Experiments sehr kurz ist:

![nicht balancierendes cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

Das Ziel eines RL-Algorithmus ist es, ein Modell - die sogenannte **Politik** œÄ - zu trainieren, das die Aktion als Antwort auf einen gegebenen Zustand zur√ºckgibt. Wir k√∂nnen die Politik auch als probabilistisch betrachten, d.h. f√ºr jeden Zustand *s* und jede Aktion *a* gibt sie die Wahrscheinlichkeit œÄ(*a*|*s*) zur√ºck, dass wir *a* im Zustand *s* w√§hlen sollten.

## Policy Gradients Algorithmus

Der offensichtlichste Weg, eine Politik zu modellieren, besteht darin, ein neuronales Netzwerk zu erstellen, das Zust√§nde als Eingabe nimmt und die entsprechenden Aktionen (oder besser gesagt die Wahrscheinlichkeiten aller Aktionen) zur√ºckgibt. In gewissem Sinne w√§re es √§hnlich wie bei einer normalen Klassifikationsaufgabe, mit einem wesentlichen Unterschied - wir wissen im Voraus nicht, welche Aktionen wir in jedem Schritt ausf√ºhren sollten.

Die Idee hier ist, diese Wahrscheinlichkeiten zu sch√§tzen. Wir erstellen einen Vektor von **kumulierten Belohnungen**, der unsere Gesamtbelohnung in jedem Schritt des Experiments zeigt. Wir wenden auch **Belohnungsdiskontierung** an, indem wir fr√ºhere Belohnungen mit einem Koeffizienten Œ≥=0.99 multiplizieren, um die Rolle der fr√ºheren Belohnungen zu verringern. Dann verst√§rken wir die Schritte entlang des Experimentpfades, die gr√∂√üere Belohnungen erzielen.

> Erfahren Sie mehr √ºber den Policy-Gradient-Algorithmus und sehen Sie ihn in Aktion im [Beispiel-Notebook](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb).

## Actor-Critic Algorithmus

Eine verbesserte Version des Policy-Gradients-Ansatzes wird als **Actor-Critic** bezeichnet. Die Hauptidee dahinter ist, dass das neuronale Netzwerk trainiert wird, um zwei Dinge zur√ºckzugeben:

* Die Politik, die bestimmt, welche Aktion zu ergreifen ist. Dieser Teil wird **Actor** genannt.
* Die Sch√§tzung der Gesamtbelohnung, die wir in diesem Zustand erwarten k√∂nnen - dieser Teil wird **Critic** genannt.

In gewissem Sinne √§hnelt diese Architektur einem [GAN](../../4-ComputerVision/10-GANs/README.md), bei dem wir zwei Netzwerke haben, die gegeneinander trainiert werden. Im Actor-Critic-Modell schl√§gt der Actor die Aktion vor, die wir ausf√ºhren m√ºssen, und der Critic versucht, kritisch zu sein und das Ergebnis zu sch√§tzen. Unser Ziel ist es jedoch, diese Netzwerke gemeinsam zu trainieren.

Da wir sowohl die tats√§chlichen kumulierten Belohnungen als auch die Ergebnisse, die der Critic w√§hrend des Experiments zur√ºckgibt, kennen, ist es relativ einfach, eine Verlustfunktion zu erstellen, die den Unterschied zwischen ihnen minimiert. Das w√ºrde uns **Critic-Verlust** geben. Wir k√∂nnen **Actor-Verlust** berechnen, indem wir denselben Ansatz wie im Policy-Gradient-Algorithmus verwenden.

Nach dem Ausf√ºhren eines dieser Algorithmen k√∂nnen wir erwarten, dass unser CartPole so aussieht:

![ein balancierendes cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ‚úçÔ∏è √úbungen: Policy Gradients und Actor-Critic RL

Setzen Sie Ihr Lernen in den folgenden Notebooks fort:

* [RL in TensorFlow](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [RL in PyTorch](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## Weitere RL-Aufgaben

Reinforcement Learning ist heutzutage ein schnell wachsendes Forschungsfeld. Einige interessante Beispiele f√ºr Reinforcement Learning sind:

* Einem Computer beibringen, **Atari-Spiele** zu spielen. Der herausfordernde Teil dieses Problems besteht darin, dass wir keinen einfachen Zustand in Form eines Vektors haben, sondern eher einen Screenshot - und wir m√ºssen das CNN verwenden, um dieses Bildschirmbild in einen Merkmalsvektor zu konvertieren oder Belohnungsinformationen zu extrahieren. Atari-Spiele sind im Gym verf√ºgbar.
* Einem Computer beibringen, Brettspiele wie Schach und Go zu spielen. K√ºrzlich wurden hochmoderne Programme wie **Alpha Zero** von zwei Agenten, die gegeneinander spielten und sich mit jedem Schritt verbesserten, von Grund auf trainiert.
* In der Industrie wird RL verwendet, um Steuerungssysteme aus Simulationen zu erstellen. Ein Dienst namens [Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) wurde speziell daf√ºr entwickelt.

## Fazit

Wir haben nun gelernt, wie man Agenten trainiert, um gute Ergebnisse zu erzielen, indem wir ihnen eine Belohnungsfunktion bereitstellen, die den gew√ºnschten Zustand des Spiels definiert, und ihnen die M√∂glichkeit geben, den Suchraum intelligent zu erkunden. Wir haben erfolgreich zwei Algorithmen ausprobiert und in relativ kurzer Zeit ein gutes Ergebnis erzielt. Dies ist jedoch nur der Anfang Ihrer Reise in das RL, und Sie sollten auf jeden Fall in Betracht ziehen, einen separaten Kurs zu belegen, wenn Sie tiefer eintauchen m√∂chten.

## üöÄ Herausforderung

Erforschen Sie die in der Rubrik 'Weitere RL-Aufgaben' aufgef√ºhrten Anwendungen und versuchen Sie, eine zu implementieren!

## [Nachlese-Quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## √úberpr√ºfung & Selbststudium

Erfahren Sie mehr √ºber klassisches Reinforcement Learning in unserem [Maschinenlernen f√ºr Anf√§nger Curriculum](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md).

Sehen Sie sich [dieses gro√üartige Video](https://www.youtube.com/watch?v=qv6UVOQ0F44) an, das dar√ºber spricht, wie ein Computer lernen kann, Super Mario zu spielen.

## Aufgabe: [Trainiere ein Mountain Car](lab/README.md)

Ihr Ziel w√§hrend dieser Aufgabe w√§re es, eine andere Gym-Umgebung zu trainieren - [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/).

**Haftungsausschluss**:  
Dieses Dokument wurde mit Hilfe von KI-gest√ºtzten maschinellen √úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.