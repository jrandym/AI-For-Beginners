# Sinir AÄŸlarÄ±na GiriÅŸ. Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±

Ã–nceki bÃ¶lÃ¼mde, en basit sinir aÄŸÄ± modelini - tek katmanlÄ± algÄ±layÄ±cÄ±yÄ±, iki sÄ±nÄ±flÄ± lineer sÄ±nÄ±flandÄ±rma modeli olarak Ã¶ÄŸrendiniz.

Bu bÃ¶lÃ¼mde, bu modeli daha esnek bir Ã§erÃ§eveye geniÅŸleteceÄŸiz, bÃ¶ylece:

* iki sÄ±nÄ±fÄ±n yanÄ± sÄ±ra **Ã§oklu sÄ±nÄ±f sÄ±nÄ±flandÄ±rmasÄ±** gerÃ§ekleÅŸtirebileceÄŸiz
* sÄ±nÄ±flandÄ±rmanÄ±n yanÄ± sÄ±ra **regresyon problemleri** Ã§Ã¶zebileceÄŸiz
* lineer olarak ayrÄ±lamayan sÄ±nÄ±flarÄ± ayÄ±rabileceÄŸiz

AyrÄ±ca, farklÄ± sinir aÄŸÄ± mimarilerini oluÅŸturmak iÃ§in Python'da kendi modÃ¼ler Ã§erÃ§evemizi geliÅŸtireceÄŸiz.

## [Ders Ã¶ncesi quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Makine Ã–ÄŸreniminin FormÃ¼lasyonu

Makine Ã–ÄŸrenimi problemini formÃ¼le etmeye baÅŸlayalÄ±m. Diyelim ki etiketleri **Y** olan bir eÄŸitim veri setimiz **X** var ve en doÄŸru tahminleri yapacak bir model *f* inÅŸa etmemiz gerekiyor. Tahminlerin kalitesi **KayÄ±p fonksiyonu** â„’ ile Ã¶lÃ§Ã¼lÃ¼r. AÅŸaÄŸÄ±daki kayÄ±p fonksiyonlarÄ± sÄ±kÃ§a kullanÄ±lÄ±r:

* Regresyon problemi iÃ§in, bir sayÄ± tahmin etmemiz gerektiÄŸinde, **mutlak hata** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| veya **kare hata** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> kullanabiliriz.
* SÄ±nÄ±flandÄ±rma iÃ§in, **0-1 kaybÄ±** (bu, modelin **doÄŸruluÄŸu** ile Ã¶zdeÅŸtir) veya **lojistik kayÄ±p** kullanÄ±rÄ±z.

Tek katmanlÄ± algÄ±layÄ±cÄ± iÃ§in, *f* fonksiyonu lineer bir fonksiyon olarak tanÄ±mlanmÄ±ÅŸtÄ± *f(x)=wx+b* (burada *w* aÄŸÄ±rlÄ±k matrisidir, *x* giriÅŸ Ã¶zellikleri vektÃ¶rÃ¼dÃ¼r ve *b* Ã¶nyargÄ± vektÃ¶rÃ¼dÃ¼r). FarklÄ± sinir aÄŸÄ± mimarileri iÃ§in, bu fonksiyon daha karmaÅŸÄ±k bir biÃ§im alabilir.

> SÄ±nÄ±flandÄ±rma durumunda, genellikle aÄŸ Ã§Ä±kÄ±ÅŸÄ± olarak ilgili sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ± almak arzu edilir. Rastgele sayÄ±larÄ± olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in (Ã¶rneÄŸin, Ã§Ä±ktÄ±yÄ± normalleÅŸtirmek amacÄ±yla) genellikle **softmax** fonksiyonu Ïƒ kullanÄ±lÄ±r ve *f* fonksiyonu *f(x)=Ïƒ(wx+b)* haline gelir.

YukarÄ±daki *f* tanÄ±mÄ±nda, *w* ve *b* **parametreler** Î¸=âŸ¨*w,b*âŸ© olarak adlandÄ±rÄ±lÄ±r. Veri seti âŸ¨**X**,**Y**âŸ© verildiÄŸinde, tÃ¼m veri seti Ã¼zerinde genel bir hatayÄ± parametreler Î¸'nÄ±n bir fonksiyonu olarak hesaplayabiliriz.

> âœ… **Sinir aÄŸÄ± eÄŸitiminin amacÄ±, parametre Î¸'yÄ± deÄŸiÅŸtirerek hatayÄ± minimize etmektir.**

## Gradyan Ä°niÅŸi Optimizasyonu

**Gradyan iniÅŸi** olarak bilinen bir fonksiyon optimizasyon yÃ¶ntemi vardÄ±r. Fikir, kayÄ±p fonksiyonunun parametreler ile olan tÃ¼revini (Ã§ok boyutlu durumda **gradyan** olarak adlandÄ±rÄ±lÄ±r) hesaplayabilmemiz ve hatanÄ±n azalacak ÅŸekilde parametreleri deÄŸiÅŸtirebilmemizdir. Bu, aÅŸaÄŸÄ±daki gibi formÃ¼le edilebilir:

* Parametreleri bazÄ± rastgele deÄŸerlerle w<sup>(0)</sup>, b<sup>(0)</sup> ile baÅŸlatÄ±n.
* AÅŸaÄŸÄ±daki adÄ±mÄ± birÃ§ok kez tekrarlayÄ±n:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

EÄŸitim sÄ±rasÄ±nda, optimizasyon adÄ±mlarÄ±nÄ±n tÃ¼m veri setini dikkate alarak hesaplanmasÄ± beklenir (kayÄ±p, tÃ¼m eÄŸitim Ã¶rnekleri Ã¼zerinden bir toplam olarak hesaplandÄ±ÄŸÄ±nÄ± unutmayÄ±n). Ancak, gerÃ§ek hayatta **minibatch** olarak adlandÄ±rÄ±lan veri setinin kÃ¼Ã§Ã¼k parÃ§alarÄ±nÄ± alÄ±r ve gradyanlarÄ± bir veri alt kÃ¼mesine dayalÄ± olarak hesaplarÄ±z. Her seferinde alt kÃ¼me rastgele alÄ±ndÄ±ÄŸÄ± iÃ§in, bu yÃ¶ntem **stochastik gradyan iniÅŸi** (SGD) olarak adlandÄ±rÄ±lÄ±r.

## Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±lar ve Geri YayÄ±lÄ±m

YukarÄ±da gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z tek katmanlÄ± aÄŸ, lineer olarak ayrÄ±lamayan sÄ±nÄ±flarÄ± sÄ±nÄ±flandÄ±rma yeteneÄŸine sahiptir. Daha zengin bir model oluÅŸturmak iÃ§in, aÄŸÄ±n birkaÃ§ katmanÄ±nÄ± birleÅŸtirebiliriz. Matematiksel olarak bu, *f* fonksiyonunun daha karmaÅŸÄ±k bir biÃ§im alacaÄŸÄ± ve birkaÃ§ adÄ±mda hesaplanacaÄŸÄ± anlamÄ±na gelir:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Burada, Î± **lineer olmayan aktivasyon fonksiyonu**dur, Ïƒ softmax fonksiyonudur ve parametreler Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>'dir.

Gradyan iniÅŸi algoritmasÄ± aynÄ± kalacak, ancak gradyanlarÄ± hesaplamak daha zor olacaktÄ±r. Zincirleme tÃ¼rev kuralÄ±nÄ± gÃ¶z Ã¶nÃ¼nde bulundurarak, tÃ¼revleri ÅŸu ÅŸekilde hesaplayabiliriz:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Zincirleme tÃ¼rev kuralÄ±, kayÄ±p fonksiyonunun parametrelerle olan tÃ¼revlerini hesaplamak iÃ§in kullanÄ±lÄ±r.

TÃ¼m bu ifadelerin en soldaki kÄ±smÄ±nÄ±n aynÄ± olduÄŸunu ve bu nedenle kayÄ±p fonksiyonundan baÅŸlayarak "geriye" hesaplama grafiÄŸi Ã¼zerinden tÃ¼revleri etkili bir ÅŸekilde hesaplayabileceÄŸimizi unutmayÄ±n. BÃ¶ylece Ã§ok katmanlÄ± bir algÄ±layÄ±cÄ±nÄ±n eÄŸitilmesi yÃ¶ntemi **geri yayÄ±lÄ±m** veya 'backprop' olarak adlandÄ±rÄ±lÄ±r.

<img alt="hesap grafiÄŸi" src="images/ComputeGraphGrad.png"/>

> TODO: resim alÄ±ntÄ±sÄ±

> âœ… Geri yayÄ±lÄ±mÄ± not defteri Ã¶rneÄŸimizde Ã§ok daha ayrÄ±ntÄ±lÄ± bir ÅŸekilde ele alacaÄŸÄ±z.  

## SonuÃ§

Bu derste, kendi sinir aÄŸÄ± kÃ¼tÃ¼phanemizi oluÅŸturduk ve bunu basit bir iki boyutlu sÄ±nÄ±flandÄ±rma gÃ¶revinde kullandÄ±k.

## ğŸš€ Zorluk

EÅŸlik eden not defterinde, Ã§ok katmanlÄ± algÄ±layÄ±cÄ±lar oluÅŸturmak ve eÄŸitmek iÃ§in kendi Ã§erÃ§evenizi uygulayacaksÄ±nÄ±z. Modern sinir aÄŸlarÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ayrÄ±ntÄ±lÄ± olarak gÃ¶rebileceksiniz.

[OwnFramework](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb) not defterine geÃ§in ve Ã¼zerinde Ã§alÄ±ÅŸÄ±n.

## [Ders sonrasÄ± quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

Geri yayÄ±lÄ±m, AI ve ML'de yaygÄ±n olarak kullanÄ±lan bir algoritmadÄ±r, [daha ayrÄ±ntÄ±lÄ± olarak](https://wikipedia.org/wiki/Backpropagation) incelenmeye deÄŸer.

## [GÃ¶rev](lab/README.md)

Bu laboratuvar Ã§alÄ±ÅŸmasÄ±nda, bu derste inÅŸa ettiÄŸiniz Ã§erÃ§eveyi MNIST el yazÄ±sÄ± rakam sÄ±nÄ±flandÄ±rma sorununu Ã§Ã¶zmek iÃ§in kullanmanÄ±z istenmektedir.

* [Talimatlar](lab/README.md)
* [Not Defteri](../../../../../lessons/3-NeuralNetworks/04-OwnFramework/lab/MyFW_MNIST.ipynb)

**AÃ§Ä±klama**:  
Bu belge, makine tabanlÄ± yapay zeka Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilmektedir. Bu Ã§evirinin kullanÄ±lmasÄ± sonucu ortaya Ã§Ä±kan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlama iÃ§in sorumluluk kabul etmiyoruz.