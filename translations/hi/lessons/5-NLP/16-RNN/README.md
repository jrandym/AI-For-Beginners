# पुनरावृत्त न्यूरल नेटवर्क

## [प्री-लेचर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

पिछले अनुभागों में, हमने टेक्स्ट के समृद्ध अर्थपूर्ण प्रतिनिधित्व और एम्बेडिंग के शीर्ष पर एक साधारण रैखिक वर्गीकरणकर्ता का उपयोग किया है। यह आर्किटेक्चर वाक्य में शब्दों के समग्र अर्थ को पकड़ता है, लेकिन यह शब्दों के **क्रम** पर ध्यान नहीं देता है, क्योंकि एम्बेडिंग के ऊपर की एकत्रीकरण प्रक्रिया ने मूल टेक्स्ट से यह जानकारी हटा दी। चूंकि ये मॉडल शब्दों के क्रम को मॉडल करने में असमर्थ हैं, वे टेक्स्ट जनरेशन या प्रश्न उत्तर देने जैसे अधिक जटिल या अस्पष्ट कार्यों को हल नहीं कर सकते।

टेक्स्ट अनुक्रम के अर्थ को पकड़ने के लिए, हमें एक अन्य न्यूरल नेटवर्क आर्किटेक्चर का उपयोग करने की आवश्यकता है, जिसे **पुनरावृत्त न्यूरल नेटवर्क** या RNN कहा जाता है। RNN में, हम अपने वाक्य को नेटवर्क के माध्यम से एक प्रतीक में एक बार पास करते हैं, और नेटवर्क कुछ **स्थिति** उत्पन्न करता है, जिसे हम फिर अगले प्रतीक के साथ नेटवर्क में फिर से पास करते हैं।

![RNN](../../../../../translated_images/rnn.27f5c29c53d727b546ad3961637a267f0fe9ec5ab01f2a26a853c92fcefbb574.hi.png)

> चित्र लेखक द्वारा

इनपुट अनुक्रम X<sub>0</sub>,...,X<sub>n</sub> के साथ, RNN न्यूरल नेटवर्क ब्लॉकों की एक श्रृंखला बनाता है, और इस श्रृंखला को बैकप्रोपगेशन का उपयोग करके अंत से अंत तक प्रशिक्षित करता है। प्रत्येक नेटवर्क ब्लॉक एक जोड़ी (X<sub>i</sub>,S<sub>i</sub>) को इनपुट के रूप में लेता है, और परिणाम के रूप में S<sub>i+1</sub> उत्पन्न करता है। अंतिम स्थिति S<sub>n</sub> या (आउटपुट Y<sub>n</sub>) परिणाम उत्पन्न करने के लिए एक रैखिक वर्गीकरणकर्ता में जाती है। सभी नेटवर्क ब्लॉकों के पास समान वज़न होते हैं, और इन्हें एक बैकप्रोपगेशन पास का उपयोग करके अंत से अंत तक प्रशिक्षित किया जाता है।

चूंकि स्थिति वेक्टर S<sub>0</sub>,...,S<sub>n</sub> नेटवर्क के माध्यम से पास किए जाते हैं, यह शब्दों के बीच अनुक्रमिक निर्भरताओं को सीखने में सक्षम होता है। उदाहरण के लिए, जब शब्द *not* अनुक्रम में कहीं दिखाई देता है, तो यह स्थिति वेक्टर के भीतर कुछ तत्वों को नकारात्मक करने के लिए सीख सकता है, जिससे नकारात्मकता उत्पन्न होती है।

> ✅ चूंकि ऊपर चित्रित सभी RNN ब्लॉकों के वज़न साझा किए जाते हैं, इसलिए एक ही चित्र को एक ब्लॉक (दाएं) के रूप में एक पुनरावृत्त फीडबैक लूप के साथ प्रदर्शित किया जा सकता है, जो नेटवर्क की आउटपुट स्थिति को इनपुट पर वापस भेजता है।

## RNN सेल का आकार

आइए देखें कि एक साधारण RNN सेल कैसे संगठित होता है। यह पिछले स्थिति S<sub>i-1</sub> और वर्तमान प्रतीक X<sub>i</sub> को इनपुट के रूप में स्वीकार करता है, और इसे आउटपुट स्थिति S<sub>i</sub> उत्पन्न करना होता है (और, कभी-कभी, हम किसी अन्य आउटपुट Y<sub>i</sub> में भी रुचि रखते हैं, जैसे कि जनरेटिव नेटवर्क के मामले में)।

एक साधारण RNN सेल के अंदर दो वजन मैट्रिक्स होते हैं: एक इनपुट प्रतीक को परिवर्तित करता है (इसे W कहते हैं), और दूसरा इनपुट स्थिति को परिवर्तित करता है (H)। इस मामले में नेटवर्क का आउटपुट σ(W×X<sub>i</sub>+H×S<sub>i-1</sub>+b) के रूप में गणना की जाती है, जहाँ σ सक्रियण कार्य है और b अतिरिक्त पूर्वाग्रह है।

<img alt="RNN Cell Anatomy" src="images/rnn-anatomy.png" width="50%"/>

> चित्र लेखक द्वारा

कई मामलों में, इनपुट टोकन RNN में प्रवेश करने से पहले एम्बेडिंग लेयर के माध्यम से पास किए जाते हैं ताकि आयाम को कम किया जा सके। इस मामले में, यदि इनपुट वेक्टर का आयाम *emb_size* है, और स्थिति वेक्टर *hid_size* है - तो W का आकार *emb_size*×*hid_size* है, और H का आकार *hid_size*×*hid_size* है।

## लंबी छोटी अवधि की मेमोरी (LSTM)

क्लासिकल RNNs की एक मुख्य समस्या **विलुप्त ग्रेडिएंट** समस्या है। चूंकि RNNs को एक बैकप्रोपगेशन पास में अंत से अंत तक प्रशिक्षित किया जाता है, इसलिए इसे नेटवर्क की पहले की परतों में त्रुटि को प्रसारित करने में कठिनाई होती है, और इस प्रकार नेटवर्क दूरस्थ टोकनों के बीच संबंधों को सीख नहीं सकता। इस समस्या से बचने के लिए एक तरीका है **स्पष्ट स्थिति प्रबंधन** को लागू करना, जिसे **गेट्स** कहा जाता है। इस प्रकार की दो प्रसिद्ध आर्किटेक्चर हैं: **लंबी छोटी अवधि की मेमोरी** (LSTM) और **गेटेड रिले यूनिट** (GRU)।

![लंबी छोटी अवधि की मेमोरी सेल का उदाहरण दिखाते हुए चित्र](../../../../../lessons/5-NLP/16-RNN/images/long-short-term-memory-cell.svg)

> चित्र स्रोत TBD

LSTM नेटवर्क RNN के समान तरीके से व्यवस्थित होता है, लेकिन यहाँ दो स्थितियाँ होती हैं जो परत से परत में पास की जाती हैं: वास्तविक स्थिति C, और छिपा हुआ वेक्टर H। प्रत्येक यूनिट में, छिपा हुआ वेक्टर H<sub>i</sub> को इनपुट X<sub>i</sub> के साथ संयोजित किया जाता है, और वे स्थिति C पर **गेट्स** के माध्यम से क्या होता है, इसे नियंत्रित करते हैं। प्रत्येक गेट एक न्यूरल नेटवर्क है जिसमें सिग्मॉइड सक्रियण (आउटपुट [0,1] के दायरे में) होता है, जिसे स्थिति वेक्टर के साथ गुणा करने पर एक बिटवाइज मास्क के रूप में सोचा जा सकता है। निम्नलिखित गेट्स हैं (ऊपर चित्र में बाएं से दाएं):

* **भूलने का गेट** एक छिपे हुए वेक्टर को लेता है और यह निर्धारित करता है कि हमें वेक्टर C के कौन से घटकों को भूलना है, और कौन से घटकों को पास करना है।
* **इनपुट गेट** इनपुट और छिपे हुए वेक्टरों से कुछ जानकारी लेता है और इसे स्थिति में डालता है।
* **आउटपुट गेट** स्थिति को *tanh* सक्रियण के साथ एक रैखिक परत के माध्यम से परिवर्तित करता है, फिर नए स्थिति C<sub>i+1</sub> उत्पन्न करने के लिए छिपे हुए वेक्टर H<sub>i</sub> का उपयोग करके इसके कुछ घटकों का चयन करता है।

स्थिति C के घटकों को कुछ झंडों के रूप में सोचा जा सकता है जिन्हें चालू और बंद किया जा सकता है। उदाहरण के लिए, जब हम अनुक्रम में नाम *Alice* का सामना करते हैं, तो हम मान सकते हैं कि यह एक महिला पात्र का संदर्भ है, और स्थिति में झंडा उठाते हैं कि वाक्य में एक महिला संज्ञा है। जब हम आगे *and Tom* जैसे वाक्यांशों का सामना करते हैं, तो हम झंडा उठाते हैं कि हमारे पास एक बहुवचन संज्ञा है। इस प्रकार, स्थिति को नियंत्रित करके हम संभवतः वाक्य के भागों की व्याकरणिक विशेषताओं को ट्रैक रख सकते हैं।

> ✅ LSTM के आंतरिक को समझने के लिए एक उत्कृष्ट संसाधन इस महान लेख [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) द्वारा क्रिस्टोफर ओलाह है।

## द्विदिशीय और मल्टीलेयर RNNs

हमने उन पुनरावृत्त नेटवर्कों पर चर्चा की है जो एक दिशा में कार्य करते हैं, अनुक्रम की शुरुआत से अंत तक। यह स्वाभाविक लगता है, क्योंकि यह उस तरीके से मिलता-जुलता है जिससे हम पढ़ते और भाषण सुनते हैं। हालांकि, चूंकि कई व्यावहारिक मामलों में हमें इनपुट अनुक्रम तक यादृच्छिक पहुंच होती है, इसलिए यह दोनों दिशाओं में पुनरावृत्त गणना करना समझ में आ सकता है। ऐसे नेटवर्कों को **द्विदिशीय** RNNs कहा जाता है। द्विदिशीय नेटवर्क के साथ काम करते समय, हमें प्रत्येक दिशा के लिए दो छिपी स्थिति वेक्टरों की आवश्यकता होगी।

एक पुनरावृत्त नेटवर्क, चाहे वह एक-निर्देशात्मक हो या द्विदिशीय, अनुक्रम के भीतर कुछ पैटर्न को पकड़ता है, और उन्हें स्थिति वेक्टर में संग्रहीत कर सकता है या आउटपुट में पास कर सकता है। कंवोल्यूशनल नेटवर्कों की तरह, हम पहले वाले पर एक और पुनरावृत्त परत बना सकते हैं ताकि उच्च स्तर के पैटर्न को पकड़ सकें और पहले परत द्वारा निकाले गए निम्न-स्तरीय पैटर्न से बना सकें। यह हमें **मल्टी-लेयर RNN** के विचार की ओर ले जाता है, जो दो या दो से अधिक पुनरावृत्त नेटवर्कों से बना होता है, जहाँ पिछले परत का आउटपुट अगली परत में इनपुट के रूप में पास किया जाता है।

![मल्टीलेयर लंबी छोटी अवधि की मेमोरी RNN का उदाहरण दिखाते हुए चित्र](../../../../../translated_images/multi-layer-lstm.dd975e29bb2a59fe58b429db833932d734c81f211cad2783797a9608984acb8c.hi.jpg)

*चित्र [इस अद्भुत पोस्ट](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) से लिया गया है जो फर्नांडो लोपेज़ द्वारा है*

## ✍️ अभ्यास: एम्बेडिंग

निम्नलिखित नोटबुक में अपनी सीख जारी रखें:

* [PyTorch के साथ RNNs](../../../../../lessons/5-NLP/16-RNN/RNNPyTorch.ipynb)
* [TensorFlow के साथ RNNs](../../../../../lessons/5-NLP/16-RNN/RNNTF.ipynb)

## निष्कर्ष

इस इकाई में, हमने देखा कि RNNs का उपयोग अनुक्रम वर्गीकरण के लिए किया जा सकता है, लेकिन वास्तव में, वे कई अन्य कार्यों को संभाल सकते हैं, जैसे टेक्स्ट जनरेशन, मशीन अनुवाद, और अधिक। हम अगले इकाई में उन कार्यों पर विचार करेंगे।

## 🚀 चुनौती

LSTMs के बारे में कुछ साहित्य पढ़ें और उनके अनुप्रयोगों पर विचार करें:

- [ग्रिड लंबी छोटी अवधि की मेमोरी](https://arxiv.org/pdf/1507.01526v1.pdf)
- [दिखाएं, ध्यान दें और बताएं: दृश्य ध्यान के साथ न्यूरल इमेज कैप्शन जनरेशन](https://arxiv.org/pdf/1502.03044v2.pdf)

## [पोस्ट-लेचर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## समीक्षा और आत्म अध्ययन

- [LSTM नेटवर्क को समझना](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) क्रिस्टोफर ओलाह द्वारा।

## [असाइनमेंट: नोटबुक](assignment.md)

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियाँ या असंगतताएँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।