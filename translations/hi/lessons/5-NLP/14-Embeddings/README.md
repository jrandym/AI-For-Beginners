# एम्बेडिंग

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/114)

जब हमने BoW या TF/IDF के आधार पर वर्गीकर्ता प्रशिक्षित किए, तो हमने उच्च-आयामी बैग-ऑफ-शब्द वेक्टर पर काम किया, जिसकी लंबाई `vocab_size` है, और हम स्पष्ट रूप से निम्न-आयामी स्थिति प्रतिनिधित्व वेक्टर कोSparse one-hot प्रतिनिधित्व में परिवर्तित कर रहे थे। हालाँकि, यह one-hot प्रतिनिधित्व मेमोरी-कुशल नहीं है। इसके अतिरिक्त, प्रत्येक शब्द को एक-दूसरे से स्वतंत्र रूप से माना जाता है, अर्थात् one-hot एन्कोडेड वेक्टर शब्दों के बीच किसी भी अर्थात्मक समानता को व्यक्त नहीं करते हैं।

**एम्बेडिंग** का विचार शब्दों का प्रतिनिधित्व करना है कम-आयामी घनत्व वेक्टर द्वारा, जो किसी तरह से शब्द के अर्थ को दर्शाते हैं। हम बाद में सार्थक शब्द एम्बेडिंग बनाने के तरीके पर चर्चा करेंगे, लेकिन अभी के लिए हम बस एम्बेडिंग को एक शब्द वेक्टर की आयाम को कम करने के तरीके के रूप में सोचते हैं।

तो, एम्बेडिंग परत एक शब्द को इनपुट के रूप में लेगी, और निर्दिष्ट `embedding_size` का आउटपुट वेक्टर उत्पन्न करेगी। एक अर्थ में, यह `Linear` परत के बहुत समान है, लेकिन यह one-hot एन्कोडेड वेक्टर लेने के बजाय, यह एक शब्द संख्या को इनपुट के रूप में ले सकेगी, जिससे हमें बड़े one-hot एन्कोडेड वेक्टर बनाने से बचने की अनुमति मिलती है।

हमारे वर्गीकर्ता नेटवर्क में पहले परत के रूप में एम्बेडिंग परत का उपयोग करके, हम बैग-ऑफ-शब्द से **एम्बेडिंग बैग** मॉडल में स्विच कर सकते हैं, जहाँ हम पहले अपने पाठ में प्रत्येक शब्द को संबंधित एम्बेडिंग में परिवर्तित करते हैं, और फिर उन सभी एम्बेडिंग पर कुछ समग्र फ़ंक्शन की गणना करते हैं, जैसे कि `sum`, `average` या `max`।

![पांच अनुक्रम शब्दों के लिए एक एम्बेडिंग वर्गीकरणकर्ता दिखाने वाली छवि।](../../../../../translated_images/embedding-classifier-example.b77f021a7ee67eeec8e68bfe11636c5b97d6eaa067515a129bfb1d0034b1ac5b.hi.png)

> छवि लेखक द्वारा

## ✍️ अभ्यास: एम्बेडिंग

निम्नलिखित नोटबुक में अपने अध्ययन को जारी रखें:
* [PyTorch के साथ एम्बेडिंग](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb)
* [TensorFlow एम्बेडिंग](../../../../../lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)

## अर्थात्मक एम्बेडिंग: Word2Vec

जबकि एम्बेडिंग परत ने शब्दों को वेक्टर प्रतिनिधित्व में मैप करना सीखा, हालाँकि, यह प्रतिनिधित्व जरूरी नहीं कि बहुत अधिक अर्थात्मक अर्थ रखता हो। यह अच्छा होगा कि एक वेक्टर प्रतिनिधित्व सीखा जाए ताकि समान शब्द या पर्यायवाची वेक्टर एक-दूसरे के करीब हों किसी वेक्टर दूरी (जैसे, यूक्लिडियन दूरी) के संदर्भ में।

इसके लिए, हमें एक बड़े पाठ संग्रह पर अपने एम्बेडिंग मॉडल को एक विशिष्ट तरीके से पूर्व-प्रशिक्षित करना होगा। अर्थात्मक एम्बेडिंग को प्रशिक्षित करने का एक तरीका [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) कहलाता है। यह दो मुख्य आर्किटेक्चर पर आधारित है जो शब्दों के वितरित प्रतिनिधित्व उत्पन्न करने के लिए उपयोग किया जाता है:

 - **निरंतर बैग-ऑफ-शब्द** (CBoW) — इस आर्किटेक्चर में, हम मॉडल को चारों ओर के संदर्भ से एक शब्द की भविष्यवाणी करने के लिए प्रशिक्षित करते हैं। दिए गए ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, मॉडल का लक्ष्य $(W_{-2},W_{-1},W_1,W_2)$ से $W_0$ की भविष्यवाणी करना है।
 - **निरंतर स्किप-ग्राम** CBoW के विपरीत है। मॉडल वर्तमान शब्द की भविष्यवाणी करने के लिए चारों ओर के संदर्भ शब्दों की विंडो का उपयोग करता है।

CBoW तेज है, जबकि स्किप-ग्राम धीमा है, लेकिन कम सामान्य शब्दों का बेहतर प्रतिनिधित्व करता है।

![शब्दों को वेक्टर में परिवर्तित करने के लिए CBoW और स्किप-ग्राम एल्गोरिदम दोनों को दिखाने वाली छवि।](../../../../../translated_images/example-algorithms-for-converting-words-to-vectors.fbe9207a726922f6f0f5de66427e8a6eda63809356114e28fb1fa5f4a83ebda7.hi.png)

> छवि [इस पेपर](https://arxiv.org/pdf/1301.3781.pdf) से

Word2Vec पूर्व-प्रशिक्षित एम्बेडिंग (जैसे अन्य समान मॉडल, जैसे GloVe) को न्यूरल नेटवर्क में एम्बेडिंग परत के स्थान पर भी उपयोग किया जा सकता है। हालाँकि, हमें शब्दावली से निपटना होगा, क्योंकि Word2Vec/GloVe को पूर्व-प्रशिक्षित करने के लिए उपयोग की गई शब्दावली हमारे पाठ संग्रह में शब्दावली से भिन्न हो सकती है। इस समस्या को हल करने के लिए ऊपर दिए गए नोटबुक पर एक नज़र डालें।

## संदर्भात्मक एम्बेडिंग

पारंपरिक पूर्व-प्रशिक्षित एम्बेडिंग प्रतिनिधित्व जैसे Word2Vec की एक प्रमुख सीमा शब्द अर्थ अस्पष्टता की समस्या है। जबकि पूर्व-प्रशिक्षित एम्बेडिंग संदर्भ में शब्दों के कुछ अर्थ को पकड़ सकते हैं, किसी भी शब्द का हर संभव अर्थ उसी एम्बेडिंग में एन्कोड किया जाता है। यह डाउनस्ट्रीम मॉडलों में समस्याएँ पैदा कर सकता है, क्योंकि कई शब्द जैसे 'play' का अर्थ संदर्भ के अनुसार भिन्न होता है जिसमें उनका उपयोग किया जाता है।

उदाहरण के लिए, शब्द 'play' इन दो अलग-अलग वाक्यों में काफी भिन्न अर्थ रखता है:

- मैं थिएटर में एक **नाटक** में गया।
- जॉन अपने दोस्तों के साथ **खेलना** चाहता है।

उपरोक्त पूर्व-प्रशिक्षित एम्बेडिंग शब्द 'play' के इन दोनों अर्थों का प्रतिनिधित्व उसी एम्बेडिंग में करती है। इस सीमा को दूर करने के लिए, हमें **भाषा मॉडल** के आधार पर एम्बेडिंग बनानी होगी, जो एक बड़े पाठ संग्रह पर प्रशिक्षित है, और *जानता है* कि विभिन्न संदर्भों में शब्दों को कैसे एक साथ रखा जा सकता है। संदर्भात्मक एम्बेडिंग पर चर्चा इस ट्यूटोरियल के दायरे से बाहर है, लेकिन हम पाठ्यक्रम में बाद में भाषा मॉडलों के बारे में बात करते समय उनके पास वापस आएंगे।

## निष्कर्ष

इस पाठ में, आपने सीखा कि कैसे TensorFlow और Pytorch में एम्बेडिंग परतों का निर्माण और उपयोग किया जाए ताकि शब्दों के अर्थात्मक अर्थों को बेहतर ढंग से दर्शाया जा सके।

## 🚀 चुनौती

Word2Vec का उपयोग कुछ दिलचस्प अनुप्रयोगों के लिए किया गया है, जिसमें गीतों के बोल और कविता का निर्माण शामिल है। [इस लेख](https://www.politetype.com/blog/word2vec-color-poems) पर एक नज़र डालें जो बताता है कि लेखक ने Word2Vec का उपयोग करके कविता कैसे बनाई। [डैन शिफमैन द्वारा इस वीडियो](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain) को भी देखें ताकि इस तकनीक का एक अलग स्पष्टीकरण पता चल सके। फिर इन तकनीकों को अपने स्वयं के पाठ संग्रह पर लागू करने का प्रयास करें, जो शायद Kaggle से प्राप्त किया गया हो।

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/214)

## समीक्षा और आत्म अध्ययन

Word2Vec पर इस पेपर को पढ़ें: [वेक्टर स्पेस में शब्द प्रतिनिधित्व का कुशल अनुमान](https://arxiv.org/pdf/1301.3781.pdf)

## [असाइनमेंट: नोटबुक](assignment.md)

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान रखें कि स्वचालित अनुवाद में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्राधिकृत स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न होने वाली किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।