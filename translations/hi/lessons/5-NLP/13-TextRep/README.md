# पाठ को टेन्सर के रूप में प्रस्तुत करना

## [प्रस्तुतिकरण से पहले का क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113)

## पाठ वर्गीकरण

इस अनुभाग के पहले भाग में, हम **पाठ वर्गीकरण** कार्य पर ध्यान केंद्रित करेंगे। हम [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) डेटासेट का उपयोग करेंगे, जिसमें निम्नलिखित जैसे समाचार लेख शामिल हैं:

* श्रेणी: विज्ञान/तकनीक
* शीर्षक: केवाई। कंपनी ने पेप्टाइड्स का अध्ययन करने के लिए अनुदान जीता (एपी)
* सामग्री: एपी - लुइसविले विश्वविद्यालय के एक रसायन विज्ञान शोधकर्ता द्वारा स्थापित एक कंपनी ने विकास के लिए अनुदान जीता...

हमारा लक्ष्य समाचार सामग्री को पाठ के आधार पर श्रेणियों में वर्गीकृत करना होगा।

## पाठ का प्रतिनिधित्व करना

यदि हम न्यूरल नेटवर्क के साथ प्राकृतिक भाषा प्रसंस्करण (NLP) कार्यों को हल करना चाहते हैं, तो हमें पाठ को टेन्सर के रूप में प्रस्तुत करने का एक तरीका चाहिए। कंप्यूटर पहले से ही पाठ्य वर्णों को संख्याओं के रूप में प्रस्तुत करते हैं जो आपकी स्क्रीन पर फ़ॉन्ट के लिए मैप होते हैं, जैसे एएससीआईआई या यूटीएफ-8 जैसे एन्कोडिंग का उपयोग करके।

<img alt="एक वर्ण को एएससीआईआई और बाइनरी प्रतिनिधित्व में मैप करने वाले आरेख को दिखाने वाली छवि" src="images/ascii-character-map.png" width="50%"/>

> [छवि स्रोत](https://www.seobility.net/en/wiki/ASCII)

मनुष्यों के रूप में, हम समझते हैं कि प्रत्येक अक्षर **क्या दर्शाता है**, और सभी वर्ण कैसे एक साथ मिलकर वाक्य के शब्द बनाते हैं। हालाँकि, कंप्यूटर अपने आप में ऐसा कोई समझ नहीं रखते हैं, और न्यूरल नेटवर्क को प्रशिक्षण के दौरान अर्थ सीखना होता है।

इसलिए, हम पाठ का प्रतिनिधित्व करते समय विभिन्न दृष्टिकोणों का उपयोग कर सकते हैं:

* **वर्ण-स्तरीय प्रतिनिधित्व**, जब हम पाठ का प्रतिनिधित्व करते हैं और प्रत्येक वर्ण को एक संख्या के रूप में मानते हैं। मान लीजिए कि हमारे पाठ कॉर्पस में *C* विभिन्न वर्ण हैं, तो शब्द *Hello* को 5x*C* टेन्सर द्वारा प्रस्तुत किया जाएगा। प्रत्येक अक्षर एक-गर्मी एन्कोडिंग में एक टेन्सर कॉलम के रूप में होगा।
* **शब्द-स्तरीय प्रतिनिधित्व**, जिसमें हम अपने पाठ में सभी शब्दों का एक **शब्दावली** बनाते हैं, और फिर एक-गर्मी एन्कोडिंग का उपयोग करके शब्दों का प्रतिनिधित्व करते हैं। यह दृष्टिकोण कुछ हद तक बेहतर है, क्योंकि प्रत्येक अक्षर अपने आप में ज्यादा अर्थ नहीं रखता, और इस प्रकार उच्च-स्तरीय सेमांटिक अवधारणाओं - शब्दों - का उपयोग करके, हम न्यूरल नेटवर्क के लिए कार्य को सरल बनाते हैं। हालाँकि, बड़े शब्दकोश के आकार को देखते हुए, हमें उच्च-आयामी विरल टेन्सरों से निपटना होगा।

प्रतिनिधित्व की परवाह किए बिना, हमें पहले पाठ को **टोकन** की एक श्रृंखला में परिवर्तित करना होगा, जिसमें एक टोकन या तो एक वर्ण, एक शब्द, या कभी-कभी एक शब्द का हिस्सा होता है। फिर, हम टोकन को एक संख्या में परिवर्तित करते हैं, आमतौर पर **शब्दावली** का उपयोग करके, और यह संख्या एक-गर्मी एन्कोडिंग का उपयोग करके न्यूरल नेटवर्क में डाली जा सकती है।

## एन-ग्राम

प्राकृतिक भाषा में, शब्दों का सटीक अर्थ केवल संदर्भ में ही निर्धारित किया जा सकता है। उदाहरण के लिए, *न्यूरल नेटवर्क* और *फिशिंग नेटवर्क* के अर्थ पूरी तरह से अलग हैं। इसे ध्यान में रखने के लिए, एक तरीका यह है कि हम अपने मॉडल को शब्दों के जोड़ों पर बनाएं, और शब्द जोड़ों को अलग शब्दावली टोकन के रूप में मानें। इस तरह, वाक्य *I like to go fishing* को टोकनों की निम्नलिखित श्रृंखला द्वारा प्रस्तुत किया जाएगा: *I like*, *like to*, *to go*, *go fishing*। इस दृष्टिकोण की समस्या यह है कि शब्दकोश का आकार काफी बढ़ जाता है, और संयोजन जैसे *go fishing* और *go shopping* को अलग टोकनों द्वारा प्रस्तुत किया जाता है, जो समान क्रिया के बावजूद कोई सेमांटिक समानता साझा नहीं करते हैं।

कुछ मामलों में, हम त्रिग्राम का उपयोग करने पर विचार कर सकते हैं - तीन शब्दों के संयोजन - भी। इस प्रकार की दृष्टिकोण को अक्सर **n-grams** कहा जाता है। इसके अलावा, वर्ण-स्तरीय प्रतिनिधित्व के साथ n-grams का उपयोग करना समझ में आता है, जिसमें n-grams लगभग विभिन्न स्वर के अनुरूप होंगे।

## बैग-ऑफ-शब्द और TF/IDF

पाठ वर्गीकरण जैसे कार्यों को हल करते समय, हमें पाठ का एक निश्चित आकार के वेक्टर के रूप में प्रतिनिधित्व करने में सक्षम होना चाहिए, जिसका उपयोग हम अंतिम घनत्व वर्गीकरणकर्ता के लिए इनपुट के रूप में करेंगे। ऐसा करने के सबसे सरल तरीकों में से एक सभी व्यक्तिगत शब्द प्रतिनिधित्वों को संयोजित करना है, जैसे कि उन्हें जोड़ना। यदि हम प्रत्येक शब्द के एक-गर्मी एन्कोडिंग को जोड़ते हैं, तो हम आवृत्तियों का एक वेक्टर प्राप्त करेंगे, जो दिखाएगा कि प्रत्येक शब्द पाठ में कितनी बार आता है। पाठ का ऐसा प्रतिनिधित्व **बैग ऑफ वर्ड्स** (BoW) कहलाता है।

<img src="images/bow.png" width="90%"/>

> छवि लेखक द्वारा

BoW मूल रूप से यह दर्शाता है कि कौन से शब्द पाठ में और कितनी मात्रा में आते हैं, जो वास्तव में यह बताने का एक अच्छा संकेत हो सकता है कि पाठ किस बारे में है। उदाहरण के लिए, राजनीति पर एक समाचार लेख में *president* और *country* जैसे शब्द हो सकते हैं, जबकि वैज्ञानिक प्रकाशन में *collider*, *discovered* आदि जैसे शब्द होंगे। इस प्रकार, शब्द आवृत्तियाँ कई मामलों में पाठ की सामग्री का एक अच्छा संकेतक हो सकती हैं।

BoW की समस्या यह है कि कुछ सामान्य शब्द, जैसे *and*, *is*, आदि अधिकांश पाठों में आते हैं, और उनकी उच्चतम आवृत्तियाँ होती हैं, जो वास्तव में महत्वपूर्ण शब्दों को छुपा देती हैं। हम इन शब्दों के महत्व को इस बात पर विचार करके कम कर सकते हैं कि शब्द पूरे दस्तावेज़ संग्रह में कितनी बार आते हैं। यह TF/IDF दृष्टिकोण के पीछे का मुख्य विचार है, जिसे इस पाठ के साथ संलग्न नोटबुक में अधिक विस्तार से कवर किया गया है।

हालांकि, इनमें से कोई भी दृष्टिकोण पाठ की **सेमांटिक्स** को पूरी तरह से ध्यान में नहीं रख सकता है। हमें ऐसा अधिक शक्तिशाली न्यूरल नेटवर्क मॉडल की आवश्यकता है जो ऐसा कर सके, जिसके बारे में हम इस अनुभाग में बाद में चर्चा करेंगे।

## ✍️ व्यायाम: पाठ प्रतिनिधित्व

नीचे दिए गए नोटबुक में अपनी शिक्षा जारी रखें:

* [PyTorch के साथ पाठ प्रतिनिधित्व](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb)
* [TensorFlow के साथ पाठ प्रतिनिधित्व](../../../../../lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)

## निष्कर्ष

अब तक, हमने विभिन्न शब्दों को आवृत्ति वजन जोड़ने की तकनीकों का अध्ययन किया है। हालाँकि, वे अर्थ या क्रम का प्रतिनिधित्व करने में असमर्थ हैं। जैसा कि प्रसिद्ध भाषाविद् जे. आर. फर्थ ने 1935 में कहा था, "एक शब्द का पूर्ण अर्थ हमेशा संदर्भित होता है, और संदर्भ के अलावा अर्थ का कोई अध्ययन गंभीरता से नहीं लिया जा सकता।" हम बाद में पाठ को भाषा मॉडलिंग का उपयोग करके संदर्भ संबंधी जानकारी कैसे कैप्चर करें, यह सीखेंगे।

## 🚀 चुनौती

बैग-ऑफ-शब्द और विभिन्न डेटा मॉडलों का उपयोग करके कुछ अन्य व्यायाम करने का प्रयास करें। आप इस [कग्गल प्रतियोगिता](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) से प्रेरित हो सकते हैं।

## [प्रस्तुतिकरण के बाद का क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213)

## समीक्षा और आत्म-अध्ययन

[Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) पर पाठ एम्बेडिंग और बैग-ऑफ-शब्द तकनीकों के साथ अपने कौशल का अभ्यास करें।

## [असाइनमेंट: नोटबुक](assignment.md)

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान रखें कि स्वचालित अनुवादों में त्रुटियाँ या असंगतियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्राधिकारिक स्रोत के रूप में माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। हम इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए जिम्मेदार नहीं हैं।