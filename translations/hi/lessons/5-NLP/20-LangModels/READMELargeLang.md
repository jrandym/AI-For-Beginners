# पूर्व-प्रशिक्षित बड़े भाषा मॉडल

हमारे सभी पिछले कार्यों में, हम एक न्यूरल नेटवर्क को एक निश्चित कार्य करने के लिए लेबल किए गए डेटा सेट का उपयोग करके प्रशिक्षित कर रहे थे। बड़े ट्रांसफार्मर मॉडलों, जैसे कि BERT, के साथ, हम एक भाषा मॉडल बनाने के लिए आत्म-निगरानी तरीके से भाषा मॉडलिंग का उपयोग करते हैं, जिसे फिर विशेष डाउनस्ट्रीम कार्य के लिए आगे के डोमेन-विशिष्ट प्रशिक्षण के साथ विशेषीकृत किया जाता है। हालाँकि, यह प्रदर्शित किया गया है कि बड़े भाषा मॉडल बिना किसी डोमेन-विशिष्ट प्रशिक्षण के भी कई कार्यों को हल कर सकते हैं। ऐसे कार्य करने में सक्षम मॉडलों का एक परिवार **GPT** कहलाता है: जनरेटिव प्री-ट्रेंड ट्रांसफार्मर।

## [प्री-व्याख्यान क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/120)

## पाठ उत्पन्न करना और पेरीप्लेक्सिटी

एक न्यूरल नेटवर्क द्वारा बिना डाउनस्ट्रीम प्रशिक्षण के सामान्य कार्य करने की क्षमता का विचार [भाषा मॉडल बिना पर्यवेक्षित बहु-कार्यकर्ता](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) पेपर में प्रस्तुत किया गया है। मुख्य विचार यह है कि कई अन्य कार्यों को **पाठ उत्पन्न करने** का उपयोग करके मॉडल किया जा सकता है, क्योंकि पाठ को समझना मूल रूप से इसे उत्पन्न करने में सक्षम होना है। चूंकि मॉडल को मानव ज्ञान को समाहित करने वाले विशाल मात्रा के पाठ पर प्रशिक्षित किया गया है, यह विभिन्न विषयों के बारे में भी जानकार हो जाता है।

> पाठ को समझना और उसे उत्पन्न करने में सक्षम होना हमारे चारों ओर की दुनिया के बारे में कुछ जानने की आवश्यकता भी रखता है। लोग भी पढ़कर बड़े पैमाने पर सीखते हैं, और GPT नेटवर्क इस संदर्भ में समान है।

पाठ उत्पन्न करने वाले नेटवर्क अगली शब्द की संभावना $$P(w_N)$$ की भविष्यवाणी करके काम करते हैं। हालाँकि, अगली शब्द की बिना शर्त संभावना इस शब्द की पाठ कॉर्पस में आवृत्ति के बराबर होती है। GPT हमें पिछले शब्दों को देखते हुए अगली शब्द की **शर्तीय संभावना** प्रदान करने में सक्षम है: $$P(w_N | w_{n-1}, ..., w_0)$$

> आप हमारी [डेटा विज्ञान के लिए प्रारंभिक पाठ्यक्रम](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/1-Introduction/04-stats-and-probability) में संभावनाओं के बारे में अधिक पढ़ सकते हैं।

भाषा उत्पन्न करने वाले मॉडल की गुणवत्ता को **पेरीप्लेक्सिटी** के माध्यम से परिभाषित किया जा सकता है। यह एक अंतर्निहित मीट्रिक है जो हमें किसी कार्य-विशिष्ट डेटा सेट के बिना मॉडल की गुणवत्ता को मापने की अनुमति देती है। यह *वाक्य की संभावना* के विचार पर आधारित है - मॉडल एक वाक्य को उच्च संभावना प्रदान करता है जो वास्तविक होने की संभावना है (यानी, मॉडल इससे **पेरीप्लेक्स** नहीं है), और कम समझ में आने वाले वाक्यों को कम संभावना प्रदान करता है (जैसे, *क्या यह क्या कर सकता है?*). जब हम अपने मॉडल को वास्तविक पाठ कॉर्पस से वाक्य देते हैं, तो हम उम्मीद करते हैं कि उनकी उच्च संभावना और कम **पेरीप्लेक्सिटी** होनी चाहिए। गणितीय रूप से, इसे परीक्षण सेट की सामान्यीकृत विपरीत संभावना के रूप में परिभाषित किया जाता है:
$$
\mathrm{Perplexity}(W) = \sqrt[N]{1\over P(W_1,...,W_N)}
$$ 

**आप [Hugging Face से GPT-संचालित पाठ संपादक](https://transformer.huggingface.co/doc/gpt2-large) का उपयोग करके पाठ उत्पन्न करने के साथ प्रयोग कर सकते हैं।** इस संपादक में, आप अपना पाठ लिखना शुरू करते हैं, और **[TAB]** दबाने से आपको कई पूर्णता विकल्प मिलते हैं। यदि वे बहुत छोटे हैं, या आप उनसे संतुष्ट नहीं हैं - फिर से [TAB] दबाएं, और आपको अधिक विकल्प मिलेंगे, जिनमें लंबे पाठ के टुकड़े भी शामिल हैं।

## GPT एक परिवार है

GPT एकल मॉडल नहीं है, बल्कि [OpenAI](https://openai.com) द्वारा विकसित और प्रशिक्षित मॉडलों का एक संग्रह है।

GPT मॉडलों के अंतर्गत, हमारे पास हैं:

| [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2#openai-gpt2) | [GPT 3](https://openai.com/research/language-models-are-few-shot-learners) | [GPT-4](https://openai.com/gpt-4) |
| -- | -- | -- |
|1.5 अरब तक के पैरामीटर वाला भाषा मॉडल। | 175 अरब तक के पैरामीटर वाला भाषा मॉडल | 100T पैरामीटर और छवि और पाठ दोनों इनपुट स्वीकार करता है और पाठ आउटपुट करता है। |

GPT-3 और GPT-4 मॉडल [Microsoft Azure से एक संज्ञानात्मक सेवा के रूप में](https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/#overview?WT.mc_id=academic-77998-cacaste) उपलब्ध हैं, और [OpenAI API](https://openai.com/api/) के रूप में भी।

## प्रॉम्प्ट इंजीनियरिंग

चूंकि GPT को भाषा और कोड को समझने के लिए विशाल मात्रा में डेटा पर प्रशिक्षित किया गया है, वे इनपुट (प्रॉम्प्ट) के जवाब में आउटपुट प्रदान करते हैं। प्रॉम्प्ट GPT इनपुट या क्वेरी होते हैं जिनके माध्यम से कोई मॉडल को कार्यों पर निर्देश प्रदान करता है जिन्हें उन्होंने अगले पूर्ण किया। इच्छित परिणाम प्राप्त करने के लिए, आपको सबसे प्रभावी प्रॉम्प्ट की आवश्यकता होती है जिसमें सही शब्दों, प्रारूपों, वाक्यांशों या यहां तक कि प्रतीकों का चयन करना शामिल होता है। यह दृष्टिकोण [प्रॉम्प्ट इंजीनियरिंग](https://learn.microsoft.com/en-us/shows/ai-show/the-basics-of-prompt-engineering-with-azure-openai-service?WT.mc_id=academic-77998-bethanycheum) कहलाता है।

[यह दस्तावेज़](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/?WT.mc_id=academic-77998-bethanycheum) आपको प्रॉम्प्ट इंजीनियरिंग के बारे में अधिक जानकारी प्रदान करता है।

## ✍️ उदाहरण नोटबुक: [OpenAI-GPT के साथ खेलना](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

निम्नलिखित नोटबुक में अपने अध्ययन को जारी रखें:

* [OpenAI-GPT और Hugging Face Transformers के साथ पाठ उत्पन्न करना](../../../../../lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb)

## निष्कर्ष

नए सामान्य पूर्व-प्रशिक्षित भाषा मॉडल न केवल भाषा संरचना को मॉडल करते हैं, बल्कि प्राकृतिक भाषा की विशाल मात्रा भी समाहित करते हैं। इसलिए, उन्हें शून्य-शॉप या कुछ-शॉट सेटिंग्स में कुछ NLP कार्यों को हल करने के लिए प्रभावी ढंग से उपयोग किया जा सकता है।

## [पोस्ट-व्याख्यान क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/220)

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या असंगतताएँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्राधिकृत स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।