# जनरेटिव नेटवर्क

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/117)

रिपीटिव न्यूरल नेटवर्क (RNNs) और उनके गेटेड सेल वेरिएंट जैसे लॉन्ग शॉर्ट टर्म मेमोरी सेल (LSTMs) और गेटेड रिपीटिव यूनिट्स (GRUs) ने भाषा मॉडलिंग के लिए एक तंत्र प्रदान किया है, क्योंकि वे शब्दों की क्रमबद्धता सीख सकते हैं और अनुक्रम में अगले शब्द के लिए भविष्यवाणियाँ कर सकते हैं। इससे हमें RNNs का उपयोग **जनरेटिव कार्यों** के लिए करने की अनुमति मिलती है, जैसे सामान्य टेक्स्ट जनरेशन, मशीन अनुवाद, और यहां तक कि छवि कैप्शनिंग।

> ✅ उन सभी बार के बारे में सोचें जब आपने टेक्स्ट पूर्णता जैसे जनरेटिव कार्यों से लाभ उठाया है। अपने पसंदीदा अनुप्रयोगों पर कुछ शोध करें कि क्या उन्होंने RNNs का लाभ उठाया है।

हमने पिछले यूनिट में जो RNN आर्किटेक्चर चर्चा की थी, उसमें प्रत्येक RNN यूनिट ने अगले हिडन स्टेट को आउटपुट के रूप में उत्पन्न किया। हालाँकि, हम प्रत्येक पुनरावर्ती यूनिट में एक और आउटपुट जोड़ सकते हैं, जिससे हमें एक **अनुक्रम** आउटपुट करने की अनुमति मिलेगी (जो मूल अनुक्रम के समान लंबाई का होता है)। इसके अलावा, हम RNN यूनिट्स का उपयोग कर सकते हैं जो प्रत्येक चरण में इनपुट स्वीकार नहीं करते हैं, और बस कुछ प्रारंभिक स्टेट वेक्टर लेते हैं, और फिर आउटपुट के अनुक्रम का उत्पादन करते हैं।

यह नीचे चित्र में दिखाए गए विभिन्न न्यूरल आर्किटेक्चर की अनुमति देता है:

![सामान्य पुनरावर्ती न्यूरल नेटवर्क पैटर्न दिखाने वाली छवि।](../../../../../translated_images/unreasonable-effectiveness-of-rnn.541ead816778f42dce6c42d8a56c184729aa2378d059b851be4ce12b993033df.hi.jpg)

> छवि ब्लॉग पोस्ट [रिपीटिव न्यूरल नेटवर्क्स की असंगत प्रभावशीलता](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) द्वारा [Andrej Karpaty](http://karpathy.github.io/)

* **एक-से-एक** एक पारंपरिक न्यूरल नेटवर्क है जिसमें एक इनपुट और एक आउटपुट होता है।
* **एक-से-बहुत** एक जनरेटिव आर्किटेक्चर है जो एक इनपुट मान को स्वीकार करता है, और आउटपुट मानों के अनुक्रम का उत्पादन करता है। उदाहरण के लिए, यदि हम एक **छवि कैप्शनिंग** नेटवर्क को प्रशिक्षित करना चाहते हैं जो एक चित्र का टेक्स्ट विवरण उत्पन्न करेगा, तो हम एक चित्र को इनपुट के रूप में ले सकते हैं, इसे एक CNN के माध्यम से पास कर सकते हैं ताकि इसका हिडन स्टेट प्राप्त किया जा सके, और फिर एक पुनरावर्ती श्रृंखला को शब्द-दर-शब्द कैप्शन उत्पन्न करने के लिए रख सकते हैं।
* **बहुत-से-एक** पिछले यूनिट में वर्णित RNN आर्किटेक्चर से संबंधित है, जैसे कि टेक्स्ट वर्गीकरण।
* **बहुत-से-बहुत**, या **अनुक्रम-से-अनुक्रम** उन कार्यों से संबंधित है जैसे **मशीन अनुवाद**, जहां पहला RNN इनपुट अनुक्रम से सभी जानकारी को हिडन स्टेट में इकट्ठा करता है, और दूसरा RNN श्रृंखला इस स्टेट को आउटपुट अनुक्रम में अनरोल करती है।

इस यूनिट में, हम सरल जनरेटिव मॉडलों पर ध्यान केंद्रित करेंगे जो हमें टेक्स्ट उत्पन्न करने में मदद करते हैं। सरलता के लिए, हम वर्ण-स्तरीय टोकनाइजेशन का उपयोग करेंगे।

हम इस RNN को चरण-दर-चरण टेक्स्ट उत्पन्न करने के लिए प्रशिक्षित करेंगे। प्रत्येक चरण में, हम `nchars` लंबाई के वर्णों के अनुक्रम को लेंगे, और नेटवर्क से प्रत्येक इनपुट वर्ण के लिए अगला आउटपुट वर्ण उत्पन्न करने के लिए कहेंगे:

![शब्द 'HELLO' के RNN जनरेशन का उदाहरण दिखाने वाली छवि।](../../../../../translated_images/rnn-generate.56c54afb52f9781d63a7c16ea9c1b86cb70e6e1eae6a742b56b7b37468576b17.hi.png)

जब टेक्स्ट उत्पन्न करते हैं (इनफेरेंस के दौरान), हम कुछ **प्रॉम्प्ट** से शुरू करते हैं, जिसे RNN सेल्स के माध्यम से पास किया जाता है ताकि इसका मध्यवर्ती स्टेट उत्पन्न किया जा सके, और फिर इस स्टेट से जनरेशन शुरू होती है। हम एक समय में एक वर्ण उत्पन्न करते हैं, और स्टेट और उत्पन्न वर्ण को अगले के लिए एक और RNN सेल में पास करते हैं, जब तक कि हम पर्याप्त वर्ण उत्पन्न नहीं कर लेते।

<img src="images/rnn-generate-inf.png" width="60%"/>

> लेखक द्वारा छवि

## ✍️ अभ्यास: जनरेटिव नेटवर्क

निम्नलिखित नोटबुक में अपने ज्ञान को जारी रखें:

* [PyTorch के साथ जनरेटिव नेटवर्क](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb)
* [TensorFlow के साथ जनरेटिव नेटवर्क](../../../../../lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)

## नरम टेक्स्ट जनरेशन और तापमान

प्रत्येक RNN सेल का आउटपुट वर्णों का एक संभावना वितरण होता है। यदि हम हमेशा उत्पन्न टेक्स्ट में अगले वर्ण के रूप में सबसे उच्च संभावना वाले वर्ण को लेते हैं, तो टेक्स्ट अक्सर एक ही वर्ण अनुक्रमों के बीच "चक्रित" हो सकता है, जैसे कि इस उदाहरण में:

```
today of the second the company and a second the company ...
```

हालांकि, यदि हम अगले वर्ण के लिए संभावना वितरण को देखते हैं, तो यह हो सकता है कि कुछ उच्चतम संभावनाओं के बीच का अंतर बहुत बड़ा न हो, जैसे कि एक वर्ण की संभावना 0.2 हो, दूसरे की 0.19, आदि। उदाहरण के लिए, जब '*play*' अनुक्रम में अगले वर्ण की खोज करते हैं, तो अगला वर्ण समान रूप से स्पेस या **e** (जैसे *player* शब्द में) हो सकता है।

इससे हमें यह निष्कर्ष निकलता है कि हमेशा उच्च संभावना वाले वर्ण का चयन करना "न्यायसंगत" नहीं है, क्योंकि दूसरे उच्चतम का चयन करना भी हमें अर्थपूर्ण टेक्स्ट की ओर ले जा सकता है। नेटवर्क आउटपुट द्वारा दिए गए संभावना वितरण से वर्णों को **नमूना** लेना अधिक बुद्धिमानी है। हम एक पैरामीटर, **तापमान**, का भी उपयोग कर सकते हैं, जो संभावना वितरण को समतल करेगा, यदि हम अधिक यादृच्छिकता जोड़ना चाहते हैं, या इसे अधिक तेज़ बनाएगा, यदि हम उच्चतम-प्रवृत्ति वाले वर्णों के प्रति अधिक प्रतिबद्ध रहना चाहते हैं।

देखें कि यह नरम टेक्स्ट जनरेशन उपरोक्त लिंक किए गए नोटबुक में कैसे लागू किया गया है।

## निष्कर्ष

हालांकि टेक्स्ट जनरेशन अपने आप में उपयोगी हो सकता है, मुख्य लाभ तब आता है जब हम कुछ प्रारंभिक फीचर वेक्टर से RNNs का उपयोग करके टेक्स्ट उत्पन्न कर सकते हैं। उदाहरण के लिए, टेक्स्ट जनरेशन मशीन अनुवाद का एक हिस्सा है (अनुक्रम-से-अनुक्रम, इस मामले में *encoder* से स्टेट वेक्टर का उपयोग किया जाता है ताकि अनुवादित संदेश उत्पन्न या *decode* किया जा सके), या छवि का टेक्स्ट विवरण उत्पन्न करना (जिसमें फीचर वेक्टर CNN एक्सट्रैक्टर से आता है)।

## 🚀 चुनौती

इस विषय पर Microsoft Learn पर कुछ पाठ लें

* टेक्स्ट जनरेशन [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/6-generative-networks/?WT.mc_id=academic-77998-cacaste)/[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/5-generative-networks/?WT.mc_id=academic-77998-cacaste)

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/217)

## समीक्षा और आत्म-अध्ययन

यहां कुछ लेख दिए गए हैं जो आपके ज्ञान को बढ़ाने के लिए हैं

* मार्कोव चेन, LSTM और GPT-2 के साथ टेक्स्ट जनरेशन के विभिन्न दृष्टिकोण: [ब्लॉग पोस्ट](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e)
* [Keras दस्तावेज़ में टेक्स्ट जनरेशन का उदाहरण](https://keras.io/examples/generative/lstm_character_level_text_generation/)

## [असाइनमेंट](lab/README.md)

हमने वर्ण-दर-वर्ण टेक्स्ट उत्पन्न करना देखा है। प्रयोगशाला में, आप शब्द-स्तरीय टेक्स्ट जनरेशन का अन्वेषण करेंगे।

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्राधिकृत स्रोत के रूप में माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। हम इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए उत्तरदायी नहीं हैं।