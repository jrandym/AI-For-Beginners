# प्राकृतिक भाषा प्रसंस्करण

![NLP कार्यों का सारांश एक डूडल में](../../../../translated_images/ai-nlp.b22dcb8ca4707ceaee8576db1c5f4089c8cac2f454e9e03ea554f07fda4556b8.hi.png)

इस अनुभाग में, हम **प्राकृतिक भाषा प्रसंस्करण (NLP)** से संबंधित कार्यों को संभालने के लिए न्यूरल नेटवर्क का उपयोग करने पर ध्यान केंद्रित करेंगे। कई NLP समस्याएँ हैं जिन्हें हम चाहते हैं कि कंप्यूटर हल कर सकें:

* **पाठ वर्गीकरण** एक विशिष्ट वर्गीकरण समस्या है जो पाठ अनुक्रमों से संबंधित है। उदाहरण के लिए, ई-मेल संदेशों को स्पैम बनाम नॉन-स्पैम के रूप में वर्गीकृत करना, या लेखों को खेल, व्यवसाय, राजनीति आदि के रूप में श्रेणीबद्ध करना। इसके अलावा, जब हम चैट बॉट विकसित करते हैं, तो हमें अक्सर यह समझने की आवश्यकता होती है कि उपयोगकर्ता क्या कहना चाहता था - इस मामले में हम **इरादे वर्गीकरण** से निपट रहे हैं। अक्सर, इरादे वर्गीकरण में हमें कई श्रेणियों से निपटना पड़ता है।
* **भावना विश्लेषण** एक विशिष्ट प्रतिगमन समस्या है, जहां हमें एक संख्या (एक भावना) को यह निर्धारित करने के लिए आवंटित करना होता है कि एक वाक्य का अर्थ कितना सकारात्मक/नकारात्मक है। भावना विश्लेषण का एक अधिक उन्नत संस्करण है **आस्पेक्ट-आधारित भावना विश्लेषण** (ABSA), जहां हम भावना को पूरे वाक्य में नहीं, बल्कि इसके विभिन्न भागों (आस्पेक्ट्स) में आवंटित करते हैं, जैसे कि *इस रेस्तरां में, मुझे भोजन पसंद आया, लेकिन माहौल भयानक था*।
* **नामित इकाई पहचान** (NER) पाठ से कुछ इकाइयों को निकालने की समस्या को संदर्भित करता है। उदाहरण के लिए, हमें समझने की आवश्यकता हो सकती है कि वाक्यांश *मुझे कल पेरिस उड़ान भरनी है* में शब्द *कल* DATE को संदर्भित करता है, और *पेरिस* एक LOCATION है।  
* **कीवर्ड निष्कर्षण** NER के समान है, लेकिन हमें वाक्य के अर्थ के लिए महत्वपूर्ण शब्दों को स्वचालित रूप से निकालना होता है, बिना विशिष्ट इकाई प्रकारों के लिए पूर्व-प्रशिक्षण के।
* **पाठ क्लस्टरिंग** तब उपयोगी हो सकती है जब हम समान वाक्यों को एक साथ समूहित करना चाहते हैं, उदाहरण के लिए, तकनीकी सहायता वार्तालापों में समान अनुरोध।
* **प्रश्न उत्तर** एक मॉडल की एक विशिष्ट प्रश्न का उत्तर देने की क्षमता को संदर्भित करता है। मॉडल एक पाठ पासेज और एक प्रश्न को इनपुट के रूप में प्राप्त करता है, और इसे पाठ में उस स्थान को प्रदान करना होता है जहां प्रश्न का उत्तर निहित है (या, कभी-कभी, उत्तर पाठ उत्पन्न करना होता है)।
* **पाठ उत्पन्न करना** एक मॉडल की नई पाठ उत्पन्न करने की क्षमता है। इसे एक वर्गीकरण कार्य के रूप में माना जा सकता है जो कुछ *पाठ प्रॉम्प्ट* के आधार पर अगला अक्षर/शब्द पूर्वानुमान करता है। उन्नत पाठ उत्पन्न करने वाले मॉडल, जैसे GPT-3, अन्य NLP कार्यों को हल करने में सक्षम होते हैं जैसे कि वर्गीकरण, एक तकनीक का उपयोग करके जिसे [प्रॉम्प्ट प्रोग्रामिंग](https://towardsdatascience.com/software-3-0-how-prompting-will-change-the-rules-of-the-game-a982fbfe1e0) या [प्रॉम्प्ट इंजीनियरिंग](https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29) कहा जाता है।
* **पाठ संक्षेपण** एक तकनीक है जब हम चाहते हैं कि एक कंप्यूटर लंबे पाठ को "पढ़े" और उसे कुछ वाक्यों में संक्षेपित करे।
* **मशीन अनुवाद** को एक भाषा में पाठ की समझ और दूसरी में पाठ उत्पन्न करने के संयोजन के रूप में देखा जा सकता है।

शुरुआत में, अधिकांश NLP कार्य पारंपरिक विधियों जैसे व्याकरण का उपयोग करके हल किए गए थे। उदाहरण के लिए, मशीन अनुवाद में पार्सर का उपयोग प्रारंभिक वाक्य को एक सिंटैक्स ट्री में बदलने के लिए किया गया, फिर उच्च स्तर की अर्थ संबंधी संरचनाएँ निकाली गईं ताकि वाक्य के अर्थ का प्रतिनिधित्व किया जा सके, और इस अर्थ और लक्षित भाषा के व्याकरण के आधार पर परिणाम उत्पन्न किया गया। आजकल, कई NLP कार्यों को न्यूरल नेटवर्क का उपयोग करके अधिक प्रभावी ढंग से हल किया जाता है।

> कई शास्त्रीय NLP विधियाँ [प्राकृतिक भाषा प्रसंस्करण टूलकिट (NLTK)](https://www.nltk.org) पायथन पुस्तकालय में लागू की गई हैं। एक शानदार [NLTK पुस्तक](https://www.nltk.org/book/) ऑनलाइन उपलब्ध है जो बताती है कि विभिन्न NLP कार्यों को NLTK का उपयोग करके कैसे हल किया जा सकता है।

हमारे पाठ्यक्रम में, हम मुख्य रूप से NLP के लिए न्यूरल नेटवर्क के उपयोग पर ध्यान केंद्रित करेंगे, और जहाँ आवश्यकता होगी वहाँ NLTK का उपयोग करेंगे।

हम पहले ही तालिका डेटा और छवियों के साथ निपटने के लिए न्यूरल नेटवर्क के उपयोग के बारे में सीख चुके हैं। उन प्रकार के डेटा और पाठ के बीच मुख्य अंतर यह है कि पाठ एक परिवर्तनशील लंबाई का अनुक्रम है, जबकि छवियों के मामले में इनपुट आकार पहले से ज्ञात होता है। जबकि संयोजक नेटवर्क इनपुट डेटा से पैटर्न निकाल सकते हैं, पाठ में पैटर्न अधिक जटिल होते हैं। उदाहरण के लिए, नकारात्मकता को कई शब्दों के लिए विषय से अलग किया जा सकता है (जैसे *मुझे संतरे पसंद नहीं हैं*, बनाम *मुझे उन बड़े रंगीन स्वादिष्ट संतरों से नफरत है*), और इसे अभी भी एक पैटर्न के रूप में व्याख्या किया जाना चाहिए। इसलिए, भाषा को संभालने के लिए हमें नए न्यूरल नेटवर्क प्रकारों को पेश करने की आवश्यकता है, जैसे कि *पुनरावृत्त नेटवर्क* और *ट्रांसफार्मर*।

## पुस्तकालय स्थापित करें

यदि आप इस पाठ्यक्रम को चलाने के लिए स्थानीय पायथन स्थापना का उपयोग कर रहे हैं, तो आपको निम्नलिखित कमांड का उपयोग करके NLP के लिए सभी आवश्यक पुस्तकालय स्थापित करने की आवश्यकता हो सकती है:

**PyTorch के लिए**
```bash
pip install -r requirements-torch.txt
```
**TensorFlow के लिए**
```bash
pip install -r requirements-tf.txt
```

> आप [Microsoft Learn](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-tensorflow/?WT.mc_id=academic-77998-cacaste) पर TensorFlow के साथ NLP का प्रयास कर सकते हैं।

## GPU चेतावनी

इस अनुभाग में, कुछ उदाहरणों में हम काफी बड़े मॉडल को प्रशिक्षित कर रहे होंगे।
* **GPU-सक्षम कंप्यूटर का उपयोग करें**: बड़े मॉडलों के साथ काम करते समय प्रतीक्षा समय को कम करने के लिए अपने नोटबुक को GPU-सक्षम कंप्यूटर पर चलाना उचित है।
* **GPU मेमोरी की सीमाएँ**: GPU पर चलाने से ऐसी स्थिति उत्पन्न हो सकती है जहाँ आपके GPU मेमोरी की कमी हो जाती है, विशेष रूप से बड़े मॉडलों को प्रशिक्षित करते समय।
* **GPU मेमोरी खपत**: प्रशिक्षण के दौरान GPU मेमोरी की मात्रा विभिन्न कारकों पर निर्भर करती है, जिसमें मिनीबैच आकार शामिल है।
* **मिनीबैच आकार को कम करें**: यदि आप GPU मेमोरी की समस्याओं का सामना कर रहे हैं, तो संभावित समाधान के रूप में अपने कोड में मिनीबैच आकार को कम करने पर विचार करें।
* **TensorFlow GPU मेमोरी रिलीज़**: TensorFlow के पुराने संस्करण एक ही पायथन कर्नेल के भीतर कई मॉडलों को प्रशिक्षित करते समय GPU मेमोरी को सही तरीके से रिलीज़ नहीं कर सकते। GPU मेमोरी उपयोग को प्रभावी ढंग से प्रबंधित करने के लिए, आप TensorFlow को केवल आवश्यकता पड़ने पर GPU मेमोरी आवंटित करने के लिए कॉन्फ़िगर कर सकते हैं।
* **कोड समावेश**: यदि आप TensorFlow को केवल आवश्यक होने पर GPU मेमोरी आवंटन बढ़ाने के लिए सेट करना चाहते हैं, तो अपने नोटबुक में निम्नलिखित कोड शामिल करें:

```python
physical_devices = tf.config.list_physical_devices('GPU') 
if len(physical_devices)>0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True) 
```

यदि आप क्लासिक ML दृष्टिकोण से NLP के बारे में सीखने में रुचि रखते हैं, तो [इस पाठों के समूह](https://github.com/microsoft/ML-For-Beginners/tree/main/6-NLP) पर जाएँ।

## इस अनुभाग में
इस अनुभाग में हम सीखेंगे:

* [पाठ को टेन्सर के रूप में प्रस्तुत करना](13-TextRep/README.md)
* [शब्द एम्बेडिंग](14-Emdeddings/README.md)
* [भाषा मॉडलिंग](15-LanguageModeling/README.md)
* [पुनरावृत्त न्यूरल नेटवर्क](16-RNN/README.md)
* [जनरेटिव नेटवर्क](17-GenerativeNetworks/README.md)
* [ट्रांसफार्मर](18-Transformers/README.md)

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मातृ भाषा में प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। हम इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए जिम्मेदार नहीं हैं।