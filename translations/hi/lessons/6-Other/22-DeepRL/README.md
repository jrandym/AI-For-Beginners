# डीप रिइंफोर्समेंट लर्निंग

रिइंफोर्समेंट लर्निंग (RL) को मशीन लर्निंग के मूल सिद्धांतों में से एक माना जाता है, जो सुपरवाइज्ड लर्निंग और अनसुपरवाइज्ड लर्निंग के साथ है। जबकि सुपरवाइज्ड लर्निंग में हम ज्ञात परिणामों के साथ डेटासेट पर निर्भर करते हैं, RL **करते हुए सीखने** पर आधारित है। उदाहरण के लिए, जब हम पहली बार एक कंप्यूटर गेम देखते हैं, तो हम खेलना शुरू कर देते हैं, भले ही हमें नियमों का पता न हो, और जल्द ही हम खेलते रहने और अपने व्यवहार को समायोजित करने के द्वारा अपनी क्षमताओं में सुधार करने में सक्षम हो जाते हैं।

## [प्री-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

RL करने के लिए, हमें निम्नलिखित की आवश्यकता है:

* एक **पर्यावरण** या **सिम्युलेटर** जो खेल के नियम निर्धारित करता है। हमें सिम्युलेटर में प्रयोग चलाने और परिणामों का अवलोकन करने में सक्षम होना चाहिए।
* कुछ **इनाम फ़ंक्शन**, जो यह दर्शाता है कि हमारा प्रयोग कितना सफल रहा। कंप्यूटर गेम खेलने के मामले में, इनाम हमारा अंतिम स्कोर होगा।

इनाम फ़ंक्शन के आधार पर, हमें अपने व्यवहार को समायोजित करने और अपनी क्षमताओं में सुधार करने में सक्षम होना चाहिए, ताकि अगली बार हम बेहतर खेल सकें। मशीन लर्निंग के अन्य प्रकारों और RL के बीच मुख्य अंतर यह है कि RL में आमतौर पर हमें यह नहीं पता होता है कि हम जीतते हैं या हारते हैं जब तक कि हम खेल खत्म नहीं कर लेते। इसलिए, हम यह नहीं कह सकते कि कोई निश्चित चाल अकेले अच्छी है या नहीं - हमें केवल खेल के अंत में एक इनाम मिलता है।

RL के दौरान, हम आमतौर पर कई प्रयोग करते हैं। प्रत्येक प्रयोग के दौरान, हमें अब तक सीखी गई सर्वोत्तम रणनीति का पालन करने (**शोषण**) और नए संभावित राज्यों का अन्वेषण करने (**अन्वेषण**) के बीच संतुलन बनाना होगा।

## OpenAI जिम

RL के लिए एक बेहतरीन उपकरण है [OpenAI Gym](https://gym.openai.com/) - एक **सिमुलेशन वातावरण**, जो एटारी खेलों से लेकर पोल संतुलन के पीछे के भौतिकी तक कई विभिन्न वातावरणों का अनुकरण कर सकता है। यह रिइंफोर्समेंट लर्निंग एल्गोरिदम को प्रशिक्षित करने के लिए सबसे लोकप्रिय सिमुलेशन वातावरणों में से एक है, और इसे [OpenAI](https://openai.com/) द्वारा बनाए रखा जाता है।

> **नोट**: आप OpenAI जिम से उपलब्ध सभी वातावरण [यहां](https://gym.openai.com/envs/#classic_control) देख सकते हैं।

## कार्टपोल संतुलन

आप सभी ने शायद आधुनिक संतुलन उपकरण जैसे *सेगवे* या *जायरोस्कूटर* देखे होंगे। वे अपने पहियों को एक एक्सेलेरोमीटर या जायरोस्कोप से प्राप्त संकेत के जवाब में समायोजित करके स्वचालित रूप से संतुलन बनाए रखने में सक्षम होते हैं। इस अनुभाग में, हम एक समान समस्या - एक पोल को संतुलित करना - हल करना सीखेंगे। यह उस स्थिति के समान है जब एक सर्कस कलाकार को अपने हाथ पर एक पोल संतुलित करना होता है - लेकिन यह पोल संतुलन केवल 1D में होता है।

संतुलन का एक सरलित संस्करण **कार्टपोल** समस्या के रूप में जाना जाता है। कार्टपोल की दुनिया में, हमारे पास एक क्षैतिज स्लाइडर है जो बाईं या दाईं ओर जा सकता है, और लक्ष्य यह है कि स्लाइडर के शीर्ष पर एक ऊर्ध्वाधर पोल को संतुलित करना है जब यह चलता है।

<img alt="a cartpole" src="images/cartpole.png" width="200"/>

इस वातावरण को बनाने और उपयोग करने के लिए, हमें कुछ पायथन कोड की आवश्यकता है:

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

प्रत्येक वातावरण को ठीक उसी तरह से एक्सेस किया जा सकता है:
* `env.reset` starts a new experiment
* `env.step` एक सिमुलेशन चरण का प्रदर्शन करता है। यह **एक्शन स्पेस** से एक **क्रिया** प्राप्त करता है, और एक **अवलोकन** (अवलोकन स्पेस से) के साथ-साथ एक इनाम और एक समाप्ति ध्वज लौटाता है।

उपर्युक्त उदाहरण में, हम प्रत्येक चरण में एक यादृच्छिक क्रिया करते हैं, यही कारण है कि प्रयोग का जीवन बहुत छोटा है:

![non-balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-nobalance.gif)

एक RL एल्गोरिदम का लक्ष्य एक मॉडल को प्रशिक्षित करना है - जिसे **नीति** π कहा जाता है - जो एक दिए गए राज्य के जवाब में क्रिया लौटाएगा। हम नीति को संभाव्य भी मान सकते हैं, जैसे किसी राज्य *s* और क्रिया *a* के लिए यह π(*a*|*s*) की संभावना लौटाएगा कि हमें राज्य *s* में *a* लेना चाहिए।

## नीति ग्रेडिएंट एल्गोरिदम

नीति को मॉडल करने का सबसे स्पष्ट तरीका एक न्यूरल नेटवर्क बनाना है जो राज्यों को इनपुट के रूप में लेगा, और संबंधित क्रियाएँ (या बल्कि सभी क्रियाओं की संभावनाएँ) लौटाएगा। एक अर्थ में, यह एक सामान्य वर्गीकरण कार्य के समान होगा, जिसमें एक बड़ा अंतर है - हमें पहले से नहीं पता होता है कि हमें प्रत्येक चरण में कौन सी क्रियाएँ लेनी चाहिए।

यहां विचार यह है कि उन संभावनाओं का अनुमान लगाना है। हम **संवृद्धि पुरस्कारों** का एक वेक्टर बनाते हैं जो प्रयोग के प्रत्येक चरण में हमारे कुल इनाम को दर्शाता है। हम कुछ गुणांक γ=0.99 से पहले के पुरस्कारों को गुणा करके **इनाम छूट** भी लागू करते हैं, ताकि पहले के पुरस्कारों की भूमिका कम हो सके। फिर, हम उन चरणों को प्रयोग के पथ के साथ मजबूत करते हैं जो बड़े पुरस्कार देते हैं।

> नीति ग्रेडिएंट एल्गोरिदम के बारे में अधिक जानें और इसे [उदाहरण नोटबुक](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb) में क्रियान्वित होते हुए देखें।

## अभिनेता-आलोचक एल्गोरिदम

नीति ग्रेडिएंट दृष्टिकोण का एक सुधारित संस्करण **अभिनेता-आलोचक** कहलाता है। इसके पीछे का मुख्य विचार यह है कि न्यूरल नेटवर्क को दो चीजें लौटाने के लिए प्रशिक्षित किया जाएगा:

* नीति, जो यह निर्धारित करती है कि कौन सी क्रिया लेनी है। इस भाग को **अभिनेता** कहा जाता है
* इस राज्य में हमें जो कुल इनाम मिलने की अपेक्षा है, उसका अनुमान - इस भाग को **आलोचक** कहा जाता है।

एक अर्थ में, यह आर्किटेक्चर एक [GAN](../../4-ComputerVision/10-GANs/README.md) के समान है, जहां हमारे पास दो नेटवर्क होते हैं जो एक-दूसरे के खिलाफ प्रशिक्षित होते हैं। अभिनेता-आलोचक मॉडल में, अभिनेता वह क्रिया प्रस्तावित करता है जो हमें लेनी होती है, और आलोचक परिणाम का अनुमान लगाने की कोशिश करता है। हालाँकि, हमारा लक्ष्य उन नेटवर्क को एक साथ प्रशिक्षित करना है।

चूंकि हमें प्रयोग के दौरान वास्तविक संवृद्धि पुरस्कार और आलोचक द्वारा लौटाए गए परिणाम दोनों ज्ञात हैं, इसलिए उनके बीच के अंतर को कम करने के लिए एक हानि फ़ंक्शन बनाना अपेक्षाकृत आसान है। इससे हमें **आलोचक हानि** मिलती है। हम नीति ग्रेडिएंट एल्गोरिदम में समान दृष्टिकोण का उपयोग करके **अभिनेता हानि** की गणना कर सकते हैं।

इनमें से एक एल्गोरिदम चलाने के बाद, हम उम्मीद कर सकते हैं कि हमारा कार्टपोल इस तरह व्यवहार करेगा:

![a balancing cartpole](../../../../../lessons/6-Other/22-DeepRL/images/cartpole-balance.gif)

## ✍️ अभ्यास: नीति ग्रेडिएंट और अभिनेता-आलोचक RL

अगले नोटबुक में अपने अध्ययन को जारी रखें:

* [TensorFlow में RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)
* [PyTorch में RL](../../../../../lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb)

## अन्य RL कार्य

आजकल रिइंफोर्समेंट लर्निंग एक तेजी से बढ़ता हुआ अनुसंधान क्षेत्र है। रिइंफोर्समेंट लर्निंग के कुछ दिलचस्प उदाहरण हैं:

* कंप्यूटर को **एटारी खेल** खेलना सिखाना। इस समस्या में चुनौती यह है कि हमारे पास एक साधारण स्थिति नहीं है जो एक वेक्टर के रूप में प्रदर्शित होती है, बल्कि एक स्क्रीनशॉट है - और हमें इस स्क्रीन इमेज को एक विशेषता वेक्टर में परिवर्तित करने के लिए CNN का उपयोग करना होगा, या इनाम की जानकारी निकालनी होगी। एटारी खेल जिम में उपलब्ध हैं।
* कंप्यूटर को बोर्ड गेम्स जैसे शतरंज और गो खेलना सिखाना। हाल ही में, अत्याधुनिक कार्यक्रम जैसे **अल्फा जीरो** को एक-दूसरे के खिलाफ खेलने वाले दो एजेंटों द्वारा शून्य से प्रशिक्षित किया गया, और प्रत्येक चरण में सुधार किया गया।
* उद्योग में, RL का उपयोग सिमुलेशन से नियंत्रण प्रणाली बनाने के लिए किया जाता है। [बोंसाई](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste) नामक एक सेवा विशेष रूप से इसके लिए डिज़ाइन की गई है।

## निष्कर्ष

अब हमने यह सीख लिया है कि एजेंटों को अच्छे परिणाम प्राप्त करने के लिए कैसे प्रशिक्षित किया जाए, बस उन्हें एक इनाम फ़ंक्शन प्रदान करके जो खेल की इच्छित स्थिति को परिभाषित करता है, और उन्हें खोज स्थान का बुद्धिमानी से अन्वेषण करने का अवसर देकर। हमने सफलतापूर्वक दो एल्गोरिदम आजमाए हैं, और अपेक्षाकृत कम समय में एक अच्छा परिणाम प्राप्त किया है। हालाँकि, यह RL में आपकी यात्रा की केवल शुरुआत है, और यदि आप गहराई से जाना चाहते हैं तो आपको निश्चित रूप से एक अलग पाठ्यक्रम लेने पर विचार करना चाहिए।

## 🚀 चुनौती

'अन्य RL कार्य' अनुभाग में सूचीबद्ध अनुप्रयोगों का अन्वेषण करें और इनमें से एक को लागू करने की कोशिश करें!

## [पोस्ट-लेक्चर क्विज़](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## समीक्षा और आत्म अध्ययन

हमारे [शुरुआतकर्ताओं के लिए मशीन लर्निंग पाठ्यक्रम](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md) में क्लासिकल रिइंफोर्समेंट लर्निंग के बारे में अधिक जानें।

देखें [यह शानदार वीडियो](https://www.youtube.com/watch?v=qv6UVOQ0F44) जो बताता है कि एक कंप्यूटर सुपर मारियो खेलना कैसे सीख सकता है।

## असाइनमेंट: [माउंटेन कार को प्रशिक्षित करें](lab/README.md)

इस असाइनमेंट के दौरान आपका लक्ष्य एक अलग जिम वातावरण - [माउंटेन कार](https://www.gymlibrary.ml/environments/classic_control/mountain_car/) को प्रशिक्षित करना होगा।

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या गलतियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्राधिकृत स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।