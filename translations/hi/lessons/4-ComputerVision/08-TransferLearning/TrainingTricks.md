# गहरे अध्ययन के प्रशिक्षण के तरीके

जैसे-जैसे न्यूरल नेटवर्क गहरे होते जाते हैं, उनके प्रशिक्षण की प्रक्रिया और अधिक चुनौतीपूर्ण होती जाती है। एक मुख्य समस्या जिसे हम [गायब होते ग्रेडिएंट्स](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) या [फूलते ग्रेडिएंट्स](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.) कहते हैं। [यह पोस्ट](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) इन समस्याओं का अच्छा परिचय देती है।

गहरे नेटवर्क के प्रशिक्षण को अधिक कुशल बनाने के लिए, कुछ तकनीकें हैं जिनका उपयोग किया जा सकता है।

## मानों को उचित अंतराल में रखना

संख्यात्मक गणनाओं को अधिक स्थिर बनाने के लिए, हम यह सुनिश्चित करना चाहते हैं कि हमारे न्यूरल नेटवर्क में सभी मान उचित पैमाने के भीतर हों, आमतौर पर [-1..1] या [0..1]। यह एक बहुत सख्त आवश्यकता नहीं है, लेकिन फ्लोटिंग पॉइंट गणनाओं की प्रकृति ऐसी है कि विभिन्न परिमाण के मानों को एक साथ सटीक रूप से संचालित नहीं किया जा सकता। उदाहरण के लिए, यदि हम 10<sup>-10</sup> और 10<sup>10</sup> को जोड़ते हैं, तो हमें संभावना है कि 10<sup>10</sup> प्राप्त होगा, क्योंकि छोटा मान बड़े वाले के समान क्रम में "परिवर्तित" हो जाएगा, और इस प्रकार मैन्टिसा खो जाएगा।

अधिकांश सक्रियण फ़ंक्शंस में [-1..1] के चारों ओर गैर-रेखीयताएँ होती हैं, और इसलिए सभी इनपुट डेटा को [-1..1] या [0..1] अंतराल में स्केल करना समझदारी है।

## प्रारंभिक वजन प्रारंभिककरण

आदर्श रूप से, हम चाहते हैं कि नेटवर्क लेयर्स के माध्यम से गुजरने के बाद मान एक ही रेंज में हों। इसलिए यह महत्वपूर्ण है कि वजन को इस प्रकार प्रारंभिककरण किया जाए कि मानों का वितरण संरक्षित रहे।

सामान्य वितरण **N(0,1)** एक अच्छा विचार नहीं है, क्योंकि यदि हमारे पास *n* इनपुट हैं, तो आउटपुट का मानक विचलन *n* होगा, और मान [0..1] अंतराल से बाहर निकलने की संभावना है।

निम्नलिखित प्रारंभिककरण अक्सर उपयोग किए जाते हैं:

 * यूनिफॉर्म वितरण -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/√n_in)** यह सुनिश्चित करता है कि शून्य औसत और 1 के मानक विचलन वाले इनपुट के लिए वही औसत/मानक विचलन बना रहे
 * **N(0,√2/(n_in+n_out))** -- जिसे **Xavier प्रारंभिककरण** (`glorot`) कहा जाता है, यह आगे और पीछे प्रसार के दौरान संकेतों को सीमा में बनाए रखने में मदद करता है

## बैच सामान्यीकरण

सही वजन प्रारंभिककरण के साथ भी, प्रशिक्षण के दौरान वजन मनमाने रूप से बड़े या छोटे हो सकते हैं, और वे संकेतों को उचित सीमा से बाहर ले जाएंगे। हम एक **सामान्यीकरण** तकनीक का उपयोग करके संकेतों को वापस ला सकते हैं। जबकि इनमें से कई हैं (वजन सामान्यीकरण, परत सामान्यीकरण), सबसे अधिक उपयोग किया जाने वाला बैच सामान्यीकरण है।

**बैच सामान्यीकरण** का विचार यह है कि मिनीबैच के सभी मानों को ध्यान में रखते हुए, और उन मानों के आधार पर सामान्यीकरण (यानी औसत घटाना और मानक विचलन द्वारा विभाजित करना) करना है। इसे एक नेटवर्क परत के रूप में लागू किया जाता है जो वजन लागू करने के बाद यह सामान्यीकरण करता है, लेकिन सक्रियण फ़ंक्शन से पहले। परिणामस्वरूप, हम उच्च अंतिम सटीकता और तेज़ प्रशिक्षण देखने की संभावना रखते हैं।

यहां बैच सामान्यीकरण पर [मूल पेपर](https://arxiv.org/pdf/1502.03167.pdf), [विकिपीडिया पर स्पष्टीकरण](https://en.wikipedia.org/wiki/Batch_normalization), और [एक अच्छा परिचयात्मक ब्लॉग पोस्ट](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) (और [रूसी में](https://habrahabr.ru/post/309302/)) है।

## ड्रॉपआउट

**ड्रॉपआउट** एक दिलचस्प तकनीक है जो प्रशिक्षण के दौरान एक निश्चित प्रतिशत यादृच्छिक न्यूरॉन्स को हटा देती है। इसे एक परत के रूप में भी लागू किया गया है जिसमें एक पैरामीटर होता है (हटाने के लिए न्यूरॉन्स का प्रतिशत, आमतौर पर 10%-50%), और प्रशिक्षण के दौरान यह इनपुट वेक्टर के यादृच्छिक तत्वों को शून्य कर देता है, इससे पहले कि इसे अगले परत में भेजा जाए।

हालांकि यह एक अजीब विचार की तरह लग सकता है, आप [`Dropout.ipynb`](../../../../../lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb) नोटबुक में MNIST अंक वर्गीकरणकर्ता के प्रशिक्षण पर ड्रॉपआउट के प्रभाव को देख सकते हैं। यह प्रशिक्षण को तेज करता है और हमें कम प्रशिक्षण युग में उच्च सटीकता प्राप्त करने की अनुमति देता है।

इस प्रभाव को कई तरीकों से समझाया जा सकता है:

 * इसे मॉडल के लिए एक यादृच्छिक झटका कारक माना जा सकता है, जो अनुकूलन को स्थानीय न्यूनतम से बाहर ले जाता है
 * इसे *अप्रत्यक्ष मॉडल औसत* के रूप में माना जा सकता है, क्योंकि हम कह सकते हैं कि ड्रॉपआउट के दौरान हम थोड़ा अलग मॉडल का प्रशिक्षण कर रहे हैं

> *कुछ लोग कहते हैं कि जब एक नशे में व्यक्ति कुछ सीखने की कोशिश करता है, तो वह इसे अगले सुबह बेहतर याद रखेगा, एक होशियार व्यक्ति की तुलना में, क्योंकि कुछ खराब काम कर रहे न्यूरॉन्स वाला मस्तिष्क अर्थ को समझने के लिए बेहतर अनुकूलन करता है। हमने कभी यह परीक्षण नहीं किया कि यह सच है या नहीं*

## ओवरफिटिंग को रोकना

गहरे अध्ययन का एक बहुत महत्वपूर्ण पहलू [ओवरफिटिंग](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) को रोकने में सक्षम होना है। जबकि बहुत शक्तिशाली न्यूरल नेटवर्क मॉडल का उपयोग करना लुभावना हो सकता है, हमें हमेशा मॉडल के पैरामीटर की संख्या को प्रशिक्षण नमूनों की संख्या के साथ संतुलित करना चाहिए।

> सुनिश्चित करें कि आप [ओवरफिटिंग](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) की अवधारणा को समझते हैं जिसे हमने पहले पेश किया था!

ओवरफिटिंग को रोकने के कई तरीके हैं:

 * प्रारंभिक रोकना -- सत्यापन सेट पर त्रुटि की निरंतर निगरानी करना और जब सत्यापन त्रुटि बढ़ना शुरू हो जाए तो प्रशिक्षण रोकना।
 * स्पष्ट वजन अपघटन / नियमितीकरण -- उच्च पूर्णांक मानों के लिए हानि फ़ंक्शन में एक अतिरिक्त दंड जोड़ना, जो मॉडल को बहुत अस्थिर परिणाम प्राप्त करने से रोकता है
 * मॉडल औसत -- कई मॉडलों को प्रशिक्षित करना और फिर परिणाम का औसत निकालना। यह भिन्नता को कम करने में मदद करता है।
 * ड्रॉपआउट (अप्रत्यक्ष मॉडल औसत)

## ऑप्टिमाइज़र / प्रशिक्षण एल्गोरिदम

प्रशिक्षण का एक और महत्वपूर्ण पहलू अच्छा प्रशिक्षण एल्गोरिदम चुनना है। जबकि पारंपरिक **ग्रेडिएंट डिसेंट** एक उचित विकल्प है, यह कभी-कभी बहुत धीमा हो सकता है, या अन्य समस्याओं का परिणाम हो सकता है।

गहरे अध्ययन में, हम **स्टोकास्टिक ग्रेडिएंट डिसेंट** (SGD) का उपयोग करते हैं, जो एक ग्रेडिएंट डिसेंट है जो मिनीबैचेस पर लागू होता है, जो प्रशिक्षण सेट से यादृच्छिक रूप से चुने जाते हैं। वजन को इस सूत्र का उपयोग करके समायोजित किया जाता है:

w<sup>t+1</sup> = w<sup>t</sup> - η∇ℒ

### संवेग

**संवेग SGD** में, हम पिछले चरणों से एक ग्रेडिएंट का एक भाग रखते हैं। यह तब के समान है जब हम कहीं इनर्शिया के साथ चल रहे होते हैं, और हमें एक अलग दिशा में एक धक्का मिलता है, तो हमारी पथ तुरंत नहीं बदलती, लेकिन मूल गति का कुछ भाग बनाए रखती है। यहां हम *गति* को दर्शाने के लिए एक और वेक्टर v पेश करते हैं:

* v<sup>t+1</sup> = γ v<sup>t</sup> - η∇ℒ
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

यहां पैरामीटर γ यह दर्शाता है कि हम इनर्शिया को कितनी मात्रा में ध्यान में रखते हैं: γ=0 पारंपरिक SGD के लिए है; γ=1 एक शुद्ध गति समीकरण है।

### एडम, एडेग्रेड, आदि।

चूंकि प्रत्येक परत में हम कुछ मैट्रिक्स W<sub>i</sub> द्वारा संकेतों को गुणा करते हैं, ||W<sub>i</sub>|| के आधार पर, ग्रेडिएंट या तो घट सकता है और 0 के करीब हो सकता है, या अनंत रूप से बढ़ सकता है। यह फूलते/गायब होते ग्रेडिएंट्स की समस्या का सार है।

इस समस्या का एक समाधान यह है कि हम समीकरण में केवल ग्रेडिएंट की दिशा का उपयोग करें, और पूर्णांक मान को नजरअंदाज करें, अर्थात्

w<sup>t+1</sup> = w<sup>t</sup> - η(∇ℒ/||∇ℒ||), जहां ||∇ℒ|| = √∑(∇ℒ)<sup>2</sup>

इस एल्गोरिदम को **एडेग्रेड** कहा जाता है। इसी विचार का उपयोग करने वाले अन्य एल्गोरिदम हैं: **आरएमएसप्रॉप**, **एडम**

> **एडम** को कई अनुप्रयोगों के लिए एक बहुत प्रभावी एल्गोरिदम माना जाता है, इसलिए यदि आप सुनिश्चित नहीं हैं कि कौन सा उपयोग करना है - एडम का उपयोग करें।

### ग्रेडिएंट क्लिपिंग

ग्रेडिएंट क्लिपिंग उपरोक्त विचार का एक विस्तार है। जब ||∇ℒ|| ≤ θ होता है, तो हम वजन अनुकूलन में मूल ग्रेडिएंट पर विचार करते हैं, और जब ||∇ℒ|| > θ - हम ग्रेडिएंट को इसके मान से विभाजित करते हैं। यहां θ एक पैरामीटर है, अधिकांश मामलों में हम θ=1 या θ=10 ले सकते हैं।

### लर्निंग रेट अपघटन

प्रशिक्षण की सफलता अक्सर लर्निंग रेट पैरामीटर η पर निर्भर करती है। यह तार्किक है कि η के बड़े मान तेज़ प्रशिक्षण का परिणाम देते हैं, जो हम आमतौर पर प्रशिक्षण की शुरुआत में चाहते हैं, और फिर η के छोटे मान हमें नेटवर्क को ठीक करने की अनुमति देते हैं। इसलिए, अधिकांश मामलों में हम प्रशिक्षण की प्रक्रिया में η को घटाना चाहते हैं।

यह η को कुछ संख्या (जैसे 0.98) से प्रत्येक प्रशिक्षण युग के बाद गुणा करके किया जा सकता है, या अधिक जटिल **लर्निंग रेट शेड्यूल** का उपयोग करके।

## विभिन्न नेटवर्क आर्किटेक्चर

आपकी समस्या के लिए सही नेटवर्क आर्किटेक्चर का चयन करना कठिन हो सकता है। सामान्यतः, हम एक ऐसे आर्किटेक्चर को अपनाते हैं जिसने हमारे विशिष्ट कार्य (या समान) के लिए काम करने का प्रमाण दिया है। यहाँ कंप्यूटर दृष्टि के लिए न्यूरल नेटवर्क आर्किटेक्चर का [एक अच्छा अवलोकन](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) है।

> यह महत्वपूर्ण है कि हम एक ऐसा आर्किटेक्चर चुनें जो हमारे पास मौजूद प्रशिक्षण नमूनों की संख्या के लिए पर्याप्त शक्तिशाली हो। बहुत शक्तिशाली मॉडल का चयन [ओवरफिटिंग](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) का परिणाम हो सकता है

एक और अच्छा तरीका होगा कि हम एक ऐसा आर्किटेक्चर उपयोग करें जो आवश्यक जटिलता के अनुसार अपने आप समायोजित हो। कुछ हद तक, **ResNet** आर्किटेक्चर और **Inception** स्व-संयोजित हैं। [कंप्यूटर दृष्टि आर्किटेक्चर पर अधिक](../07-ConvNets/CNN_Architectures.md)

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान रखें कि स्वचालित अनुवाद में त्रुटियाँ या असंगतियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्राधिकृत स्रोत के रूप में माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। हम इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए उत्तरदायी नहीं हैं।