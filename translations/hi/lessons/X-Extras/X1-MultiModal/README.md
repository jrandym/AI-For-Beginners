# मल्टी-मोडल नेटवर्क्स

NLP कार्यों को हल करने के लिए ट्रांसफार्मर मॉडल की सफलता के बाद, समान या समान आर्किटेक्चर को कंप्यूटर विज़न कार्यों पर लागू किया गया है। ऐसे मॉडलों का निर्माण करने में बढ़ती रुचि है जो दृष्टि और प्राकृतिक भाषा क्षमताओं को *संयोजित* करेंगे। ऐसे प्रयासों में से एक OpenAI द्वारा किया गया था, और इसे CLIP और DALL.E कहा जाता है।

## कंट्रास्टिव इमेज प्री-ट्रेनिंग (CLIP)

CLIP का मुख्य विचार यह है कि यह टेक्स्ट प्रॉम्प्ट्स की तुलना एक छवि के साथ कर सके और यह निर्धारित कर सके कि छवि प्रॉम्प्ट के साथ कितनी अच्छी तरह मेल खाती है।

![CLIP आर्किटेक्चर](../../../../../translated_images/clip-arch.b3dbf20b4e8ed8be1c38e2bc6100fd3cc257c33cda4692b301be91f791b13ea7.hi.png)

> *चित्र [इस ब्लॉग पोस्ट](https://openai.com/blog/clip/) से*

मॉडल को इंटरनेट से प्राप्त छवियों और उनके कैप्शन पर प्रशिक्षित किया गया है। प्रत्येक बैच के लिए, हम (छवि, टेक्स्ट) के N जोड़ों को लेते हैं, और उन्हें कुछ वेक्टर प्रतिनिधित्व में परिवर्तित करते हैं। 
वे प्रतिनिधित्व फिर एक साथ मिलाए जाते हैं। लॉस फंक्शन को एक जोड़े (जैसे I और T) के लिए संबंधित वेक्टरों के बीच कोसाइन समानता को अधिकतम करने के लिए परिभाषित किया गया है, और सभी अन्य जोड़ों के बीच कोसाइन समानता को न्यूनतम करने के लिए। यही कारण है कि इस दृष्टिकोण को **कंट्रास्टिव** कहा जाता है।

CLIP मॉडल/लाइब्रेरी [OpenAI GitHub](https://github.com/openai/CLIP) से उपलब्ध है। इस दृष्टिकोण का वर्णन [इस ब्लॉग पोस्ट](https://openai.com/blog/clip/) में किया गया है, और अधिक विस्तार में [इस पेपर](https://arxiv.org/pdf/2103.00020.pdf) में।

एक बार जब इस मॉडल को प्री-ट्रेन किया जाता है, तो हम इसे छवियों का एक बैच और टेक्स्ट प्रॉम्प्ट्स का एक बैच दे सकते हैं, और यह हमें संभावनाओं के साथ टेन्सर लौटाएगा। CLIP का उपयोग कई कार्यों के लिए किया जा सकता है:

**छवि वर्गीकरण**

मान लीजिए कि हमें छवियों को, कहने के लिए, बिल्लियों, कुत्तों और मनुष्यों के बीच वर्गीकृत करने की आवश्यकता है। इस मामले में, हम मॉडल को एक छवि और टेक्स्ट प्रॉम्प्ट्स की एक श्रृंखला दे सकते हैं: "*बिल्ली की तस्वीर*", "*कुत्ते की तस्वीर*", "*मनुष्य की तस्वीर*"। परिणामस्वरूप 3 संभावनाओं के वेक्टर में, हमें बस उच्चतम मूल्य वाला अनुक्रमांक चुनना है।

![CLIP के लिए छवि वर्गीकरण](../../../../../translated_images/clip-class.3af42ef0b2b19369a633df5f20ddf4f5a01d6c8ffa181e9d3a0572c19f919f72.hi.png)

> *चित्र [इस ब्लॉग पोस्ट](https://openai.com/blog/clip/) से*

**टेक्स्ट-आधारित छवि खोज**

हम इसके विपरीत भी कर सकते हैं। यदि हमारे पास छवियों का एक संग्रह है, तो हम इस संग्रह को मॉडल को पास कर सकते हैं, और एक टेक्स्ट प्रॉम्प्ट - यह हमें उस छवि को देगा जो दिए गए प्रॉम्प्ट के साथ सबसे अधिक समान है।

## ✍️ उदाहरण: [CLIP का उपयोग करके छवि वर्गीकरण और छवि खोज](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb)

CLIP को क्रियान्वित करने के लिए [Clip.ipynb](../../../../../lessons/X-Extras/X1-MultiModal/Clip.ipynb) नोटबुक खोलें।

## VQGAN+ CLIP के साथ छवि निर्माण

CLIP का उपयोग टेक्स्ट प्रॉम्प्ट से **छवि निर्माण** के लिए भी किया जा सकता है। ऐसा करने के लिए, हमें एक **जनरेटर मॉडल** की आवश्यकता है जो कुछ वेक्टर इनपुट के आधार पर छवियां उत्पन्न कर सके। ऐसे मॉडलों में से एक को [VQGAN](https://compvis.github.io/taming-transformers/) (वेक्टर-क्वांटाइज्ड GAN) कहा जाता है।

VQGAN के मुख्य विचार जो इसे साधारण [GAN](../../4-ComputerVision/10-GANs/README.md) से अलग करते हैं, निम्नलिखित हैं:
* छवि को बनाने वाले संदर्भ-समृद्ध दृश्य भागों की एक श्रृंखला उत्पन्न करने के लिए ऑटोरेग्रेसिव ट्रांसफार्मर आर्किटेक्चर का उपयोग करना। ये दृश्य भाग [CNN](../../4-ComputerVision/07-ConvNets/README.md) द्वारा सीखे जाते हैं।
* उप-छवि विभेदक का उपयोग करना जो यह पहचानता है कि छवि के भाग "वास्तविक" हैं या "नकली" (पारंपरिक GAN में "सभी या कुछ नहीं" दृष्टिकोण के विपरीत)।

VQGAN के बारे में अधिक जानने के लिए [Taming Transformers](https://compvis.github.io/taming-transformers/) वेबसाइट पर जाएं।

VQGAN और पारंपरिक GAN के बीच एक महत्वपूर्ण अंतर यह है कि बाद वाला किसी भी इनपुट वेक्टर से एक उचित छवि उत्पन्न कर सकता है, जबकि VQGAN संभवतः एक छवि उत्पन्न करेगा जो सामंजस्यपूर्ण नहीं होगी। इसलिए, हमें छवि निर्माण प्रक्रिया को आगे निर्देशित करने की आवश्यकता है, और यह CLIP का उपयोग करके किया जा सकता है।

![VQGAN+CLIP आर्किटेक्चर](../../../../../translated_images/vqgan.5027fe05051dfa3101950cfa930303f66e6478b9bd273e83766731796e462d9b.hi.png)

टेक्स्ट प्रॉम्प्ट के अनुसार छवि उत्पन्न करने के लिए, हम एक यादृच्छिक एन्कोडिंग वेक्टर से शुरू करते हैं जो VQGAN के माध्यम से पास किया जाता है ताकि एक छवि उत्पन्न की जा सके। फिर CLIP का उपयोग लॉस फंक्शन उत्पन्न करने के लिए किया जाता है जो दिखाता है कि छवि टेक्स्ट प्रॉम्प्ट के साथ कितनी अच्छी तरह मेल खाती है। लक्ष्य फिर इस लॉस को न्यूनतम करना है, बैक प्रॉपगेशन का उपयोग करके इनपुट वेक्टर पैरामीटर को समायोजित करना है।

VQGAN+CLIP को लागू करने वाली एक शानदार लाइब्रेरी [Pixray](http://github.com/pixray/pixray) है।

![Pixray द्वारा उत्पन्न चित्र](../../../../../translated_images/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.2384968e9db8a0d09dc96de938b9f95bde8a7e1c721f48f286a7795bf16d56c7.hi.png) |  ![Pixray द्वारा उत्पन्न चित्र](../../../../../translated_images/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.e0b6495f210a439077e1c32cc8afdf714e634fe24dc78dc5aa45fd2f560b0ed5.hi.png) | ![Pixray द्वारा उत्पन्न चित्र](../../../../../translated_images/a_closeup_oil_portrait_of_old_male_teacher_of_math.5362e67aa7fc2683b9d36a613b364deb7454760cd39205623fc1e3938fa133c0.hi.png)
----|----|----
*एक किताब के साथ युवा पुरुष साहित्य शिक्षक का एक करीबी जलरंग पोर्ट्रेट* प्रॉम्प्ट से उत्पन्न चित्र | *एक कंप्यूटर के साथ युवा महिला कंप्यूटर विज्ञान शिक्षक का एक करीबी तेल पोर्ट्रेट* प्रॉम्प्ट से उत्पन्न चित्र | *कालेबोर्ड के सामने पुराने पुरुष गणित शिक्षक का एक करीबी तेल पोर्ट्रेट* प्रॉम्प्ट से उत्पन्न चित्र

> **आर्टिफिशियल टीचर्स** संग्रह से चित्र [Dmitry Soshnikov](http://soshnikov.com) द्वारा

## DALL-E
### [DALL-E 1](https://openai.com/research/dall-e)
DALL-E एक GPT-3 का संस्करण है जिसे प्रॉम्प्ट से छवियां उत्पन्न करने के लिए प्रशिक्षित किया गया है। इसे 12 अरब पैरामीटर के साथ प्रशिक्षित किया गया है।

CLIP के विपरीत, DALL-E टेक्स्ट और छवि दोनों को छवियों और टेक्स्ट के लिए एकल टोकन स्ट्रीम के रूप में प्राप्त करता है। इसलिए, कई प्रॉम्प्ट्स से, आप टेक्स्ट के आधार पर छवियां उत्पन्न कर सकते हैं।

### [DALL-E 2](https://openai.com/dall-e-2)
DALL-E 1 और 2 के बीच मुख्य अंतर यह है कि यह अधिक यथार्थवादी छवियां और कला उत्पन्न करता है।

DALL-E के साथ छवि निर्माण के उदाहरण:
![Pixray द्वारा उत्पन्न चित्र](../../../../../translated_images/DALL·E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.6c235e8271d9ed10ce985d86aeb241a58518958647973af136912116b9518fce.hi.png) |  ![Pixray द्वारा उत्पन्न चित्र](../../../../../translated_images/DALL·E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.f21dc4166340b6c8b4d1cb57efd1e22127407f9b28c9ac7afe11344065369e64.hi.png) | ![Pixray द्वारा उत्पन्न चित्र](../../../../../translated_images/DALL·E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.d331c2dfbdc3f7c46aa65c0809066f5e7ed4b49609cd259852e760df21051e4a.hi.png)
----|----|----
*एक किताब के साथ युवा पुरुष साहित्य शिक्षक का एक करीबी जलरंग पोर्ट्रेट* प्रॉम्प्ट से उत्पन्न चित्र | *एक कंप्यूटर के साथ युवा महिला कंप्यूटर विज्ञान शिक्षक का एक करीबी तेल पोर्ट्रेट* प्रॉम्प्ट से उत्पन्न चित्र | *कालेबोर्ड के सामने पुराने पुरुष गणित शिक्षक का एक करीबी तेल पोर्ट्रेट* प्रॉम्प्ट से उत्पन्न चित्र

## संदर्भ

* VQGAN पेपर: [उच्च-रिज़ॉल्यूशन छवि संश्लेषण के लिए ट्रांसफार्मर्स को वश में करना](https://compvis.github.io/taming-transformers/paper/paper.pdf)
* CLIP पेपर: [प्राकृतिक भाषा सुपरविजन से ट्रांसफर करने योग्य दृश्य मॉडल सीखना](https://arxiv.org/pdf/2103.00020.pdf)

**अस्वीकृति**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में अधिकृत स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।